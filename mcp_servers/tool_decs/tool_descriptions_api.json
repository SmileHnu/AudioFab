{
    "cosyvoice2tool_api": {
      "description": "CosyVoice2 Text-to-Speech Tool",
      "detailed_description": "Uses the CosyVoice2 model to convert text into realistic speech. You can provide a reference audio as a voice style prompt (3-second fast cloning), or use text instructions to control the generation effect (natural language control).",
      "parameters": {
        "tts_text": {
          "type": "string",
          "description": "The text content to be synthesized into speech.",
          "required": true,
          "example": "Hello, this is a test of the CosyVoice2 text-to-speech tool."
        },
        "mode": {
          "type": "string",
          "description": "Inference mode. Use 'cosy' for 3s-fast-cloning with a prompt wav, or 'instruct' for natural language control.",
          "required": "optional",
          "default": "cosy",
          "enum": [
            "cosy",
            "instruct"
          ],
          "example": "cosy"
        },
        "prompt_text": {
          "type": "string",
          "description": "Prompt text, used only in certain modes.",
          "required": "optional",
          "example": "A gentle and soothing voice."
        },
        "prompt_wav_path": {
          "type": "string",
          "description": "Path to the reference audio file, providing a voice style sample for 'cosy' mode.",
          "required": "optional",
          "example": "input/reference_voice.wav"
        },
        "instruct_text": {
          "type": "string",
          "description": "Instructional text used to control the generation effect in 'instruct' mode.",
          "required": "optional",
          "example": "Read the text in a happy and excited tone."
        },
        "seed": {
          "type": "number",
          "description": "Random inference seed to control the randomness of the generated result.",
          "required": "optional",
          "default": 0,
          "example": 42.0
        },
        "output_path": {
          "type": "string",
          "description": "The path to save the generated audio file.",
          "required": "optional",
          "default": "output.wav",
          "example": "output/generated_speech.wav"
        }
      },
      "examples": [
        {
          "task_description": "Synthesize speech using a reference voice audio (voice cloning).",
          "Act": {
            "tool": "cosyvoice2tool_api",
            "arguments": {
              "tts_text": "The quick brown fox jumps over the lazy dog.",
              "mode": "cosy",
              "prompt_wav_path": "input/source_voice.wav",
              "output_path": "output/cloned_voice.wav"
            }
          }
        },
        {
          "task_description": "Synthesize speech using natural language instructions.",
          "Act": {
            "tool": "cosyvoice2tool_api",
            "arguments": {
              "tts_text": "Welcome to the world of tomorrow!",
              "mode": "instruct",
              "instruct_text": "Speak in a deep, booming announcer voice.",
              "output_path": "output/announcer_voice.wav"
            }
          }
        },
        {
          "task_description": "Perform a basic text-to-speech conversion with a specific random seed.",
          "Act": {
            "tool": "cosyvoice2tool_api",
            "arguments": {
              "tts_text": "This is a simple text-to-speech conversion.",
              "seed": 123.0,
              "output_path": "output/simple_tts.wav"
            }
          }
        }
      ]
    },
    "AudioX_api": {
      "description": "AudioX Audio Generation Tool",
      "detailed_description": "Uses the AudioX model to generate high-quality audio based on text prompts, video, or audio prompts. This tool can synthesize audio from various inputs, including text descriptions, reference video files, and audio prompts, offering detailed control over the generation process through numerous parameters.",
      "parameters": {
        "prompt": {
          "type": "string",
          "description": "A text prompt describing the audio content to be generated.",
          "required": "optional",
          "default": "",
          "example": "A cinematic explosion with debris sounds"
        },
        "negative_prompt": {
          "type": "string",
          "description": "A negative prompt describing features that should not appear in the generated result.",
          "required": "optional",
          "default": null,
          "example": "low quality, muffled, noisy"
        },
        "video_file_path": {
          "type": "string",
          "description": "Path to the video file to be used as a generation reference.",
          "required": "optional",
          "default": null,
          "example": "input/source_video.mp4"
        },
        "audio_prompt_file_path": {
          "type": "string",
          "description": "Path to the audio prompt file to be used as a generation reference.",
          "required": "optional",
          "default": null,
          "example": "input/reference_audio.wav"
        },
        "audio_prompt_path": {
          "type": "string",
          "description": "Path to the audio prompt to be used as a generation reference.",
          "required": "optional",
          "default": null,
          "example": "input/reference_prompt.wav"
        },
        "seconds_start": {
          "type": "float",
          "description": "The start time in seconds of the video to use for generation.",
          "required": "optional",
          "default": 0,
          "example": 5.0
        },
        "seconds_total": {
          "type": "float",
          "description": "The total duration in seconds of the audio to be generated.",
          "required": "optional",
          "default": 10,
          "example": 15.5
        },
        "cfg_scale": {
          "type": "float",
          "description": "CFG scale parameter, controlling how closely the generation follows the prompt.",
          "required": "optional",
          "default": 7,
          "example": 8.5
        },
        "steps": {
          "type": "float",
          "description": "Number of sampling steps, affecting generation quality and time.",
          "required": "optional",
          "default": 100,
          "example": 150
        },
        "preview_every": {
          "type": "float",
          "description": "Sets the preview frequency during generation.",
          "required": "optional",
          "default": 0,
          "example": 10
        },
        "seed": {
          "type": "string",
          "description": "Random seed for generation. Set to '-1' for a random seed.",
          "required": "optional",
          "default": "-1",
          "example": "12345"
        },
        "sampler_type": {
          "type": "string",
          "description": "The type of sampler to use.",
          "required": "optional",
          "default": "dpmpp-3m-sde",
          "enum": [
            "dpmpp-2m-sde",
            "dpmpp-3m-sde"
          ],
          "example": "dpmpp-2m-sde"
        },
        "sigma_min": {
          "type": "float",
          "description": "The minimum sigma value for the sampler.",
          "required": "optional",
          "default": 0.03,
          "example": 0.05
        },
        "sigma_max": {
          "type": "float",
          "description": "The maximum sigma value for the sampler.",
          "required": "optional",
          "default": 500,
          "example": 400
        },
        "cfg_rescale": {
          "type": "float",
          "description": "CFG rescale amount.",
          "required": "optional",
          "default": 0,
          "example": 0.5
        },
        "use_init": {
          "type": "boolean",
          "description": "Whether to use an initial audio file for generation.",
          "required": "optional",
          "default": false,
          "example": true
        },
        "init_audio_path": {
          "type": "string",
          "description": "Path to the initial audio file.",
          "required": "optional",
          "default": null,
          "example": "input/initial_sound.wav"
        },
        "init_noise_level": {
          "type": "float",
          "description": "The noise level to apply to the initial audio.",
          "required": "optional",
          "default": 0.1,
          "example": 0.2
        },
        "output_audio_path": {
          "type": "string",
          "description": "Path to save the output audio file.",
          "required": "optional",
          "default": "output_audio.wav",
          "example": "output/generated_sound.wav"
        },
        "output_video_path": {
          "type": "string",
          "description": "Path to save the output video file.",
          "required": "optional",
          "default": "output_video.mp4",
          "example": "output/generated_video.mp4"
        }
      },
      "examples": [
        {
          "task_description": "Generate audio from a text prompt",
          "Act": {
            "tool": "AudioX_api",
            "arguments": {
              "prompt": "sound of a heavy rainstorm with thunder",
              "seconds_total": 15,
              "output_audio_path": "output/rainstorm.wav"
            }
          }
        },
        {
          "task_description": "Generate audio conditioned on a video file",
          "Act": {
            "tool": "AudioX_api",
            "arguments": {
              "prompt": "footsteps on a gravel path",
              "video_file_path": "input/walking_video.mp4",
              "seconds_total": 20,
              "output_audio_path": "output/footsteps_audio.wav",
              "output_video_path": "output/walking_video_with_audio.mp4"
            }
          }
        },
        {
          "task_description": "Generate audio using an initial audio file and advanced settings",
          "Act": {
            "tool": "AudioX_api",
            "arguments": {
              "prompt": "futuristic vehicle engine sound",
              "use_init": true,
              "init_audio_path": "input/base_engine_sound.wav",
              "init_noise_level": 0.2,
              "steps": 150,
              "cfg_scale": 9.0,
              "sampler_type": "dpmpp-2m-sde",
              "output_audio_path": "output/futuristic_engine.wav"
            }
          }
        }
      ]
    },
    "Qwen2audio_api": {
      "description": "Qwen2-Audio-7B-Instruct Multimodal Dialogue Tool",
      "detailed_description": "Engage in multimodal conversations using both text and audio with the Qwen2-Audio-7B-Instruct model. You can send text or audio for dialogue and also manage the conversation history.",
      "parameters": {
        "prompt": {
          "type": "string",
          "description": "Text prompt, the user's text input.",
          "required": "optional",
          "default": "",
          "example": "Please describe what you hear in this audio."
        },
        "audio_file_path": {
          "type": "string",
          "description": "Path to the input audio file, the user's audio input.",
          "required": "optional",
          "example": "input/audio/user_query.wav"
        },
        "chatbot_history": {
          "type": "list",
          "description": "The chat history, formatted as a list. If None, a new conversation is started.",
          "required": "optional",
          "example": "[{\"role\": \"user\", \"content\": \"What is this sound?\"}, {\"role\": \"assistant\", \"content\": \"It sounds like a bird chirping.\"}]"
        },
        "action": {
          "type": "string",
          "description": "The type of action to perform. Options are 'chat' (to continue the conversation), 'regenerate' (to regenerate the last response), and 'reset' (to reset the conversation).",
          "required": "optional",
          "default": "chat",
          "enum": [
            "chat",
            "regenerate",
            "reset"
          ],
          "example": "regenerate"
        },
        "save_history": {
          "type": "boolean",
          "description": "Whether to save the conversation history to a file.",
          "required": "optional",
          "default": true,
          "example": false
        },
        "output_file": {
          "type": "string",
          "description": "The file path to save the conversation history.",
          "required": "optional",
          "default": "conversation_history.json",
          "example": "output/my_conversation.json"
        }
      },
      "examples": [
        {
          "task_description": "Start a new multimodal chat with an audio file and a text prompt.",
          "Act": {
            "tool": "Qwen2audio_api",
            "arguments": {
              "prompt": "What kind of bird is singing in this audio?",
              "audio_file_path": "input/bird_song.mp3",
              "action": "chat",
              "output_file": "output/bird_chat.json"
            }
          }
        },
        {
          "task_description": "Continue a text-only conversation without sending a new audio file.",
          "Act": {
            "tool": "Qwen2audio_api",
            "arguments": {
              "prompt": "Thank you. Can you tell me more about that species?",
              "chatbot_history": "[...]",
              "action": "chat"
            }
          }
        },
        {
          "task_description": "Regenerate the last response from the assistant.",
          "Act": {
            "tool": "Qwen2audio_api",
            "arguments": {
              "chatbot_history": "[...]",
              "action": "regenerate"
            }
          }
        },
        {
          "task_description": "Reset the conversation and start fresh.",
          "Act": {
            "tool": "Qwen2audio_api",
            "arguments": {
              "action": "reset"
            }
          }
        }
      ]
    },
    "clearervoice_api": {
      "description": "A multi-task audio processing tool from the ClearerVoice Studio platform, capable of speech enhancement, separation, super-resolution, and audio-visual speaker extraction.",
      "detailed_description": "This tool encapsulates four core capabilities of the ClearerVoice Studio platform, allowing selection of different tasks via the 'task' parameter, which automatically calls the corresponding API endpoint. Supported tasks include: 1. 'enhancement' for noise reduction and speech enhancement with selectable models. 2. 'separation' to split audio into two tracks, like vocals and background. 3. 'super_resolution' to upscale low-sample-rate audio, with an option to apply enhancement. 4. 'av_extraction' to extract a speaker's audio or video from a video file.",
      "parameters": {
        "task": {
          "type": "string",
          "description": "The audio processing task to perform. Select from 'enhancement', 'separation', 'super_resolution', or 'av_extraction'.",
          "required": "optional",
          "default": "enhancement",
          "enum": [
            "enhancement",
            "separation",
            "super_resolution",
            "av_extraction"
          ],
          "example": "separation"
        },
        "input_path": {
          "type": "string",
          "description": "Path to the input audio or video file (e.g., WAV, MP3, MP4). Required for all tasks.",
          "required": true,
          "example": "input/meeting_recording.wav"
        },
        "model": {
          "type": "string",
          "description": "Specifies the model for the 'enhancement' task. Options: 'FRCRN_16000Hz' (low-compute, 16kHz), 'MossFormerGAN_16000Hz' (advanced GAN, 16kHz), 'MossFormer2_48000Hz' (flagship, 48kHz).",
          "required": "optional",
          "default": "MossFormer2_48000Hz",
          "enum": [
            "FRCRN_16000Hz",
            "MossFormerGAN_16000Hz",
            "MossFormer2_48000Hz"
          ],
          "example": "FRCRN_16000Hz"
        },
        "apply_se": {
          "type": "boolean",
          "description": "Used only for the 'super_resolution' task. If true, applies speech enhancement after upscaling.",
          "required": "optional",
          "default": true,
          "example": false
        },
        "output_audio_path": {
          "type": "string",
          "description": "Path to save the output audio for 'enhancement', 'super_resolution', or the first track of the 'separation' task.",
          "required": "optional",
          "default": "output.wav",
          "example": "output/enhanced_speech.wav"
        },
        "output_audio_path2": {
          "type": "string",
          "description": "Path to save the second output audio track for the 'separation' task (e.g., background noise).",
          "required": "optional",
          "default": "output_2.wav",
          "example": "output/background_track.wav"
        },
        "output_dir": {
          "type": "string",
          "description": "Directory to save the output files (images/videos) for the 'av_extraction' task.",
          "required": "optional",
          "default": "av_outputs",
          "example": "output/extracted_clips/"
        }
      },
      "examples": [
        {
          "task_description": "Enhance a noisy speech recording using the flagship model.",
          "Act": {
            "tool": "ClearerVoiceTool",
            "arguments": {
              "task": "enhancement",
              "input_path": "input/noisy_podcast.wav",
              "output_audio_path": "output/podcast_enhanced.wav",
              "model": "MossFormer2_48000Hz"
            }
          }
        },
        {
          "task_description": "Separate a speaker's voice from background music.",
          "Act": {
            "tool": "ClearerVoiceTool",
            "arguments": {
              "task": "separation",
              "input_path": "input/interview_with_music.wav",
              "output_audio_path": "output/interview_voice.wav",
              "output_audio_path2": "output/interview_background.wav"
            }
          }
        },
        {
          "task_description": "Upscale a low-quality recording and apply enhancement.",
          "Act": {
            "tool": "ClearerVoiceTool",
            "arguments": {
              "task": "super_resolution",
              "input_path": "input/old_recording_8khz.wav",
              "output_audio_path": "output/restored_recording_48khz.wav",
              "apply_se": true
            }
          }
        },
        {
          "task_description": "Extract the active speaker's video clips from a meeting recording.",
          "Act": {
            "tool": "ClearerVoiceTool",
            "arguments": {
              "task": "av_extraction",
              "input_path": "input/team_meeting.mp4",
              "output_dir": "output/meeting_speaker_clips"
            }
          }
        }
      ]
    },
    "diffrhythm_api": {
      "description": "DiffRhythm music generation and utility tool.",
      "detailed_description": "This tool encapsulates multiple endpoints of the DiffRhythm (ASLP-lab/DiffRhythm) web platform, providing a one-stop capability from theme/tag generation and lyric alignment to final instrumental music generation. It supports several distinct tasks: 'prompt_type' to query available prompt types; 'theme_tags' to generate timed LRC snippets from a theme and tags; 'lyrics_lrc' to convert raw lyrics into aligned LRC format; 'lambda_val' for querying internal parameters; and 'infer_music' as the primary function to generate music based on LRC files, text/audio prompts, and various diffusion parameters. The music generation task allows fine-tuning through parameters like seed, steps, CFG strength, output format, and even supports editing previously generated audio.",
      "parameters": {
        "task": {
          "type": "string",
          "description": "The specific task to perform. 'prompt_type': query prompt types. 'theme_tags': generate LRC from a theme. 'lyrics_lrc': convert lyrics to LRC. 'lambda_val': query internal lambda value. 'infer_music': generate music.",
          "required": "optional",
          "default": "infer_music",
          "enum": [
            "prompt_type",
            "theme_tags",
            "lyrics_lrc",
            "lambda_val",
            "infer_music"
          ]
        },
        "theme": {
          "type": "string",
          "description": "Theme description, used for the 'theme_tags' task.",
          "required": "optional",
          "example": "A quiet evening by the sea"
        },
        "tags_gen": {
          "type": "string",
          "description": "Comma-separated tags for generation, used for the 'theme_tags' task.",
          "required": "optional",
          "example": "soft piano, gentle waves, slow tempo"
        },
        "language": {
          "type": "string",
          "description": "Language for the 'theme_tags' task.",
          "required": "optional",
          "default": "en",
          "enum": [
            "en",
            "cn"
          ]
        },
        "tags_lyrics": {
          "type": "string",
          "description": "Tags associated with the lyrics, used for the 'lyrics_lrc' task.",
          "required": "optional",
          "example": "pop, upbeat"
        },
        "lyrics_input": {
          "type": "string",
          "description": "The raw lyric string (with line breaks) to be converted to LRC format, for the 'lyrics_lrc' task.",
          "required": "optional",
          "example": "Verse 1\nWalking down the lonely road\nChorus\nI see the sunshine"
        },
        "lrc": {
          "type": "string",
          "description": "Path to the aligned LRC file, required for the 'infer_music' task.",
          "required": "optional",
          "example": "input/lyrics/my_song.lrc"
        },
        "ref_audio_path": {
          "type": "string",
          "description": "Path to a reference audio file for style transfer in the 'infer_music' task.",
          "required": "optional",
          "example": "input/audio/reference_style.wav"
        },
        "text_prompt": {
          "type": "string",
          "description": "Text prompt describing the desired music style, required for the 'infer_music' task.",
          "required": "optional",
          "example": "A beautiful, soothing, and emotional piece of music, with a soft piano and strings, 80 bpm."
        },
        "seed": {
          "type": "number",
          "description": "Seed for the diffusion process to ensure reproducibility. Used in the 'infer_music' task.",
          "required": "optional",
          "default": 0
        },
        "randomize_seed": {
          "type": "boolean",
          "description": "If true, ignores the provided seed and uses a random one. Used in the 'infer_music' task.",
          "required": "optional",
          "default": true
        },
        "steps": {
          "type": "number",
          "description": "Number of inference steps for the diffusion model. Used in the 'infer_music' task.",
          "required": "optional",
          "default": 32
        },
        "cfg_strength": {
          "type": "number",
          "description": "Classifier-Free Guidance strength. Higher values adhere more strictly to the prompt. Used in the 'infer_music' task.",
          "required": "optional",
          "default": 4
        },
        "file_type": {
          "type": "string",
          "description": "The output format for the generated audio file. Used in the 'infer_music' task.",
          "required": "optional",
          "default": "mp3",
          "enum": [
            "mp3",
            "wav",
            "ogg"
          ]
        },
        "odeint_method": {
          "type": "string",
          "description": "The ODE solver method for the inference process. Used in the 'infer_music' task.",
          "required": "optional",
          "default": "euler",
          "enum": [
            "euler",
            "midpoint",
            "rk4",
            "implicit_adams"
          ]
        },
        "preference_infer": {
          "type": "string",
          "description": "Inference preference, trading off between speed and quality. Used in the 'infer_music' task.",
          "required": "optional",
          "default": "quality first",
          "enum": [
            "quality first",
            "speed first"
          ]
        },
        "edit": {
          "type": "boolean",
          "description": "Enable editing of a previously generated audio file. Used in the 'infer_music' task.",
          "required": "optional",
          "default": false
        },
        "edit_segments": {
          "type": "string",
          "description": "Specifies the segments to edit if 'edit' is true. Used in the 'infer_music' task.",
          "required": "optional",
          "default": null
        },
        "output_music_path": {
          "type": "string",
          "description": "Path to save the final generated music file. Used in the 'infer_music' task.",
          "required": "optional",
          "default": "diff_music_output.mp3",
          "example": "output/music/final_track.mp3"
        }
      },
      "examples": [
        {
          "task_description": "Generate a piece of music based on an LRC file and a text prompt.",
          "Act": {
            "tool": "diffrhythm_api",
            "arguments": {
              "task": "infer_music",
              "lrc": "path/to/your/lyrics.lrc",
              "text_prompt": "Epic orchestral music, fantasy, cinematic, powerful brass section, 120 bpm",
              "cfg_strength": 5,
              "steps": 40,
              "output_music_path": "output/epic_soundtrack.mp3"
            }
          }
        },
        {
          "task_description": "Generate an LRC timing file from a theme and descriptive tags in Chinese.",
          "Act": {
            "tool": "diffrhythm_api",
            "arguments": {
              "task": "theme_tags",
              "theme": "ËµõÂçöÊúãÂÖãÈÉΩÂ∏ÇÁöÑÈõ®Â§ú",
              "tags_gen": "ÁîµÂ≠ê, Ê∞õÂõ¥, ÊÖ¢ËäÇÂ•è, ÈúìËôπÁÅØ",
              "language": "cn"
            }
          }
        },
        {
          "task_description": "Convert a block of raw English lyrics into an LRC file with timing.",
          "Act": {
            "tool": "diffrhythm_api",
            "arguments": {
              "task": "lyrics_lrc",
              "tags_lyrics": "Acoustic, ballad, heartfelt",
              "lyrics_input": "The stars are out tonight\nI feel you by my side\nAnd everything's alright"
            }
          }
        },
        {
          "task_description": "Generate music with a reference audio for style, prioritizing inference speed.",
          "Act": {
            "tool": "diffrhythm_api",
            "arguments": {
              "task": "infer_music",
              "lrc": "path/to/song.lrc",
              "text_prompt": "Lo-fi hip hop beat, chill, relaxing",
              "ref_audio_path": "input/audio/lofi_style_ref.mp3",
              "preference_infer": "speed first",
              "file_type": "mp3",
              "output_music_path": "output/lofi_beat.mp3"
            }
          }
        }
      ]
    },
    "ACE_Step_api": {
      "description": "An integrated tool for end-to-end music generation, editing, and extension from text.",
      "detailed_description": "ACE-Step is an end-to-end web platform for text-to-music generation and subsequent audio fine-tuning, repainting, and extension. This tool encapsulates nine key endpoints, allowing users to complete the entire pipeline from scratch to multi-round editing using a single 'task' parameter and its corresponding arguments. Supported tasks include 'text2music' for initial generation, 'retake' for resampling, 'repaint' to redraw a specific time segment, 'edit' for modifying lyrics or style, and 'extend' to lengthen the audio. Tasks can be chained together; for example, the output of 'text2music' can be fed into 'repaint' or 'extend' for further modifications. The tool also provides utility functions like 'sample_data' to fetch hyperparameter templates and 'get_audio' to retrieve the audio from any stage of the process.",
      "parameters": {
        "task": {
          "type": "string",
          "description": "The primary operation to perform. Determines which endpoint is called and which other parameters are required.",
          "required": true,
          "default": "text2music",
          "enum": [
            "text2music",
            "retake",
            "repaint",
            "edit",
            "extend",
            "sample_data",
            "get_audio",
            "edit_type"
          ]
        },
        "input_json_1": {
          "type": [
            "object",
            "array",
            "string",
            "number",
            "boolean"
          ],
          "description": "The primary JSON data from a previous step. Required for 'retake', 'repaint', 'edit', and 'extend' tasks, where it usually represents the parameters of the music to be modified.",
          "required": "optional"
        },
        "input_json_2": {
          "type": [
            "object",
            "array",
            "string",
            "number",
            "boolean"
          ],
          "description": "The secondary JSON data for complex tasks. Required for 'repaint', 'edit', and 'extend' tasks.",
          "required": "optional"
        },
        "input_audio_path": {
          "type": "string",
          "description": "Path to an input audio file, used when a task requires an uploaded audio source (e.g., repaint, edit, extend from an upload).",
          "required": "optional"
        },
        "prompt": {
          "type": "string",
          "description": "Music style tags, separated by commas. Required for 'text2music' task. Example: 'pop, 120BPM, energetic'.",
          "required": "optional",
          "default": ""
        },
        "lyrics": {
          "type": "string",
          "description": "The lyrics for the music. Required for 'text2music' task. Can be multi-line text.",
          "required": "optional",
          "default": ""
        },
        "infer_step": {
          "type": "number",
          "description": "Number of topology steps. Higher values increase quality but are slower. Range: 1-100.",
          "required": "optional",
          "default": 27
        },
        "guidance_scale": {
          "type": "number",
          "description": "The CFG Scale for inference. Range: 1-30.",
          "required": "optional",
          "default": 15
        },
        "scheduler_type": {
          "type": "string",
          "description": "The type of sampler to use.",
          "required": "optional",
          "default": "euler",
          "enum": [
            "euler",
            "heun"
          ]
        },
        "cfg_type": {
          "type": "string",
          "description": "The CFG scheme to use.",
          "required": "optional",
          "default": "apg",
          "enum": [
            "cfg",
            "apg",
            "cfg_star"
          ]
        },
        "omega_scale": {
          "type": "number",
          "description": "Granularity control. Range: 0-20.",
          "required": "optional",
          "default": 10
        },
        "manual_seeds": {
          "type": "string",
          "description": "Manual seeds for reproducibility, in the format '42,17'. If null or empty, seeds are chosen randomly.",
          "required": "optional",
          "default": null
        },
        "guidance_interval": {
          "type": "number",
          "description": "CFG trigger interval. Range: 0-1.",
          "required": "optional",
          "default": 0.5
        },
        "guidance_interval_decay": {
          "type": "number",
          "description": "Decay for the guidance interval.",
          "required": "optional",
          "default": 0.0
        },
        "min_guidance_scale": {
          "type": "number",
          "description": "Minimum guidance scale.",
          "required": "optional",
          "default": 3
        },
        "use_erg_tag": {
          "type": "boolean",
          "description": "Enable the ERG enhancement module for tags.",
          "required": "optional",
          "default": true
        },
        "use_erg_lyric": {
          "type": "boolean",
          "description": "Enable the ERG enhancement module for lyrics.",
          "required": "optional",
          "default": true
        },
        "use_erg_diffusion": {
          "type": "boolean",
          "description": "Enable the ERG enhancement module for diffusion.",
          "required": "optional",
          "default": true
        },
        "oss_steps": {
          "type": "string",
          "description": "Comma-separated OSS stage steps, e.g., '0.2,0.6'. If null, uses default.",
          "required": "optional",
          "default": null
        },
        "guidance_scale_text": {
          "type": "number",
          "description": "Specific guidance scale for text.",
          "required": "optional",
          "default": 0.0
        },
        "guidance_scale_lyric": {
          "type": "number",
          "description": "Specific guidance scale for lyrics.",
          "required": "optional",
          "default": 0.0
        },
        "retake_variance": {
          "type": "number",
          "description": "Random perturbation amplitude for 'retake' and 'repaint' tasks. Range: 0-1.",
          "required": "optional",
          "default": 0.2
        },
        "retake_seeds": {
          "type": "string",
          "description": "Comma-separated seeds for resampling in 'retake' task. Required for 'retake'.",
          "required": "optional",
          "default": ""
        },
        "repaint_start": {
          "type": "number",
          "description": "Start time in seconds for the repaint region.",
          "required": "optional",
          "default": 0.0
        },
        "repaint_end": {
          "type": "number",
          "description": "End time in seconds for the repaint region.",
          "required": "optional",
          "default": 30.0
        },
        "repaint_source": {
          "type": "string",
          "description": "Source audio for the 'repaint' task.",
          "required": "optional",
          "default": "text2music",
          "enum": [
            "text2music",
            "last_repaint",
            "upload"
          ]
        },
        "edit_type": {
          "type": "string",
          "description": "The type of edit to perform in the 'edit' task.",
          "required": "optional",
          "default": "only_lyrics",
          "enum": [
            "only_lyrics",
            "remix"
          ]
        },
        "edit_prompt": {
          "type": "string",
          "description": "New style tags for the 'edit' task.",
          "required": "optional",
          "default": ""
        },
        "edit_lyrics": {
          "type": "string",
          "description": "New lyrics for the 'edit' task.",
          "required": "optional",
          "default": ""
        },
        "edit_n_min": {
          "type": "number",
          "description": "Minimum random range for 'remix' editing.",
          "required": "optional",
          "default": 0.6
        },
        "edit_n_max": {
          "type": "number",
          "description": "Maximum random range for 'remix' editing.",
          "required": "optional",
          "default": 1.0
        },
        "left_extend_length": {
          "type": "number",
          "description": "Duration in seconds to extend to the left for the 'extend' task.",
          "required": "optional",
          "default": 0.0
        },
        "right_extend_length": {
          "type": "number",
          "description": "Duration in seconds to extend to the right for the 'extend' task.",
          "required": "optional",
          "default": 30.0
        },
        "extend_source": {
          "type": "string",
          "description": "Source audio for the 'extend' or 'edit' task.",
          "required": "optional",
          "default": "text2music",
          "enum": [
            "text2music",
            "last_extend",
            "upload"
          ]
        },
        "extend_seeds": {
          "type": "string",
          "description": "Comma-separated seeds for the 'extend' task. Required for 'extend'.",
          "required": "optional",
          "default": ""
        },
        "lambda_stage": {
          "type": "string",
          "description": "Specifies which stage's source audio to retrieve for the 'get_audio' task.",
          "required": "optional",
          "default": "text2music",
          "enum": [
            "text2music",
            "last_repaint",
            "upload",
            "last_edit",
            "last_extend"
          ]
        },
        "output_audio_path": {
          "type": "string",
          "description": "Path to save the output audio file.",
          "required": "optional",
          "default": "ace_step_output.wav"
        },
        "output_json_path": {
          "type": "string",
          "description": "Path to save the output parameters JSON file.",
          "required": "optional",
          "default": "ace_step_params.json"
        }
      },
      "examples": [
        {
          "task_description": "Generate music from text and lyrics.",
          "Act": {
            "tool": "ACE_Step_api",
            "arguments": {
              "task": "text2music",
              "prompt": "lofi, chill, instrumental",
              "lyrics": "Verse 1: Coding through the late night hours.",
              "output_audio_path": "output/lofi_song.wav",
              "output_json_path": "output/lofi_song_params.json"
            }
          }
        },
        {
          "task_description": "Retake a previously generated music piece with new seeds for variation.",
          "Act": {
            "tool": "ACE_Step_api",
            "arguments": {
              "task": "retake",
              "input_json_1": "output/lofi_song_params.json",
              "retake_variance": 0.3,
              "retake_seeds": "123,456",
              "output_audio_path": "output/lofi_song_retake.wav",
              "output_json_path": "output/lofi_song_retake_params.json"
            }
          }
        },
        {
          "task_description": "Extend a music piece by 10 seconds to the right.",
          "Act": {
            "tool": "ACE_Step_api",
            "arguments": {
              "task": "extend",
              "input_json_1": "output/lofi_song_retake_params.json",
              "input_json_2": {},
              "right_extend_length": 10.0,
              "extend_seeds": "789,101",
              "extend_source": "text2music",
              "output_audio_path": "output/lofi_extended.wav",
              "output_json_path": "output/lofi_extended_params.json"
            }
          }
        },
        {
          "task_description": "Fetch hyperparameter templates for quick tuning.",
          "Act": {
            "tool": "ACE_Step_api",
            "arguments": {
              "task": "sample_data"
            }
          }
        }
      ]
    },
    "SenseVoice_api": {
      "description": "A speech understanding tool based on SenseVoice-Small for multi-task processing.",
      "detailed_description": "Based on the iic-sensevoice Space (endpoint /model_inference), this tool performs multiple speech-related tasks in a single call: 1. Automatic Speech Recognition (ASR), 2. Language Identification (LID), 3. Speech Emotion Recognition (SER), and 4. Acoustic Event Detection (AED). The model features ultra-low latency and is optimized for inputs of 30 seconds or less. It supports several languages, including Chinese (zh), English (en), Cantonese (yue), Japanese (ja), and Korean (ko), with an 'auto' option for automatic language detection. The output is a text file containing the transcribed text, detected events, and emotions.",
      "parameters": {
        "input_wav_path": {
          "type": "string",
          "description": "Path to the local input audio file. Both 16k and 44k sample rates are supported.",
          "required": true,
          "example": "input/audio/recording_to_transcribe.wav"
        },
        "language": {
          "type": "string",
          "description": "Language code for the input audio. 'auto' enables automatic detection.",
          "required": "optional",
          "default": "auto",
          "enum": [
            "auto",
            "zh",
            "en",
            "yue",
            "ja",
            "ko",
            "nospeech"
          ],
          "example": "en"
        },
        "output_txt_path": {
          "type": "string",
          "description": "Path to save the text file with the inference result. Example content: 'üéº[music] Hello, I'm in a good mood today. üòä'",
          "required": "optional",
          "default": "sensevoice_result.txt",
          "example": "output/transcription_result.txt"
        }
      },
      "examples": [
        {
          "task_description": "Transcribe an English audio file and save to a specific path",
          "Act": {
            "tool": "SenseVoiceTool",
            "arguments": {
              "input_wav_path": "path/to/english_speech.wav",
              "language": "en",
              "output_txt_path": "results/english_transcript.txt"
            }
          }
        },
        {
          "task_description": "Transcribe an audio file with auto language detection and default output path",
          "Act": {
            "tool": "SenseVoiceTool",
            "arguments": {
              "input_wav_path": "path/to/unknown_language.wav"
            }
          }
        },
        {
          "task_description": "Transcribe a Japanese audio file using the default output filename",
          "Act": {
            "tool": "SenseVoiceTool",
            "arguments": {
              "input_wav_path": "path/to/japanese_podcast.wav",
              "language": "ja"
            }
          }
        }
      ]
    },
    "whisper_large_v3_turbo_api": {
      "description": "Performs audio transcription or translation using the hf-audio/whisper-large-v3-turbo model.",
      "detailed_description": "This tool supports three input methods: local audio files, online audio file URLs, and YouTube video URLs. When 'audio_path' is provided, the /predict endpoint is called. When 'yt_url' is provided, the /predict_2 endpoint is called. The 'audio_path' and 'yt_url' parameters are mutually exclusive; please provide only one. Supported tasks include 'transcribe' (speech-to-text) and 'translate' (translates audio to English). The result is returned as transcribed or translated text. If 'output_path' is specified, the result will be saved to the file and the file path will be returned.",
      "parameters": {
        "audio_path": {
          "type": "string",
          "description": "Path to the local audio file or a directly accessible audio file URL. This is mutually exclusive with 'yt_url'.",
          "required": "optional",
          "example": "path/to/audio.wav"
        },
        "yt_url": {
          "type": "string",
          "description": "The URL of a YouTube video. This is mutually exclusive with 'audio_path'.",
          "required": "optional",
          "example": "https://www.youtube.com/watch?v=example"
        },
        "task": {
          "type": "string",
          "description": "The task to be performed. Can be 'transcribe' (default) or 'translate'.",
          "required": "optional",
          "default": "transcribe",
          "enum": [
            "transcribe",
            "translate"
          ],
          "example": "transcribe"
        },
        "output_path": {
          "type": "string",
          "description": "Optional path to save the result file. If provided, the function writes the result to this file and returns the path; otherwise, it returns the text result directly.",
          "required": "optional",
          "example": "output/result.txt"
        }
      },
      "examples": [
        {
          "task_description": "Transcribe a local audio file and save the result.",
          "Act": {
            "tool": "whisper_large_v3_turbo_api",
            "arguments": {
              "audio_path": "path/to/your/audio.mp3",
              "task": "transcribe",
              "output_path": "output/transcription.txt"
            }
          }
        },
        {
          "task_description": "Translate the audio from a YouTube video into English.",
          "Act": {
            "tool": "whisper_large_v3_turbo_api",
            "arguments": {
              "yt_url": "https://www.youtube.com/watch?v=example_video_id",
              "task": "translate"
            }
          }
        },
        {
          "task_description": "Transcribe an audio file from a URL and return the text directly.",
          "Act": {
            "tool": "whisper_large_v3_turbo_api",
            "arguments": {
              "audio_path": "https://example.com/audio.mp3"
            }
          }
        }
      ]
    },
    "tiger_api": {
      "description": "TIGER audio extraction tool, capable of separating audio tracks from audio or video files.",
      "detailed_description": "This tool utilizes the TIGER model to perform various audio separation tasks. Supported API endpoints/tasks: '/separate_dnr' separates dialogue, sound effects, and music from audio files. '/separate_speakers' separates up to 4 speakers from an audio file. '/separate_dnr_video' separates dialogue, sound effects, and music from a video file and returns the separated videos. '/separate_speakers_video' separates up to 4 speakers from a video file and returns the separated videos. The tool accepts a path to a single audio or video file and returns a list containing the paths of all output files.",
      "parameters": {
        "input_file_path": {
          "type": "string",
          "description": "Path to the input audio or video file.",
          "required": true,
          "example": "path/to/audio.wav"
        },
        "task": {
          "type": "string",
          "description": "The task to be performed. Must be one of the four valid API endpoints.",
          "required": true,
          "enum": [
            "/separate_dnr",
            "/separate_speakers",
            "/separate_dnr_video",
            "/separate_speakers_video"
          ],
          "example": "/separate_speakers"
        },
        "output_dir": {
          "type": "string",
          "description": "The directory path to save the output files.",
          "required": "optional",
          "default": "output",
          "example": "results/speakers"
        }
      },
      "examples": [
        {
          "task_description": "Separate up to 4 speakers from an audio file.",
          "Act": {
            "tool": "tiger_audio_extraction_api",
            "arguments": {
              "input_file_path": "path/to/audio.wav",
              "task": "/separate_speakers",
              "output_dir": "results/speakers"
            }
          }
        },
        {
          "task_description": "Separate dialogue, sound effects, and music from a video file.",
          "Act": {
            "tool": "tiger_audio_extraction_api",
            "arguments": {
              "input_file_path": "path/to/video.mp4",
              "task": "/separate_dnr_video",
              "output_dir": "results/dnr"
            }
          }
        },
        {
          "task_description": "Separate dialogue, sound effects, and music from an audio file.",
          "Act": {
            "tool": "tiger_audio_extraction_api",
            "arguments": {
              "input_file_path": "path/to/meeting.mp3",
              "task": "/separate_dnr"
            }
          }
        }
      ]
    },
    "audio_super_resolution_api": {
      "description": "Uses the Nick088/Audio-SR model for audio super-resolution.",
      "detailed_description": "This tool enhances the quality of an audio file by increasing its resolution. It uses a pre-trained model to generate a higher-quality version of the input audio. Supported models: 'basic', 'speech'. Input: Path to a local audio file (e.g., .wav, .mp3). Returns: The file path of the enhanced output audio.",
      "parameters": {
        "audio_file_path": {
          "type": "string",
          "description": "Required. The path to the input audio file to be enhanced.",
          "required": true,
          "example": "input/audio/low_quality.wav"
        },
        "output_path": {
          "type": "string",
          "description": "Required. The path to save the resulting enhanced audio file.",
          "required": true,
          "example": "output/enhanced_audio.wav"
        },
        "model_name": {
          "type": "string",
          "description": "The model to use. Options are 'basic' or 'speech'.",
          "required": "optional",
          "default": "basic",
          "enum": [
            "basic",
            "speech"
          ],
          "example": "speech"
        },
        "guidance_scale": {
          "type": "float",
          "description": "The scale used to guide the generation process.",
          "required": "optional",
          "default": 3.5,
          "example": 4.0
        },
        "ddim_steps": {
          "type": "integer",
          "description": "Number of DDIM diffusion model steps.",
          "required": "optional",
          "default": 50,
          "example": 75
        },
        "seed": {
          "type": "integer",
          "description": "Random seed for reproducing results.",
          "required": "optional",
          "default": 42,
          "example": 123
        }
      },
      "examples": [
        {
          "task_description": "Enhance a speech recording using the 'speech' model and custom settings.",
          "Act": {
            "tool": "audio_super_resolution_api",
            "arguments": {
              "audio_file_path": "input/conference_call.mp3",
              "output_path": "output/enhanced_conference_call.wav",
              "model_name": "speech",
              "guidance_scale": 4.0,
              "ddim_steps": 75,
              "seed": 123
            }
          }
        },
        {
          "task_description": "Enhance a general audio file using default optional parameters.",
          "Act": {
            "tool": "audio_super_resolution_api",
            "arguments": {
              "audio_file_path": "input/music_demo.wav",
              "output_path": "output/enhanced_music_demo.wav",
              "model_name": "basic"
            }
          }
        },
        {
          "task_description": "Upscale an audio file with only the required parameters specified.",
          "Act": {
            "tool": "audio_super_resolution_api",
            "arguments": {
              "audio_file_path": "path/to/my_audio.wav",
              "output_path": "path/to/my_enhanced_audio.wav"
            }
          }
        }
      ]
    },
    "index_tts_1.5_api": {
      "description": "A Text-to-Speech (TTS) tool that clones a voice from a reference audio to generate speech for the target text.",
      "detailed_description": "This tool provides Text-to-Speech (TTS) functionality by cloning a voice from a reference audio file to synthesize speech for a given text. It utilizes the '/gen_single' API endpoint. The process involves providing a path to a reference audio (e.g., .wav, .mp3) and the text to be converted. The tool then generates a new audio file with the voice from the reference audio speaking the provided text. The final path of the successfully generated audio file is returned.",
      "parameters": {
        "prompt_audio_path": {
          "type": "string",
          "description": "The file path to the reference audio for voice cloning. Supports formats like .wav, .mp3, etc.",
          "required": true,
          "example": "my_reference_voice.wav"
        },
        "target_text": {
          "type": "string",
          "description": "The target text to be converted into speech.",
          "required": true,
          "example": "Hello, welcome to this speech synthesis tool!"
        },
        "output_path": {
          "type": "string",
          "description": "The path to save the generated audio file.",
          "required": "optional",
          "default": "generated_audio.wav",
          "example": "output/result.wav"
        }
      },
      "examples": [
        {
          "task_description": "Generate speech by cloning a voice from a reference audio and saving it to a specific output path.",
          "Act": {
            "tool": "index_tts_1.5_api",
            "arguments": {
              "prompt_audio_path": "input/reference_voice.wav",
              "target_text": "Hello, this is a demonstration of voice cloning.",
              "output_path": "output/cloned_voice_audio.wav"
            }
          }
        },
        {
          "task_description": "Use a reference audio to generate speech for a target text, using the default output path.",
          "Act": {
            "tool": "index_tts_1.5_api",
            "arguments": {
              "prompt_audio_path": "samples/my_voice.mp3",
              "target_text": "This tool makes it easy to generate speech in any voice."
            }
          }
        }
      ]
    },
    "audiocraft_jasco_api": {
      "description": "Audiocraft Music Generation Tool",
      "detailed_description": "Calls the /predict_full API endpoint of Tonic/audiocraft to generate music based on text, chords, melody, and drums. Supported input types: text descriptions, chord progression strings, melody audio files, and drum audio files. Returns a tuple containing the local saved paths of the two generated audio files (Jasco Stem 1, Jasco Stem 2). Note: Melody and drum files are optional, but their use depends on the selected model. For example, models containing 'melody' in their name require melody_file_path.",
      "parameters": {
        "model": {
          "type": "string",
          "description": "The name of the model to use. Available options: 'facebook/jasco-chords-drums-400M', 'facebook/jasco-chords-drums-1B', 'facebook/jasco-chords-drums-melody-400M', 'facebook/jasco-chords-drums-melody-1B'.",
          "required": "optional",
          "default": "facebook/jasco-chords-drums-melody-400M",
          "enum": [
            "facebook/jasco-chords-drums-400M",
            "facebook/jasco-chords-drums-1B",
            "facebook/jasco-chords-drums-melody-400M",
            "facebook/jasco-chords-drums-melody-1B"
          ]
        },
        "text": {
          "type": "string",
          "description": "Text prompt describing the music style, instruments, etc.",
          "required": "optional",
          "default": "Strings, woodwind, orchestral, symphony."
        },
        "chords_sym": {
          "type": "string",
          "description": "Chord progression string in the format `(CHORD, START_TIME_IN_SECONDS)`.",
          "required": "optional",
          "default": "(C, 0.0), (D, 2.0), (F, 4.0), (Ab, 6.0), (Bb, 7.0), (C, 8.0)"
        },
        "melody_file_path": {
          "type": "string",
          "description": "Local path to the melody reference audio file. Required for models that use melody.",
          "required": "optional",
          "default": ""
        },
        "drums_file_path": {
          "type": "string",
          "description": "Local path to the drums reference audio file. Used when `drum_input_src` is 'file'.",
          "required": "optional",
          "default": ""
        },
        "drums_mic_path": {
          "type": "string",
          "description": "Local path to the drum audio file recorded via microphone. Used when `drum_input_src` is 'mic'.",
          "required": "optional",
          "default": ""
        },
        "drum_input_src": {
          "type": "string",
          "description": "The source of the drum input. Available options: 'file', 'mic'.",
          "required": "optional",
          "default": "file",
          "enum": [
            "file",
            "mic"
          ]
        },
        "cfg_coef_all": {
          "type": "number",
          "description": "Global coefficient for Classifier-Free Guidance (CFG).",
          "required": "optional",
          "default": 1.25
        },
        "cfg_coef_txt": {
          "type": "number",
          "description": "CFG coefficient for the text condition.",
          "required": "optional",
          "default": 2.5
        },
        "ode_rtol": {
          "type": "number",
          "description": "Relative tolerance for the ODE solver.",
          "required": "optional",
          "default": 0.0001
        },
        "ode_atol": {
          "type": "number",
          "description": "Absolute tolerance for the ODE solver.",
          "required": "optional",
          "default": 0.0001
        },
        "ode_solver": {
          "type": "string",
          "description": "Type of ODE solver. Available options: 'euler', 'dopri5'.",
          "required": "optional",
          "default": "euler",
          "enum": [
            "euler",
            "dopri5"
          ]
        },
        "ode_steps": {
          "type": "number",
          "description": "Number of steps for the 'euler' solver.",
          "required": "optional",
          "default": 10
        },
        "output_dir": {
          "type": "string",
          "description": "Directory path to save the generated audio files. Defaults to 'output_audio'.",
          "required": "optional",
          "default": "output_audio"
        }
      },
      "examples": [
        {
          "task_description": "Generate music using text and chords",
          "Act": {
            "tool": "audiocraft_jasco_api",
            "arguments": {
              "model": "facebook/jasco-chords-drums-400M",
              "text": "Acoustic folk song with a gentle guitar and a simple beat.",
              "chords_sym": "(G, 0.0), (C, 4.0), (G, 8.0), (D, 12.0)",
              "output_dir": "generated_music"
            }
          }
        },
        {
          "task_description": "Generate music with melody and drums from files",
          "Act": {
            "tool": "audiocraft_jasco_api",
            "arguments": {
              "model": "facebook/jasco-chords-drums-melody-400M",
              "text": "Upbeat pop track with a catchy synth melody.",
              "chords_sym": "(Am, 0.0), (F, 2.0), (C, 4.0), (G, 6.0)",
              "melody_file_path": "path/to/your/melody.wav",
              "drums_file_path": "path/to/your/drums.wav",
              "drum_input_src": "file",
              "output_dir": "pop_track"
            }
          }
        }
      ]
    },
    "step_audio_tts_3b_api": {
      "description": "Voice cloning tool: Use a reference audio to clone its timbre and generate new speech.",
      "detailed_description": "This tool performs voice cloning text-to-speech (TTS) by using a reference audio to clone its timbre and generate new speech from the provided text. It utilizes the /generate_clone API endpoint. The inputs required are the target text for synthesis, a reference audio file for timbre cloning, and the corresponding text of the reference audio. The tool returns the file path of the generated audio.",
      "parameters": {
        "text": {
          "type": "string",
          "description": "The target text to be converted into speech.",
          "required": true,
          "example": "Hello, welcome to use this voice cloning tool."
        },
        "prompt_audio": {
          "type": "string",
          "description": "Path to the reference audio file used for cloning the timbre. Supports common audio formats like WAV, MP3, etc.",
          "required": true,
          "example": "path/to/sample.wav"
        },
        "prompt_text": {
          "type": "string",
          "description": "The text content corresponding to the 'prompt_audio' reference file.",
          "required": true,
          "example": "This is the text of the reference audio."
        },
        "output_path": {
          "type": "string",
          "description": "Path to save the generated audio file.",
          "required": "optional",
          "default": "generated_clone_audio.wav",
          "example": "output/cloned_speech.wav"
        }
      },
      "examples": [
        {
          "task_description": "Clone a voice from a reference audio and text to generate new speech, saving to a specified output path.",
          "Act": {
            "tool": "VoiceCloneTTS",
            "arguments": {
              "text": "Hello, welcome to use this voice cloning tool.",
              "prompt_audio": "path/to/sample.wav",
              "prompt_text": "This is the text of the reference audio.",
              "output_path": "output/cloned_speech.wav"
            }
          }
        }
      ]
    },
    "sparkTTS_tool_api": {
      "description": "A tool for text-to-speech, supporting voice cloning and custom voice creation.",
      "detailed_description": "The SparkTTS tool provides text-to-speech functionality with two main tasks. The 'voice_clone' task allows cloning a voice from an audio sample to read the given text. The 'voice_creation' task generates a custom voice based on specified gender, pitch, and speed parameters to read the text. For the 'voice_clone' task, the 'text', 'prompt_text', and 'prompt_audio_path' are required. For the 'voice_creation' task, 'text' is required, while 'gender', 'pitch', and 'speed' have default values. The function returns the path to the generated audio file upon successful execution.",
      "parameters": {
        "task": {
          "type": "string",
          "description": "The task to be performed, either 'voice_clone' or 'voice_creation'.",
          "required": true,
          "enum": [
            "voice_clone",
            "voice_creation"
          ],
          "example": "voice_clone"
        },
        "text": {
          "type": "string",
          "description": "The input text to be converted to speech.",
          "required": true,
          "example": "This is the cloned voice."
        },
        "output_path": {
          "type": "string",
          "description": "The path to save the generated audio file.",
          "required": true,
          "example": "./clone_output.wav"
        },
        "prompt_text": {
          "type": "string",
          "description": "The text corresponding to the prompt audio for voice cloning. Required when task is 'voice_clone'.",
          "required": "optional",
          "example": "This is the prompt audio's text."
        },
        "prompt_audio_path": {
          "type": "string",
          "description": "The path to the prompt audio file (e.g., .wav) for voice cloning. A sample rate of at least 16kHz is recommended. Required when task is 'voice_clone'.",
          "required": "optional",
          "example": "./sample.wav"
        },
        "gender": {
          "type": "string",
          "description": "The gender of the generated voice, can be 'male' or 'female'. Used only when task is 'voice_creation'.",
          "required": "optional",
          "default": "male",
          "enum": [
            "male",
            "female"
          ],
          "example": "female"
        },
        "pitch": {
          "type": "float",
          "description": "The pitch of the generated voice. Used only when task is 'voice_creation'.",
          "required": "optional",
          "default": 3.0,
          "example": 4.0
        },
        "speed": {
          "type": "float",
          "description": "The speed of the generated voice. Used only when task is 'voice_creation'.",
          "required": "optional",
          "default": 3.0,
          "example": 2.0
        }
      },
      "examples": [
        {
          "task_description": "Clone a voice using a prompt audio and text",
          "Act": {
            "tool": "SparkTTS_tool",
            "arguments": {
              "task": "voice_clone",
              "text": "This is the cloned voice.",
              "prompt_text": "This is the prompt audio's text.",
              "prompt_audio_path": "./sample.wav",
              "output_path": "./clone_output.wav"
            }
          }
        },
        {
          "task_description": "Create a custom female voice with specific pitch and speed",
          "Act": {
            "tool": "SparkTTS_tool",
            "arguments": {
              "task": "voice_creation",
              "text": "This is a custom voice.",
              "gender": "female",
              "pitch": 4.0,
              "speed": 2.0,
              "output_path": "./creation_output.wav"
            }
          }
        }
      ]
    },
    "yue_api": {
      "description": "YuE Music Generation Tool.",
      "detailed_description": "Calls the 'innova-ai/YuE-music-generator-demo' API on Hugging Face Space to generate music. Supports generation by specifying music genre, lyrics, or by providing an audio file as a prompt. The API returns three audio files: a mixed final track, a vocals-only track, and an instrumental-only track.",
      "parameters": {
        "genre_txt": {
          "type": "string",
          "description": "Text description of the music genre, e.g., 'Pop' or 'Lyrical Folk'.",
          "required": "optional",
          "example": "Pop"
        },
        "lyrics_txt": {
          "type": "string",
          "description": "The lyrics text for the music.",
          "required": "optional",
          "example": "Verse 1: The sun shines bright."
        },
        "num_segments": {
          "type": "integer",
          "description": "The number of music segments to generate.",
          "required": "optional",
          "default": 2,
          "example": 4
        },
        "duration": {
          "type": "integer",
          "description": "The duration of the generated song in seconds.",
          "required": "optional",
          "default": 30,
          "example": 60
        },
        "use_audio_prompt": {
          "type": "boolean",
          "description": "Whether to use the provided audio file as a generation prompt.",
          "required": "optional",
          "default": false,
          "example": true
        },
        "audio_prompt_path": {
          "type": "string",
          "description": "Path (local or URL) to the audio file to be used as a prompt. This parameter is required if use_audio_prompt is True.",
          "required": "optional",
          "example": "input/audio/my_prompt.wav"
        },
        "output_dir": {
          "type": "string",
          "description": "Directory path to save the three final generated audio files.",
          "required": "optional",
          "default": "yue_music_output",
          "example": "output/my_generated_music"
        }
      },
      "examples": [
        {
          "task_description": "Generate a 60-second pop song with specified lyrics and save it to a custom directory.",
          "Act": {
            "tool": "yue_api",
            "arguments": {
              "genre_txt": "Pop",
              "lyrics_txt": "Oh, dancing in the moonlight, feeling so free tonight.",
              "duration": 60,
              "output_dir": "output/pop_song_project"
            }
          }
        },
        {
          "task_description": "Generate new music based on an existing audio file as a prompt.",
          "Act": {
            "tool": "yue_api",
            "arguments": {
              "use_audio_prompt": true,
              "audio_prompt_path": "input/prompts/melody_idea.wav",
              "num_segments": 4,
              "duration": 45
            }
          }
        },
        {
          "task_description": "Generate a short instrumental piece by only specifying the genre.",
          "Act": {
            "tool": "yue_api",
            "arguments": {
              "genre_txt": "Lofi hip hop",
              "duration": 30
            }
          }
        }
      ]
    },
    "voicecraft_tts_and_edit_api": {
      "description": "Performs text-to-speech (TTS), audio editing, and long-form text synthesis using the VoiceCraft model.",
      "detailed_description": "Provides three modes of operation: 'TTS' for generating speech from text (with optional voice cloning from a reference audio), 'Edit' for replacing a segment of an audio file with new speech, and 'Long TTS' for synthesizing long-form text by splitting it into sentences and generating them sequentially. The tool requires 'mode' and 'transcript' for all operations. An 'audio_path' is necessary for 'Edit' and 'Long TTS' modes. For 'Long TTS', the 'selected_sentence' to be synthesized is also required. It returns a dictionary containing the path to the generated audio file and the inferred transcript.",
      "parameters": {
        "mode": {
          "type": "string",
          "description": "The operating mode. 'TTS' for text-to-speech, 'Edit' for audio editing, 'Long TTS' for long text synthesis. Required.",
          "required": true,
          "enum": [
            "TTS",
            "Edit",
            "Long TTS"
          ],
          "example": "Edit"
        },
        "transcript": {
          "type": "string",
          "description": "The text to be synthesized or used for editing. Required.",
          "required": true,
          "example": "The quick brown fox jumps over the lazy dog."
        },
        "audio_path": {
          "type": "string",
          "description": "Path to the input audio file. Required for 'Edit' and 'Long TTS' modes.",
          "required": "optional",
          "example": "input/original_audio.wav"
        },
        "output_path": {
          "type": "string",
          "description": "Path to save the generated audio file.",
          "required": "optional",
          "default": "output.wav",
          "example": "output/generated_speech.wav"
        },
        "seed": {
          "type": "integer",
          "description": "Random seed for reproducibility. Use -1 for a random seed.",
          "required": "optional",
          "default": -1,
          "example": 42
        },
        "smart_transcript": {
          "type": "boolean",
          "description": "Whether to enable smart transcription.",
          "required": "optional",
          "default": true,
          "example": true
        },
        "prompt_end_time": {
          "type": "number",
          "description": "In 'Edit' mode, the end time of the audio prompt.",
          "required": "optional",
          "default": 3.675,
          "example": 3.5
        },
        "edit_start_time": {
          "type": "number",
          "description": "In 'Edit' mode, the start time of the segment to be edited.",
          "required": "optional",
          "default": 3.83,
          "example": 4.0
        },
        "edit_end_time": {
          "type": "number",
          "description": "In 'Edit' mode, the end time of the segment to be edited.",
          "required": "optional",
          "default": 5.113,
          "example": 5.0
        },
        "left_margin": {
          "type": "number",
          "description": "Left margin for the audio processing.",
          "required": "optional",
          "default": 0.08,
          "example": 0.1
        },
        "right_margin": {
          "type": "number",
          "description": "Right margin for the audio processing.",
          "required": "optional",
          "default": 0.08,
          "example": 0.1
        },
        "temperature": {
          "type": "number",
          "description": "Controls the randomness of the generation. Higher values mean more randomness.",
          "required": "optional",
          "default": 1.0,
          "example": 0.95
        },
        "top_p": {
          "type": "number",
          "description": "Nucleus sampling threshold.",
          "required": "optional",
          "default": 0.9,
          "example": 0.9
        },
        "top_k": {
          "type": "integer",
          "description": "Top-k sampling. Set to 0 to disable.",
          "required": "optional",
          "default": 0,
          "example": 5
        },
        "sample_batch_size": {
          "type": "integer",
          "description": "Sample batch size, which can affect the speech rate.",
          "required": "optional",
          "default": 2,
          "example": 2
        },
        "stop_repetition": {
          "type": "string",
          "description": "Level for stopping repetition.",
          "required": "optional",
          "default": "3",
          "enum": [
            "-1",
            "1",
            "2",
            "3",
            "4"
          ],
          "example": "3"
        },
        "kvcache": {
          "type": "string",
          "description": "Whether to use KV caching. '1' for yes, '0' for no.",
          "required": "optional",
          "default": "1",
          "enum": [
            "0",
            "1"
          ],
          "example": "1"
        },
        "split_text": {
          "type": "string",
          "description": "In 'Long TTS' mode, how to split the text.",
          "required": "optional",
          "default": "Newline",
          "enum": [
            "Newline",
            "Sentence"
          ],
          "example": "Sentence"
        },
        "selected_sentence": {
          "type": "string",
          "description": "In 'Long TTS' mode, the specific sentence to process. Required for this mode.",
          "required": "optional",
          "example": "This is the sentence to be synthesized."
        },
        "codec_audio_sr": {
          "type": "integer",
          "description": "Codec audio sample rate.",
          "required": "optional",
          "default": 16000,
          "example": 16000
        },
        "codec_sr": {
          "type": "integer",
          "description": "Codec sample rate.",
          "required": "optional",
          "default": 50,
          "example": 50
        },
        "silence_tokens": {
          "type": "string",
          "description": "A string representing the list of silence tokens.",
          "required": "optional",
          "default": "[1388,1898,131]",
          "example": "[1388,1898,131]"
        }
      },
      "examples": [
        {
          "task_description": "Generate speech from a piece of text using the TTS mode.",
          "Act": {
            "tool": "VoiceCraftTool",
            "arguments": {
              "mode": "TTS",
              "transcript": "Hello world, this is a demonstration of text-to-speech.",
              "output_path": "output/hello_world.wav"
            }
          }
        },
        {
          "task_description": "Edit a specific portion of an existing audio file.",
          "Act": {
            "tool": "VoiceCraftTool",
            "arguments": {
              "mode": "Edit",
              "transcript": "The quick brown fox jumps over the lazy cat.",
              "audio_path": "input/original_speech.wav",
              "edit_start_time": 3.83,
              "edit_end_time": 5.113,
              "output_path": "output/edited_speech.wav"
            }
          }
        },
        {
          "task_description": "Generate a single sentence as part of a long-form text synthesis.",
          "Act": {
            "tool": "VoiceCraftTool",
            "arguments": {
              "mode": "Long TTS",
              "transcript": "This is the first sentence. This is the second sentence which we will generate now.",
              "audio_path": "input/long_tts_prompt.wav",
              "output_path": "output/long_tts_sentence.wav",
              "split_text": "Sentence",
              "selected_sentence": "This is the second sentence which we will generate now."
            }
          }
        }
      ]
    }
  }
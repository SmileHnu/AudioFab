{
    "load_audio": {
      "description": "加载音频数据。",
      "detailed_description": "该工具用于加载音频文件并可选择性地重采样到指定采样率。它会将加载的音频保存为临时文件并返回相关信息。",
      "parameters": {
        "audio_path": {
          "type": "string",
          "description": "音频文件路径",
          "required": true
        },
        "target_sr": {
          "type": "integer",
          "description": "目标采样率（可选）",
          "required": "optional"
        }
      },
      "examples": [
        {
          "task_description": "加载音频文件",
          "Act": {
            "tool": "load_audio",
            "arguments": {
              "audio_path": "input/my_audio.wav"
            }
          }
        },
        {
          "task_description": "加载音频并重采样到16kHz",
          "Act": {
            "tool": "load_audio",
            "arguments": {
              "audio_path": "input/my_audio.wav",
              "target_sr": 16000
            }
          }
        }
      ]
    },
    "resample_audio": {
      "description": "重采样音频。",
      "detailed_description": "该工具用于将音频从一个采样率转换到另一个采样率。它会保存重采样后的音频并返回相关信息。",
      "parameters": {
        "audio_path": {
          "type": "string",
          "description": "音频文件路径",
          "required": true
        },
        "orig_sr": {
          "type": "integer",
          "description": "原始采样率",
          "required": true
        },
        "target_sr": {
          "type": "integer",
          "description": "目标采样率",
          "required": true
        }
      },
      "examples": [
        {
          "task_description": "将44.1kHz音频重采样到16kHz",
          "Act": {
            "tool": "resample_audio",
            "arguments": {
              "audio_path": "output/audio/processed_20231215_123456.wav",
              "orig_sr": 44100,
              "target_sr": 16000
            }
          }
        }
      ]
    },
    "compute_stft": {
      "description": "计算短时傅里叶变换。",
      "detailed_description": "该工具计算音频信号的短时傅里叶变换（STFT），并将结果保存为NumPy数组文件。",
      "parameters": {
        "audio_path": {
          "type": "string",
          "description": "音频文件路径",
          "required": true
        },
        "n_fft": {
          "type": "integer",
          "description": "FFT窗口大小",
          "required": "optional",
          "default": 2048
        },
        "hop_length": {
          "type": "integer",
          "description": "帧移",
          "required": "optional",
          "default": 512
        }
      },
      "examples": [
        {
          "task_description": "计算音频的STFT",
          "Act": {
            "tool": "compute_stft",
            "arguments": {
              "audio_path": "output/audio/processed_20231215_123456.wav",
              "n_fft": 2048,
              "hop_length": 512
            }
          }
        }
      ]
    },
    "compute_mfcc": {
      "description": "计算MFCC特征。",
      "detailed_description": "该工具计算音频的梅尔频率倒谱系数（MFCC）特征，这是一种常用于语音识别和音频分析的特征。",
      "parameters": {
        "audio_path": {
          "type": "string",
          "description": "音频文件路径",
          "required": true
        },
        "n_mfcc": {
          "type": "integer",
          "description": "MFCC系数数量",
          "required": "optional",
          "default": 13
        }
      },
      "examples": [
        {
          "task_description": "计算音频的MFCC特征",
          "Act": {
            "tool": "compute_mfcc",
            "arguments": {
              "audio_path": "output/audio/processed_20231215_123456.wav",
              "n_mfcc": 13
            }
          }
        }
      ]
    },
    "compute_mel_spectrogram": {
      "description": "计算梅尔频谱图并生成可视化图像。",
      "detailed_description": "该工具计算音频的梅尔频谱图，一种基于人类听觉感知的频谱表示，并可以选择性地生成可视化图像。在audio_processor中生成可视化，在dsp_processor中只生成数据文件。",
      "parameters": {
        "audio_path": {
          "type": "string",
          "description": "音频文件路径",
          "required": true
        },
        "n_mels": {
          "type": "integer",
          "description": "梅尔滤波器组数量",
          "required": "optional",
          "default": 128
        }
      },
      "examples": [
        {
          "task_description": "计算并可视化音频的梅尔频谱图",
          "Act": {
            "tool": "compute_mel_spectrogram",
            "arguments": {
              "audio_path": "output/audio/processed_20231215_123456.wav",
              "n_mels": 128
            }
          }
        }
      ]
    },
    "add_reverb": {
      "description": "添加混响效果。",
      "detailed_description": "该工具对音频添加混响效果，可用于模拟不同空间声学环境下的声音。",
      "parameters": {
        "audio_path": {
          "type": "string",
          "description": "音频文件路径",
          "required": true
        },
        "room_scale": {
          "type": "float",
          "description": "房间大小因子（0-1）",
          "required": "optional",
          "default": 0.8
        }
      },
      "examples": [
        {
          "task_description": "为音频添加混响效果",
          "Act": {
            "tool": "add_reverb",
            "arguments": {
              "audio_path": "output/audio/processed_20231215_123456.wav",
              "room_scale": 0.8
            }
          }
        }
      ]
    },
    "mix_audio": {
      "description": "混合多个音频。",
      "detailed_description": "该工具将多个音频文件混合成一个单一的音频文件，可以指定每个音频的混合权重。",
      "parameters": {
        "audio_paths": {
          "type": "array",
          "description": "音频文件路径列表",
          "required": true
        },
        "weights": {
          "type": "array",
          "description": "混合权重列表（可选）",
          "required": "optional"
        }
      },
      "examples": [
        {
          "task_description": "混合两个音频文件",
          "Act": {
            "tool": "mix_audio",
            "arguments": {
              "audio_paths": [
                "output/audio/processed_1.wav",
                "output/audio/processed_2.wav"
              ],
              "weights": [
                0.6,
                0.4
              ]
            }
          }
        }
      ]
    },
    "apply_fade": {
      "description": "应用淡入淡出效果。",
      "detailed_description": "该工具对音频应用淡入淡出效果，可以使音频的开始和结束更平滑。",
      "parameters": {
        "audio_path": {
          "type": "string",
          "description": "音频文件路径",
          "required": true
        },
        "fade_duration": {
          "type": "float",
          "description": "淡入淡出时长（秒）",
          "required": true
        }
      },
      "examples": [
        {
          "task_description": "为音频添加1秒的淡入淡出效果",
          "Act": {
            "tool": "apply_fade",
            "arguments": {
              "audio_path": "output/audio/processed_20231215_123456.wav",
              "fade_duration": 1.0
            }
          }
        }
      ]
    },
    "serve_local_audio": {
      "description": "将本地音频文件转换为可访问的URL。",
      "detailed_description": "该工具启动一个本地HTTP服务器，使音频文件可以通过网络URL访问，便于在网页或其他应用中播放。",
      "parameters": {
        "audio_path": {
          "type": "string",
          "description": "本地音频文件路径",
          "required": true
        },
        "port": {
          "type": "integer",
          "description": "HTTP服务器端口号",
          "required": "optional",
          "default": 8000
        }
      },
      "examples": [
        {
          "task_description": "将处理后的音频文件通过HTTP服务器提供访问",
          "Act": {
            "tool": "serve_local_audio",
            "arguments": {
              "audio_path": "output/audio/processed_20231215_123456.wav",
              "port": 8080
            }
          }
        }
      ]
    },
    "stop_audio_server": {
      "description": "停止音频文件上传服务器，并释放资源。",
      "detailed_description": "该工具停止由serve_local_audio启动的HTTP服务器，并清理相关的临时文件。",
      "parameters": {},
      "examples": [
        {
          "task_description": "停止正在运行的音频服务器",
          "Act": {
            "tool": "stop_audio_server",
            "arguments": {}
          }
        }
      ]
    },
    "convert_audio_format": {
      "description": "将音频从一种格式转换为另一种格式。",
      "detailed_description": "该工具支持多种音频格式之间的转换，如WAV、MP3、OGG、FLAC等，同时可以调整采样率、通道数和位深度。",
      "parameters": {
        "input_path": {
          "type": "string",
          "description": "输入音频文件路径",
          "required": true
        },
        "output_format": {
          "type": "string",
          "description": "输出格式（如 'wav', 'mp3', 'ogg', 'flac' 等）",
          "required": "optional",
          "default": "wav"
        },
        "sample_rate": {
          "type": "integer",
          "description": "目标采样率（可选）",
          "required": "optional"
        },
        "channels": {
          "type": "integer",
          "description": "目标通道数（可选，1=单声道，2=立体声）",
          "required": "optional"
        },
        "bit_depth": {
          "type": "integer",
          "description": "目标位深度（可选，仅适用于WAV格式）",
          "required": "optional"
        }
      },
      "examples": [
        {
          "task_description": "将MP3文件转换为44.1kHz、16位的WAV文件",
          "Act": {
            "tool": "convert_audio_format",
            "arguments": {
              "input_path": "input/music.mp3",
              "output_format": "wav",
              "sample_rate": 44100,
              "bit_depth": 16
            }
          }
        }
      ]
    },
    "trim_audio": {
      "description": "裁剪音频文件的指定时间区间。",
      "detailed_description": "该工具用于从音频文件中提取指定时间段的内容，并可选择性地应用淡入淡出效果以使过渡更平滑。",
      "parameters": {
        "input_path": {
          "type": "string",
          "description": "输入音频文件路径",
          "required": true
        },
        "start_time": {
          "type": "float",
          "description": "开始时间（秒）",
          "required": true
        },
        "end_time": {
          "type": "float",
          "description": "结束时间（秒）",
          "required": true
        },
        "fade_in": {
          "type": "float",
          "description": "淡入时长（秒，可选）",
          "required": "optional"
        },
        "fade_out": {
          "type": "float",
          "description": "淡出时长（秒，可选）",
          "required": "optional"
        }
      },
      "examples": [
        {
          "task_description": "从音频文件中提取10秒到30秒的部分，并应用0.5秒的淡入淡出",
          "Act": {
            "tool": "trim_audio",
            "arguments": {
              "input_path": "input/long_audio.wav",
              "start_time": 10.0,
              "end_time": 30.0,
              "fade_in": 0.5,
              "fade_out": 0.5
            }
          }
        }
      ]
    },
    "align_audio_lengths": {
      "description": "将多个音频文件对齐到相同长度。",
      "detailed_description": "该工具使用多种方法（如填充、裁剪、循环或拉伸）将一组音频文件调整到相同的时长，便于后续处理或混合。",
      "parameters": {
        "audio_paths": {
          "type": "array",
          "description": "音频文件路径列表（至少2个）",
          "required": true
        },
        "target_duration": {
          "type": "float",
          "description": "目标时长（秒，可选）。如果不指定，将使用最长音频的时长",
          "required": "optional"
        },
        "method": {
          "type": "string",
          "description": "对齐方法: 'pad'（填充）, 'trim'（裁剪）, 'loop'（循环）或 'stretch'（拉伸）",
          "required": "optional",
          "default": "pad"
        },
        "fade_duration": {
          "type": "float",
          "description": "淡入淡出时长（秒），用于平滑过渡",
          "required": "optional",
          "default": 0.1
        }
      },
      "examples": [
        {
          "task_description": "将两个音频文件对齐到15秒长度，使用填充方法并添加0.2秒淡入淡出",
          "Act": {
            "tool": "align_audio_lengths",
            "arguments": {
              "audio_paths": [
                "output/audio/vocal.wav",
                "output/audio/accompaniment.wav"
              ],
              "target_duration": 15.0,
              "method": "pad",
              "fade_duration": 0.2
            }
          }
        }
      ]
    },
    "read_file": {
      "description": "读取指定类型的所有文件内容，支持md, txt, json文件。",
      "detailed_description": "该工具可以读取指定目录下所有指定类型的文件内容。对于json文件会返回解析后的字典对象，对于md和txt文件会返回合并后的文本内容。每个文件的内容会以文件名作为分隔符。",
      "parameters": {
        "directory_path": {
          "type": "string",
          "description": "要读取的文件所在目录路径",
          "required": true
        },
        "file_type": {
          "type": "string",
          "description": "要读取的文件类型，支持md、txt、json",
          "required": "optional",
          "default": "md",
          "enum": [
            "md",
            "txt",
            "json"
          ]
        }
      },
      "examples": [
        {
          "task_description": "读取目录下所有markdown文件",
          "Act": {
            "tool": "read_file",
            "arguments": {
              "directory_path": "docs",
              "file_type": "md"
            }
          }
        },
        {
          "task_description": "读取目录下所有json文件",
          "Act": {
            "tool": "read_file",
            "arguments": {
              "directory_path": "config",
              "file_type": "json"
            }
          }
        }
      ]
    },
    "write_file": {
      "description": "写入（或新建）指定类型文件，支持md、txt、json。",
      "detailed_description": "该工具可以创建新文件或覆盖已存在的文件。支持写入markdown、txt和json格式的文件。对于json文件，会自动将内容解析为json对象后写入。如果文件已存在且未设置覆盖，则会返回错误信息。",
      "parameters": {
        "directory_path": {
          "type": "string",
          "description": "要写入文件的目录路径",
          "required": true
        },
        "filename": {
          "type": "string",
          "description": "文件名（可以带后缀，如果不带后缀会自动添加.md后缀）",
          "required": true
        },
        "content": {
          "type": "string",
          "description": "要写入的文件内容（对于json文件必须是可解析的json格式字符串）",
          "required": true
        },
        "overwrite": {
          "type": "boolean",
          "description": "是否覆盖已存在的文件",
          "required": "optional",
          "default": false
        }
      },
      "examples": [
        {
          "task_description": "创建新的markdown文件",
          "Act": {
            "tool": "write_file",
            "arguments": {
              "directory_path": "docs",
              "filename": "readme",
              "content": "# 项目说明\n\n这是一个示例文档。"
            }
          }
        },
        {
          "task_description": "创建json配置文件并覆盖已存在的文件",
          "Act": {
            "tool": "write_file",
            "arguments": {
              "directory_path": "config",
              "filename": "settings.json",
              "content": "{\"name\": \"updated\", \"value\": 456}",
              "overwrite": true
            }
          }
        }
      ]
    },
    "modify_file": {
      "description": "修改（覆盖）单个已存在的.md/.txt/.json文件。",
      "detailed_description": "该工具用于修改已存在的文件内容。支持修改markdown、txt和json格式的文件。对于json文件，会自动将新内容解析为json对象后写入。文件必须已经存在，否则会返回错误信息。",
      "parameters": {
        "file_path": {
          "type": "string",
          "description": "要修改的文件的完整路径",
          "required": true
        },
        "new_content": {
          "type": "string",
          "description": "新的文件内容（对于json文件必须是可解析的json格式字符串）",
          "required": true
        }
      },
      "examples": [
        {
          "task_description": "修改markdown文件内容",
          "Act": {
            "tool": "modify_file",
            "arguments": {
              "file_path": "docs/readme.md",
              "new_content": "# 更新后的文档\n\n这是更新后的内容。"
            }
          }
        },
        {
          "task_description": "修改json配置文件",
          "Act": {
            "tool": "modify_file",
            "arguments": {
              "file_path": "config/settings.json",
              "new_content": "{\"name\": \"updated\", \"value\": 456}"
            }
          }
        }
      ]
    },
    "cosyvoice2tool_api": {
      "description": "CosyVoice2 Text-to-Speech Tool",
      "detailed_description": "Uses the CosyVoice2 model to convert text into realistic speech. You can provide a reference audio as a voice style prompt (3-second fast cloning), or use text instructions to control the generation effect (natural language control).",
      "parameters": {
        "tts_text": {
          "type": "string",
          "description": "The text content to be synthesized into speech.",
          "required": true,
          "example": "Hello, this is a test of the CosyVoice2 text-to-speech tool."
        },
        "mode": {
          "type": "string",
          "description": "Inference mode. Use 'cosy' for 3s-fast-cloning with a prompt wav, or 'instruct' for natural language control.",
          "required": "optional",
          "default": "cosy",
          "enum": [
            "cosy",
            "instruct"
          ],
          "example": "cosy"
        },
        "prompt_text": {
          "type": "string",
          "description": "Prompt text, used only in certain modes.",
          "required": "optional",
          "example": "A gentle and soothing voice."
        },
        "prompt_wav_path": {
          "type": "string",
          "description": "Path to the reference audio file, providing a voice style sample for 'cosy' mode.",
          "required": "optional",
          "example": "input/reference_voice.wav"
        },
        "instruct_text": {
          "type": "string",
          "description": "Instructional text used to control the generation effect in 'instruct' mode.",
          "required": "optional",
          "example": "Read the text in a happy and excited tone."
        },
        "seed": {
          "type": "number",
          "description": "Random inference seed to control the randomness of the generated result.",
          "required": "optional",
          "default": 0,
          "example": 42.0
        },
        "output_path": {
          "type": "string",
          "description": "The path to save the generated audio file.",
          "required": "optional",
          "default": "output.wav",
          "example": "output/generated_speech.wav"
        }
      },
      "examples": [
        {
          "task_description": "Synthesize speech using a reference voice audio (voice cloning).",
          "Act": {
            "tool": "cosyvoice2tool_api",
            "arguments": {
              "tts_text": "The quick brown fox jumps over the lazy dog.",
              "mode": "cosy",
              "prompt_wav_path": "input/source_voice.wav",
              "output_path": "output/cloned_voice.wav"
            }
          }
        },
        {
          "task_description": "Synthesize speech using natural language instructions.",
          "Act": {
            "tool": "cosyvoice2tool_api",
            "arguments": {
              "tts_text": "Welcome to the world of tomorrow!",
              "mode": "instruct",
              "instruct_text": "Speak in a deep, booming announcer voice.",
              "output_path": "output/announcer_voice.wav"
            }
          }
        },
        {
          "task_description": "Perform a basic text-to-speech conversion with a specific random seed.",
          "Act": {
            "tool": "cosyvoice2tool_api",
            "arguments": {
              "tts_text": "This is a simple text-to-speech conversion.",
              "seed": 123.0,
              "output_path": "output/simple_tts.wav"
            }
          }
        }
      ]
    },
    "AudioX_api": {
      "description": "AudioX Audio Generation Tool",
      "detailed_description": "Uses the AudioX model to generate high-quality audio based on text prompts, video, or audio prompts. This tool can synthesize audio from various inputs, including text descriptions, reference video files, and audio prompts, offering detailed control over the generation process through numerous parameters.",
      "parameters": {
        "prompt": {
          "type": "string",
          "description": "A text prompt describing the audio content to be generated.",
          "required": "optional",
          "default": "",
          "example": "A cinematic explosion with debris sounds"
        },
        "negative_prompt": {
          "type": "string",
          "description": "A negative prompt describing features that should not appear in the generated result.",
          "required": "optional",
          "default": null,
          "example": "low quality, muffled, noisy"
        },
        "video_file_path": {
          "type": "string",
          "description": "Path to the video file to be used as a generation reference.",
          "required": "optional",
          "default": null,
          "example": "input/source_video.mp4"
        },
        "audio_prompt_file_path": {
          "type": "string",
          "description": "Path to the audio prompt file to be used as a generation reference.",
          "required": "optional",
          "default": null,
          "example": "input/reference_audio.wav"
        },
        "audio_prompt_path": {
          "type": "string",
          "description": "Path to the audio prompt to be used as a generation reference.",
          "required": "optional",
          "default": null,
          "example": "input/reference_prompt.wav"
        },
        "seconds_start": {
          "type": "float",
          "description": "The start time in seconds of the video to use for generation.",
          "required": "optional",
          "default": 0,
          "example": 5.0
        },
        "seconds_total": {
          "type": "float",
          "description": "The total duration in seconds of the audio to be generated.",
          "required": "optional",
          "default": 10,
          "example": 15.5
        },
        "cfg_scale": {
          "type": "float",
          "description": "CFG scale parameter, controlling how closely the generation follows the prompt.",
          "required": "optional",
          "default": 7,
          "example": 8.5
        },
        "steps": {
          "type": "float",
          "description": "Number of sampling steps, affecting generation quality and time.",
          "required": "optional",
          "default": 100,
          "example": 150
        },
        "preview_every": {
          "type": "float",
          "description": "Sets the preview frequency during generation.",
          "required": "optional",
          "default": 0,
          "example": 10
        },
        "seed": {
          "type": "string",
          "description": "Random seed for generation. Set to '-1' for a random seed.",
          "required": "optional",
          "default": "-1",
          "example": "12345"
        },
        "sampler_type": {
          "type": "string",
          "description": "The type of sampler to use.",
          "required": "optional",
          "default": "dpmpp-3m-sde",
          "enum": [
            "dpmpp-2m-sde",
            "dpmpp-3m-sde"
          ],
          "example": "dpmpp-2m-sde"
        },
        "sigma_min": {
          "type": "float",
          "description": "The minimum sigma value for the sampler.",
          "required": "optional",
          "default": 0.03,
          "example": 0.05
        },
        "sigma_max": {
          "type": "float",
          "description": "The maximum sigma value for the sampler.",
          "required": "optional",
          "default": 500,
          "example": 400
        },
        "cfg_rescale": {
          "type": "float",
          "description": "CFG rescale amount.",
          "required": "optional",
          "default": 0,
          "example": 0.5
        },
        "use_init": {
          "type": "boolean",
          "description": "Whether to use an initial audio file for generation.",
          "required": "optional",
          "default": false,
          "example": true
        },
        "init_audio_path": {
          "type": "string",
          "description": "Path to the initial audio file.",
          "required": "optional",
          "default": null,
          "example": "input/initial_sound.wav"
        },
        "init_noise_level": {
          "type": "float",
          "description": "The noise level to apply to the initial audio.",
          "required": "optional",
          "default": 0.1,
          "example": 0.2
        },
        "output_audio_path": {
          "type": "string",
          "description": "Path to save the output audio file.",
          "required": "optional",
          "default": "output_audio.wav",
          "example": "output/generated_sound.wav"
        },
        "output_video_path": {
          "type": "string",
          "description": "Path to save the output video file.",
          "required": "optional",
          "default": "output_video.mp4",
          "example": "output/generated_video.mp4"
        }
      },
      "examples": [
        {
          "task_description": "Generate audio from a text prompt",
          "Act": {
            "tool": "AudioX_api",
            "arguments": {
              "prompt": "sound of a heavy rainstorm with thunder",
              "seconds_total": 15,
              "output_audio_path": "output/rainstorm.wav"
            }
          }
        },
        {
          "task_description": "Generate audio conditioned on a video file",
          "Act": {
            "tool": "AudioX_api",
            "arguments": {
              "prompt": "footsteps on a gravel path",
              "video_file_path": "input/walking_video.mp4",
              "seconds_total": 20,
              "output_audio_path": "output/footsteps_audio.wav",
              "output_video_path": "output/walking_video_with_audio.mp4"
            }
          }
        },
        {
          "task_description": "Generate audio using an initial audio file and advanced settings",
          "Act": {
            "tool": "AudioX_api",
            "arguments": {
              "prompt": "futuristic vehicle engine sound",
              "use_init": true,
              "init_audio_path": "input/base_engine_sound.wav",
              "init_noise_level": 0.2,
              "steps": 150,
              "cfg_scale": 9.0,
              "sampler_type": "dpmpp-2m-sde",
              "output_audio_path": "output/futuristic_engine.wav"
            }
          }
        }
      ]
    },
    "Qwen2audio_api": {
      "description": "Qwen2-Audio-7B-Instruct Multimodal Dialogue Tool",
      "detailed_description": "Engage in multimodal conversations using both text and audio with the Qwen2-Audio-7B-Instruct model. You can send text or audio for dialogue and also manage the conversation history.",
      "parameters": {
        "prompt": {
          "type": "string",
          "description": "Text prompt, the user's text input.",
          "required": "optional",
          "default": "",
          "example": "Please describe what you hear in this audio."
        },
        "audio_file_path": {
          "type": "string",
          "description": "Path to the input audio file, the user's audio input.",
          "required": "optional",
          "example": "input/audio/user_query.wav"
        },
        "chatbot_history": {
          "type": "list",
          "description": "The chat history, formatted as a list. If None, a new conversation is started.",
          "required": "optional",
          "example": "[{\"role\": \"user\", \"content\": \"What is this sound?\"}, {\"role\": \"assistant\", \"content\": \"It sounds like a bird chirping.\"}]"
        },
        "action": {
          "type": "string",
          "description": "The type of action to perform. Options are 'chat' (to continue the conversation), 'regenerate' (to regenerate the last response), and 'reset' (to reset the conversation).",
          "required": "optional",
          "default": "chat",
          "enum": [
            "chat",
            "regenerate",
            "reset"
          ],
          "example": "regenerate"
        },
        "save_history": {
          "type": "boolean",
          "description": "Whether to save the conversation history to a file.",
          "required": "optional",
          "default": true,
          "example": false
        },
        "output_file": {
          "type": "string",
          "description": "The file path to save the conversation history.",
          "required": "optional",
          "default": "conversation_history.json",
          "example": "output/my_conversation.json"
        }
      },
      "examples": [
        {
          "task_description": "Start a new multimodal chat with an audio file and a text prompt.",
          "Act": {
            "tool": "Qwen2audio_api",
            "arguments": {
              "prompt": "What kind of bird is singing in this audio?",
              "audio_file_path": "input/bird_song.mp3",
              "action": "chat",
              "output_file": "output/bird_chat.json"
            }
          }
        },
        {
          "task_description": "Continue a text-only conversation without sending a new audio file.",
          "Act": {
            "tool": "Qwen2audio_api",
            "arguments": {
              "prompt": "Thank you. Can you tell me more about that species?",
              "chatbot_history": "[...]",
              "action": "chat"
            }
          }
        },
        {
          "task_description": "Regenerate the last response from the assistant.",
          "Act": {
            "tool": "Qwen2audio_api",
            "arguments": {
              "chatbot_history": "[...]",
              "action": "regenerate"
            }
          }
        },
        {
          "task_description": "Reset the conversation and start fresh.",
          "Act": {
            "tool": "Qwen2audio_api",
            "arguments": {
              "action": "reset"
            }
          }
        }
      ]
    },
    "clearervoice_api": {
      "description": "A multi-task audio processing tool from the ClearerVoice Studio platform, capable of speech enhancement, separation, super-resolution, and audio-visual speaker extraction.",
      "detailed_description": "This tool encapsulates four core capabilities of the ClearerVoice Studio platform, allowing selection of different tasks via the 'task' parameter, which automatically calls the corresponding API endpoint. Supported tasks include: 1. 'enhancement' for noise reduction and speech enhancement with selectable models. 2. 'separation' to split audio into two tracks, like vocals and background. 3. 'super_resolution' to upscale low-sample-rate audio, with an option to apply enhancement. 4. 'av_extraction' to extract a speaker's audio or video from a video file.",
      "parameters": {
        "task": {
          "type": "string",
          "description": "The audio processing task to perform. Select from 'enhancement', 'separation', 'super_resolution', or 'av_extraction'.",
          "required": "optional",
          "default": "enhancement",
          "enum": [
            "enhancement",
            "separation",
            "super_resolution",
            "av_extraction"
          ],
          "example": "separation"
        },
        "input_path": {
          "type": "string",
          "description": "Path to the input audio or video file (e.g., WAV, MP3, MP4). Required for all tasks.",
          "required": true,
          "example": "input/meeting_recording.wav"
        },
        "sr": {
          "type": "string",
          "description": "Sample rate selection for the 'enhancement' task. Acceptable values: '16000 Hz' or '48000 Hz'.",
          "required": "optional",
          "default": "48000 Hz",
          "enum": [
            "16000 Hz",
            "48000 Hz"
          ],
          "example": "48000 Hz"
        },
        "apply_se": {
          "type": "boolean",
          "description": "Used only for the 'super_resolution' task. If true, applies speech enhancement after upscaling.",
          "required": "optional",
          "default": true,
          "example": false
        },
        "output_audio_path": {
          "type": "string",
          "description": "Path to save the output audio for 'enhancement', 'super_resolution', or the first track of the 'separation' task.",
          "required": "optional",
          "default": "output.wav",
          "example": "output/enhanced_speech.wav"
        },
        "output_audio_path2": {
          "type": "string",
          "description": "Path to save the second output audio track for the 'separation' task (e.g., background noise).",
          "required": "optional",
          "default": "output_2.wav",
          "example": "output/background_track.wav"
        },
        "output_dir": {
          "type": "string",
          "description": "Directory to save the output files (images/videos) for the 'av_extraction' task.",
          "required": "optional",
          "default": "av_outputs",
          "example": "output/extracted_clips/"
        }
      },
      "examples": [
        {
          "task_description": "Enhance a noisy speech recording using the flagship model.",
          "Act": {
            "tool": "ClearerVoiceTool",
            "arguments": {
              "task": "enhancement",
              "input_path": "input/noisy_podcast.wav",
              "output_audio_path": "output/podcast_enhanced.wav",
              "sr": "48000 Hz"
            }
          }
        },
        {
          "task_description": "Separate a speaker's voice from background music.",
          "Act": {
            "tool": "ClearerVoiceTool",
            "arguments": {
              "task": "separation",
              "input_path": "input/interview_with_music.wav",
              "output_audio_path": "output/interview_voice.wav",
              "output_audio_path2": "output/interview_background.wav"
            }
          }
        },
        {
          "task_description": "Upscale a low-quality recording and apply enhancement.",
          "Act": {
            "tool": "ClearerVoiceTool",
            "arguments": {
              "task": "super_resolution",
              "input_path": "input/old_recording_8khz.wav",
              "output_audio_path": "output/restored_recording_48khz.wav",
              "apply_se": true
            }
          }
        },
        {
          "task_description": "Extract the active speaker's video clips from a meeting recording.",
          "Act": {
            "tool": "ClearerVoiceTool",
            "arguments": {
              "task": "av_extraction",
              "input_path": "input/team_meeting.mp4",
              "output_dir": "output/meeting_speaker_clips"
            }
          }
        }
      ]
    },
    "diffrhythm_api": {
      "description": "DiffRhythm music generation and utility tool.",
      "detailed_description": "This tool encapsulates multiple endpoints of the DiffRhythm (ASLP-lab/DiffRhythm) web platform, providing a one-stop capability from theme/tag generation and lyric alignment to final instrumental music generation. It supports several distinct tasks: 'prompt_type' to query available prompt types; 'theme_tags' to generate timed LRC snippets from a theme and tags; 'lyrics_lrc' to convert raw lyrics into aligned LRC format; 'lambda_val' for querying internal parameters; and 'infer_music' as the primary function to generate music based on LRC files, text/audio prompts, and various diffusion parameters. The music generation task allows fine-tuning through parameters like seed, steps, CFG strength, output format, and even supports editing previously generated audio.",
      "parameters": {
        "task": {
          "type": "string",
          "description": "The specific task to perform. 'prompt_type': query prompt types. 'theme_tags': generate LRC from a theme. 'lyrics_lrc': convert lyrics to LRC. 'lambda_val': query internal lambda value. 'infer_music': generate music.",
          "required": "optional",
          "default": "infer_music",
          "enum": [
            "prompt_type",
            "theme_tags",
            "lyrics_lrc",
            "lambda_val",
            "infer_music"
          ]
        },
        "theme": {
          "type": "string",
          "description": "Theme description, used for the 'theme_tags' task.",
          "required": "optional",
          "example": "A quiet evening by the sea"
        },
        "tags_gen": {
          "type": "string",
          "description": "Comma-separated tags for generation, used for the 'theme_tags' task.",
          "required": "optional",
          "example": "soft piano, gentle waves, slow tempo"
        },
        "language": {
          "type": "string",
          "description": "Language for the 'theme_tags' task.",
          "required": "optional",
          "default": "en",
          "enum": [
            "en",
            "cn"
          ]
        },
        "tags_lyrics": {
          "type": "string",
          "description": "Tags associated with the lyrics, used for the 'lyrics_lrc' task.",
          "required": "optional",
          "example": "pop, upbeat"
        },
        "lyrics_input": {
          "type": "string",
          "description": "The raw lyric string (with line breaks) to be converted to LRC format, for the 'lyrics_lrc' task.",
          "required": "optional",
          "example": "Verse 1\nWalking down the lonely road\nChorus\nI see the sunshine"
        },
        "lrc": {
          "type": "string",
          "description": "Path to the aligned LRC file, required for the 'infer_music' task.",
          "required": "optional",
          "example": "input/lyrics/my_song.lrc"
        },
        "ref_audio_path": {
          "type": "string",
          "description": "[Deprecated] Parameter no longer supported by current implementation.",
          "required": "optional",
          "example": "input/audio/reference_style.wav"
        },
        "text_prompt": {
          "type": "string",
          "description": "Text prompt describing the desired music style, required for the 'infer_music' task.",
          "required": "optional",
          "example": "A beautiful, soothing, and emotional piece of music, with a soft piano and strings, 80 bpm."
        },
        "seed": {
          "type": "number",
          "description": "Seed for the diffusion process to ensure reproducibility. Used in the 'infer_music' task.",
          "required": "optional",
          "default": 0
        },
        "randomize_seed": {
          "type": "boolean",
          "description": "If true, ignores the provided seed and uses a random one. Used in the 'infer_music' task.",
          "required": "optional",
          "default": true
        },
        "steps": {
          "type": "number",
          "description": "Number of inference steps for the diffusion model. Used in the 'infer_music' task.",
          "required": "optional",
          "default": 32
        },
        "cfg_strength": {
          "type": "number",
          "description": "Classifier-Free Guidance strength. Higher values adhere more strictly to the prompt. Used in the 'infer_music' task.",
          "required": "optional",
          "default": 4
        },
        "file_type": {
          "type": "string",
          "description": "The output format for the generated audio file. Used in the 'infer_music' task.",
          "required": "optional",
          "default": "mp3",
          "enum": [
            "mp3",
            "wav",
            "ogg"
          ]
        },
        "odeint_method": {
          "type": "string",
          "description": "The ODE solver method for the inference process. Used in the 'infer_music' task.",
          "required": "optional",
          "default": "euler",
          "enum": [
            "euler",
            "midpoint",
            "rk4",
            "implicit_adams"
          ]
        },
        "preference_infer": {
          "type": "string",
          "description": "[Deprecated] Parameter no longer supported by current implementation.",
          "required": "optional",
          "default": "quality first",
          "enum": [
            "quality first",
            "speed first"
          ]
        },
        "edit": {
          "type": "boolean",
          "description": "Enable editing of a previously generated audio file. Used in the 'infer_music' task.",
          "required": "optional",
          "default": false
        },
        "edit_segments": {
          "type": "string",
          "description": "Specifies the segments to edit if 'edit' is true. Used in the 'infer_music' task.",
          "required": "optional",
          "default": null
        },
        "output_music_path": {
          "type": "string",
          "description": "Path to save the final generated music file. Used in the 'infer_music' task.",
          "required": "optional",
          "default": "diff_music_output.mp3",
          "example": "output/music/final_track.mp3"
        }
      },
      "examples": [
        {
          "task_description": "Generate a piece of music based on an LRC file and a text prompt.",
          "Act": {
            "tool": "diffrhythm_api",
            "arguments": {
              "task": "infer_music",
              "lrc": "path/to/your/lyrics.lrc",
              "text_prompt": "Epic orchestral music, fantasy, cinematic, powerful brass section, 120 bpm",
              "cfg_strength": 5,
              "steps": 40,
              "output_music_path": "output/epic_soundtrack.mp3"
            }
          }
        },
        {
          "task_description": "Generate an LRC timing file from a theme and descriptive tags in Chinese.",
          "Act": {
            "tool": "diffrhythm_api",
            "arguments": {
              "task": "theme_tags",
              "theme": "赛博朋克都市的雨夜",
              "tags_gen": "电子, 氛围, 慢节奏, 霓虹灯",
              "language": "cn"
            }
          }
        },
        {
          "task_description": "Convert a block of raw English lyrics into an LRC file with timing.",
          "Act": {
            "tool": "diffrhythm_api",
            "arguments": {
              "task": "lyrics_lrc",
              "tags_lyrics": "Acoustic, ballad, heartfelt",
              "lyrics_input": "The stars are out tonight\nI feel you by my side\nAnd everything's alright"
            }
          }
        },
        {
          "task_description": "Generate music with a reference audio for style, prioritizing inference speed.",
          "Act": {
            "tool": "diffrhythm_api",
            "arguments": {
              "task": "infer_music",
              "lrc": "path/to/song.lrc",
              "text_prompt": "Lo-fi hip hop beat, chill, relaxing",
              "file_type": "mp3",
              "output_music_path": "output/lofi_beat.mp3"
            }
          }
        }
      ]
    },
    "ACE_Step_api": {
      "description": "An integrated tool for end-to-end music generation, editing, and extension from text.",
      "detailed_description": "ACE-Step is an end-to-end web platform for text-to-music generation and subsequent audio fine-tuning, repainting, and extension. This tool encapsulates nine key endpoints, allowing users to complete the entire pipeline from scratch to multi-round editing using a single 'task' parameter and its corresponding arguments. Supported tasks include 'text2music' for initial generation, 'retake' for resampling, 'repaint' to redraw a specific time segment, 'edit' for modifying lyrics or style, and 'extend' to lengthen the audio. Tasks can be chained together; for example, the output of 'text2music' can be fed into 'repaint' or 'extend' for further modifications. The tool also provides utility functions like 'sample_data' to fetch hyperparameter templates and 'get_audio' to retrieve the audio from any stage of the process.",
      "parameters": {
        "task": {
          "type": "string",
          "description": "The primary operation to perform. Determines which endpoint is called and which other parameters are required.",
          "required": true,
          "default": "text2music",
          "enum": [
            "text2music",
            "retake",
            "repaint",
            "edit",
            "extend",
            "sample_data",
            "get_audio",
            "edit_type"
          ]
        },
        "input_json_1": {
          "type": [
            "object",
            "array",
            "string",
            "number",
            "boolean"
          ],
          "description": "The primary JSON data from a previous step. Required for 'retake', 'repaint', 'edit', and 'extend' tasks, where it usually represents the parameters of the music to be modified.",
          "required": "optional"
        },
        "input_json_2": {
          "type": [
            "object",
            "array",
            "string",
            "number",
            "boolean"
          ],
          "description": "The secondary JSON data for complex tasks. Required for 'repaint', 'edit', and 'extend' tasks.",
          "required": "optional"
        },
        "input_audio_path": {
          "type": "string",
          "description": "Path to an input audio file, used when a task requires an uploaded audio source (e.g., repaint, edit, extend from an upload).",
          "required": "optional"
        },
        "prompt": {
          "type": "string",
          "description": "Music style tags, separated by commas. Required for 'text2music' task. Example: 'pop, 120BPM, energetic'.",
          "required": "optional",
          "default": ""
        },
        "lyrics": {
          "type": "string",
          "description": "The lyrics for the music. Required for 'text2music' task. Can be multi-line text.",
          "required": "optional",
          "default": ""
        },
        "infer_step": {
          "type": "number",
          "description": "Number of topology steps. Higher values increase quality but are slower. Range: 1-100.",
          "required": "optional",
          "default": 27
        },
        "guidance_scale": {
          "type": "number",
          "description": "The CFG Scale for inference. Range: 1-30.",
          "required": "optional",
          "default": 15
        },
        "scheduler_type": {
          "type": "string",
          "description": "The type of sampler to use.",
          "required": "optional",
          "default": "euler",
          "enum": [
            "euler",
            "heun"
          ]
        },
        "cfg_type": {
          "type": "string",
          "description": "The CFG scheme to use.",
          "required": "optional",
          "default": "apg",
          "enum": [
            "cfg",
            "apg",
            "cfg_star"
          ]
        },
        "omega_scale": {
          "type": "number",
          "description": "Granularity control. Range: 0-20.",
          "required": "optional",
          "default": 10
        },
        "manual_seeds": {
          "type": "string",
          "description": "Manual seeds for reproducibility, in the format '42,17'. If null or empty, seeds are chosen randomly.",
          "required": "optional",
          "default": null
        },
        "guidance_interval": {
          "type": "number",
          "description": "CFG trigger interval. Range: 0-1.",
          "required": "optional",
          "default": 0.5
        },
        "guidance_interval_decay": {
          "type": "number",
          "description": "Decay for the guidance interval.",
          "required": "optional",
          "default": 0.0
        },
        "min_guidance_scale": {
          "type": "number",
          "description": "Minimum guidance scale.",
          "required": "optional",
          "default": 3
        },
        "use_erg_tag": {
          "type": "boolean",
          "description": "Enable the ERG enhancement module for tags.",
          "required": "optional",
          "default": true
        },
        "use_erg_lyric": {
          "type": "boolean",
          "description": "Enable the ERG enhancement module for lyrics.",
          "required": "optional",
          "default": true
        },
        "use_erg_diffusion": {
          "type": "boolean",
          "description": "Enable the ERG enhancement module for diffusion.",
          "required": "optional",
          "default": true
        },
        "oss_steps": {
          "type": "string",
          "description": "Comma-separated OSS stage steps, e.g., '0.2,0.6'. If null, uses default.",
          "required": "optional",
          "default": null
        },
        "guidance_scale_text": {
          "type": "number",
          "description": "Specific guidance scale for text.",
          "required": "optional",
          "default": 0.0
        },
        "guidance_scale_lyric": {
          "type": "number",
          "description": "Specific guidance scale for lyrics.",
          "required": "optional",
          "default": 0.0
        },
        "retake_variance": {
          "type": "number",
          "description": "Random perturbation amplitude for 'retake' and 'repaint' tasks. Range: 0-1.",
          "required": "optional",
          "default": 0.2
        },
        "retake_seeds": {
          "type": "string",
          "description": "Comma-separated seeds for resampling in 'retake' task. Required for 'retake'.",
          "required": "optional",
          "default": ""
        },
        "repaint_start": {
          "type": "number",
          "description": "Start time in seconds for the repaint region.",
          "required": "optional",
          "default": 0.0
        },
        "repaint_end": {
          "type": "number",
          "description": "End time in seconds for the repaint region.",
          "required": "optional",
          "default": 30.0
        },
        "repaint_source": {
          "type": "string",
          "description": "Source audio for the 'repaint' task.",
          "required": "optional",
          "default": "text2music",
          "enum": [
            "text2music",
            "last_repaint",
            "upload"
          ]
        },
        "edit_type": {
          "type": "string",
          "description": "The type of edit to perform in the 'edit' task.",
          "required": "optional",
          "default": "only_lyrics",
          "enum": [
            "only_lyrics",
            "remix"
          ]
        },
        "edit_prompt": {
          "type": "string",
          "description": "New style tags for the 'edit' task.",
          "required": "optional",
          "default": ""
        },
        "edit_lyrics": {
          "type": "string",
          "description": "New lyrics for the 'edit' task.",
          "required": "optional",
          "default": ""
        },
        "edit_n_min": {
          "type": "number",
          "description": "Minimum random range for 'remix' editing.",
          "required": "optional",
          "default": 0.6
        },
        "edit_n_max": {
          "type": "number",
          "description": "Maximum random range for 'remix' editing.",
          "required": "optional",
          "default": 1.0
        },
        "left_extend_length": {
          "type": "number",
          "description": "Duration in seconds to extend to the left for the 'extend' task.",
          "required": "optional",
          "default": 0.0
        },
        "right_extend_length": {
          "type": "number",
          "description": "Duration in seconds to extend to the right for the 'extend' task.",
          "required": "optional",
          "default": 30.0
        },
        "extend_source": {
          "type": "string",
          "description": "Source audio for the 'extend' or 'edit' task.",
          "required": "optional",
          "default": "text2music",
          "enum": [
            "text2music",
            "last_extend",
            "upload"
          ]
        },
        "extend_seeds": {
          "type": "string",
          "description": "Comma-separated seeds for the 'extend' task. Required for 'extend'.",
          "required": "optional",
          "default": ""
        },
        "lambda_stage": {
          "type": "string",
          "description": "Specifies which stage's source audio to retrieve for the 'get_audio' task.",
          "required": "optional",
          "default": "text2music",
          "enum": [
            "text2music",
            "last_repaint",
            "upload",
            "last_edit",
            "last_extend"
          ]
        },
        "output_audio_path": {
          "type": "string",
          "description": "Path to save the output audio file.",
          "required": "optional",
          "default": "ace_step_output.wav"
        },
        "output_json_path": {
          "type": "string",
          "description": "Path to save the output parameters JSON file.",
          "required": "optional",
          "default": "ace_step_params.json"
        }
      },
      "examples": [
        {
          "task_description": "Generate music from text and lyrics.",
          "Act": {
            "tool": "ACE_Step_api",
            "arguments": {
              "task": "text2music",
              "prompt": "lofi, chill, instrumental",
              "lyrics": "Verse 1: Coding through the late night hours.",
              "output_audio_path": "output/lofi_song.wav",
              "output_json_path": "output/lofi_song_params.json"
            }
          }
        },
        {
          "task_description": "Retake a previously generated music piece with new seeds for variation.",
          "Act": {
            "tool": "ACE_Step_api",
            "arguments": {
              "task": "retake",
              "input_json_1": "output/lofi_song_params.json",
              "retake_variance": 0.3,
              "retake_seeds": "123,456",
              "output_audio_path": "output/lofi_song_retake.wav",
              "output_json_path": "output/lofi_song_retake_params.json"
            }
          }
        },
        {
          "task_description": "Extend a music piece by 10 seconds to the right.",
          "Act": {
            "tool": "ACE_Step_api",
            "arguments": {
              "task": "extend",
              "input_json_1": "output/lofi_song_retake_params.json",
              "input_json_2": {},
              "right_extend_length": 10.0,
              "extend_seeds": "789,101",
              "extend_source": "text2music",
              "output_audio_path": "output/lofi_extended.wav",
              "output_json_path": "output/lofi_extended_params.json"
            }
          }
        },
        {
          "task_description": "Fetch hyperparameter templates for quick tuning.",
          "Act": {
            "tool": "ACE_Step_api",
            "arguments": {
              "task": "sample_data"
            }
          }
        }
      ]
    },
    "SenseVoice_api": {
      "description": "A speech understanding tool based on SenseVoice-Small for multi-task processing.",
      "detailed_description": "Based on the iic-sensevoice Space (endpoint /model_inference), this tool performs multiple speech-related tasks in a single call: 1. Automatic Speech Recognition (ASR), 2. Language Identification (LID), 3. Speech Emotion Recognition (SER), and 4. Acoustic Event Detection (AED). The model features ultra-low latency and is optimized for inputs of 30 seconds or less. It supports several languages, including Chinese (zh), English (en), Cantonese (yue), Japanese (ja), and Korean (ko), with an 'auto' option for automatic language detection. The output is a text file containing the transcribed text, detected events, and emotions.",
      "parameters": {
        "input_wav_path": {
          "type": "string",
          "description": "Path to the local input audio file. Both 16k and 44k sample rates are supported.",
          "required": true,
          "example": "input/audio/recording_to_transcribe.wav"
        },
        "language": {
          "type": "string",
          "description": "Language code for the input audio. 'auto' enables automatic detection.",
          "required": "optional",
          "default": "auto",
          "enum": [
            "auto",
            "zh",
            "en",
            "yue",
            "ja",
            "ko",
            "nospeech"
          ],
          "example": "en"
        },
        "output_txt_path": {
          "type": "string",
          "description": "[Deprecated] This parameter is no longer supported and will be ignored.",
          "required": "deprecated"
        }
      },
      "examples": [
        {
          "task_description": "Transcribe an English audio file and save to a specific path",
          "Act": {
            "tool": "SenseVoiceTool",
            "arguments": {
              "input_wav_path": "path/to/english_speech.wav",
              "language": "en"
            }
          }
        },
        {
          "task_description": "Transcribe an audio file with auto language detection and default output path",
          "Act": {
            "tool": "SenseVoiceTool",
            "arguments": {
              "input_wav_path": "path/to/unknown_language.wav"
            }
          }
        },
        {
          "task_description": "Transcribe a Japanese audio file using the default output filename",
          "Act": {
            "tool": "SenseVoiceTool",
            "arguments": {
              "input_wav_path": "path/to/japanese_podcast.wav",
              "language": "ja"
            }
          }
        }
      ]
    },
    "whisper_large_v3_turbo_api": {
      "description": "Performs audio transcription or translation using the hf-audio/whisper-large-v3-turbo model.",
      "detailed_description": "This tool supports three input methods: local audio files, online audio file URLs, and YouTube video URLs. When 'audio_path' is provided, the /predict endpoint is called. When 'yt_url' is provided, the /predict_2 endpoint is called. The 'audio_path' and 'yt_url' parameters are mutually exclusive; please provide only one. Supported tasks include 'transcribe' (speech-to-text) and 'translate' (translates audio to English). The result is returned as transcribed or translated text. If 'output_path' is specified, the result will be saved to the file and the file path will be returned.",
      "parameters": {
        "audio_path": {
          "type": "string",
          "description": "Path to the local audio file or a directly accessible audio file URL. This is mutually exclusive with 'yt_url'.",
          "required": "optional",
          "example": "path/to/audio.wav"
        },
        "yt_url": {
          "type": "string",
          "description": "The URL of a YouTube video. This is mutually exclusive with 'audio_path'.",
          "required": "optional",
          "example": "https://www.youtube.com/watch?v=example"
        },
        "task": {
          "type": "string",
          "description": "The task to be performed. Can be 'transcribe' (default) or 'translate'.",
          "required": "optional",
          "default": "transcribe",
          "enum": [
            "transcribe",
            "translate"
          ],
          "example": "transcribe"
        },
        "output_path": {
          "type": "string",
          "description": "Optional path to save the result file. If provided, the function writes the result to this file and returns the path; otherwise, it returns the text result directly.",
          "required": "optional",
          "example": "output/result.txt"
        }
      },
      "examples": [
        {
          "task_description": "Transcribe a local audio file and save the result.",
          "Act": {
            "tool": "whisper_large_v3_turbo_api",
            "arguments": {
              "audio_path": "path/to/your/audio.mp3",
              "task": "transcribe",
              "output_path": "output/transcription.txt"
            }
          }
        },
        {
          "task_description": "Translate the audio from a YouTube video into English.",
          "Act": {
            "tool": "whisper_large_v3_turbo_api",
            "arguments": {
              "yt_url": "https://www.youtube.com/watch?v=example_video_id",
              "task": "translate"
            }
          }
        },
        {
          "task_description": "Transcribe an audio file from a URL and return the text directly.",
          "Act": {
            "tool": "whisper_large_v3_turbo_api",
            "arguments": {
              "audio_path": "https://example.com/audio.mp3"
            }
          }
        }
      ]
    },
    "tiger_api": {
      "description": "TIGER audio extraction tool, capable of separating audio tracks from audio or video files.",
      "detailed_description": "This tool utilizes the TIGER model to perform various audio separation tasks. Supported API endpoints/tasks: '/separate_dnr' separates dialogue, sound effects, and music from audio files. '/separate_speakers' separates up to 4 speakers from an audio file. '/separate_dnr_video' separates dialogue, sound effects, and music from a video file and returns the separated videos. '/separate_speakers_video' separates up to 4 speakers from a video file and returns the separated videos. The tool accepts a path to a single audio or video file and returns a list containing the paths of all output files.",
      "parameters": {
        "input_file_path": {
          "type": "string",
          "description": "Path to the input audio or video file.",
          "required": true,
          "example": "path/to/audio.wav"
        },
        "task": {
          "type": "string",
          "description": "The task to be performed. Must be one of the four valid API endpoints.",
          "required": true,
          "enum": [
            "/separate_dnr",
            "/separate_speakers",
            "/separate_dnr_video",
            "/separate_speakers_video"
          ],
          "example": "/separate_speakers"
        },
        "output_dir": {
          "type": "string",
          "description": "The directory path to save the output files.",
          "required": "optional",
          "default": "output",
          "example": "results/speakers"
        }
      },
      "examples": [
        {
          "task_description": "Separate up to 4 speakers from an audio file.",
          "Act": {
            "tool": "tiger_audio_extraction_api",
            "arguments": {
              "input_file_path": "path/to/audio.wav",
              "task": "/separate_speakers",
              "output_dir": "results/speakers"
            }
          }
        },
        {
          "task_description": "Separate dialogue, sound effects, and music from a video file.",
          "Act": {
            "tool": "tiger_audio_extraction_api",
            "arguments": {
              "input_file_path": "path/to/video.mp4",
              "task": "/separate_dnr_video",
              "output_dir": "results/dnr"
            }
          }
        },
        {
          "task_description": "Separate dialogue, sound effects, and music from an audio file.",
          "Act": {
            "tool": "tiger_audio_extraction_api",
            "arguments": {
              "input_file_path": "path/to/meeting.mp3",
              "task": "/separate_dnr"
            }
          }
        }
      ]
    },
    "audio_super_resolution_api": {
      "description": "Uses the Nick088/Audio-SR model for audio super-resolution.",
      "detailed_description": "This tool enhances the quality of an audio file by increasing its resolution. It uses a pre-trained model to generate a higher-quality version of the input audio. Supported models: 'basic', 'speech'. Input: Path to a local audio file (e.g., .wav, .mp3). Returns: The file path of the enhanced output audio.",
      "parameters": {
        "audio_file_path": {
          "type": "string",
          "description": "Required. The path to the input audio file to be enhanced.",
          "required": true,
          "example": "input/audio/low_quality.wav"
        },
        "output_path": {
          "type": "string",
          "description": "Required. The path to save the resulting enhanced audio file.",
          "required": true,
          "example": "output/enhanced_audio.wav"
        },
        "model_name": {
          "type": "string",
          "description": "The model to use. Options are 'basic' or 'speech'.",
          "required": "optional",
          "default": "basic",
          "enum": [
            "basic",
            "speech"
          ],
          "example": "speech"
        },
        "guidance_scale": {
          "type": "float",
          "description": "The scale used to guide the generation process.",
          "required": "optional",
          "default": 3.5,
          "example": 4.0
        },
        "ddim_steps": {
          "type": "integer",
          "description": "Number of DDIM diffusion model steps.",
          "required": "optional",
          "default": 50,
          "example": 75
        },
        "seed": {
          "type": "integer",
          "description": "Random seed for reproducing results.",
          "required": "optional",
          "default": 42,
          "example": 123
        }
      },
      "examples": [
        {
          "task_description": "Enhance a speech recording using the 'speech' model and custom settings.",
          "Act": {
            "tool": "audio_super_resolution_api",
            "arguments": {
              "audio_file_path": "input/conference_call.mp3",
              "output_path": "output/enhanced_conference_call.wav",
              "model_name": "speech",
              "guidance_scale": 4.0,
              "ddim_steps": 75,
              "seed": 123
            }
          }
        },
        {
          "task_description": "Enhance a general audio file using default optional parameters.",
          "Act": {
            "tool": "audio_super_resolution_api",
            "arguments": {
              "audio_file_path": "input/music_demo.wav",
              "output_path": "output/enhanced_music_demo.wav",
              "model_name": "basic"
            }
          }
        },
        {
          "task_description": "Upscale an audio file with only the required parameters specified.",
          "Act": {
            "tool": "audio_super_resolution_api",
            "arguments": {
              "audio_file_path": "path/to/my_audio.wav",
              "output_path": "path/to/my_enhanced_audio.wav"
            }
          }
        }
      ]
    },
    "index_tts_1_5_api": {
      "description": "A Text-to-Speech (TTS) tool that clones a voice from a reference audio to generate speech for the target text.",
      "detailed_description": "This tool provides Text-to-Speech (TTS) functionality by cloning a voice from a reference audio file to synthesize speech for a given text. It utilizes the '/gen_single' API endpoint. The process involves providing a path to a reference audio (e.g., .wav, .mp3) and the text to be converted. The tool then generates a new audio file with the voice from the reference audio speaking the provided text. The final path of the successfully generated audio file is returned.",
      "parameters": {
        "prompt_audio_path": {
          "type": "string",
          "description": "The file path to the reference audio for voice cloning. Supports formats like .wav, .mp3, etc.",
          "required": true,
          "example": "my_reference_voice.wav"
        },
        "target_text": {
          "type": "string",
          "description": "The target text to be converted into speech.",
          "required": true,
          "example": "Hello, welcome to this speech synthesis tool!"
        },
        "output_path": {
          "type": "string",
          "description": "The path to save the generated audio file.",
          "required": "optional",
          "default": "generated_audio.wav",
          "example": "output/result.wav"
        }
      },
      "examples": [
        {
          "task_description": "Generate speech by cloning a voice from a reference audio and saving it to a specific output path.",
          "Act": {
            "tool": "index_tts_1_5_api",
            "arguments": {
              "prompt_audio_path": "input/reference_voice.wav",
              "target_text": "Hello, this is a demonstration of voice cloning.",
              "output_path": "output/cloned_voice_audio.wav"
            }
          }
        },
        {
          "task_description": "Use a reference audio to generate speech for a target text, using the default output path.",
          "Act": {
            "tool": "index_tts_1_5_api",
            "arguments": {
              "prompt_audio_path": "samples/my_voice.mp3",
              "target_text": "This tool makes it easy to generate speech in any voice."
            }
          }
        }
      ]
    },
    "audiocraft_jasco_api": {
      "description": "Audiocraft Music Generation Tool",
      "detailed_description": "Calls the /predict_full API endpoint of Tonic/audiocraft to generate music based on text, chords, melody, and drums. Supported input types: text descriptions, chord progression strings, melody audio files, and drum audio files. Returns a tuple containing the local saved paths of the two generated audio files (Jasco Stem 1, Jasco Stem 2). Note: Melody and drum files are optional, but their use depends on the selected model. For example, models containing 'melody' in their name require melody_file_path.",
      "parameters": {
        "model": {
          "type": "string",
          "description": "The name of the model to use. Available options: 'facebook/jasco-chords-drums-400M', 'facebook/jasco-chords-drums-1B', 'facebook/jasco-chords-drums-melody-400M', 'facebook/jasco-chords-drums-melody-1B'.",
          "required": "optional",
          "default": "facebook/jasco-chords-drums-melody-400M",
          "enum": [
            "facebook/jasco-chords-drums-400M",
            "facebook/jasco-chords-drums-1B",
            "facebook/jasco-chords-drums-melody-400M",
            "facebook/jasco-chords-drums-melody-1B"
          ]
        },
        "text": {
          "type": "string",
          "description": "Text prompt describing the music style, instruments, etc.",
          "required": "optional",
          "default": "Strings, woodwind, orchestral, symphony."
        },
        "chords_sym": {
          "type": "string",
          "description": "Chord progression string in the format `(CHORD, START_TIME_IN_SECONDS)`.",
          "required": "optional",
          "default": "(C, 0.0), (D, 2.0), (F, 4.0), (Ab, 6.0), (Bb, 7.0), (C, 8.0)"
        },
        "melody_file_path": {
          "type": "string",
          "description": "Local path to the melody reference audio file. Required for models that use melody.",
          "required": "optional",
          "default": ""
        },
        "drums_file_path": {
          "type": "string",
          "description": "Local path to the drums reference audio file. Used when `drum_input_src` is 'file'.",
          "required": "optional",
          "default": ""
        },
        "drums_mic_path": {
          "type": "string",
          "description": "Local path to the drum audio file recorded via microphone. Used when `drum_input_src` is 'mic'.",
          "required": "optional",
          "default": ""
        },
        "drum_input_src": {
          "type": "string",
          "description": "The source of the drum input. Available options: 'file', 'mic'.",
          "required": "optional",
          "default": "file",
          "enum": [
            "file",
            "mic"
          ]
        },
        "cfg_coef_all": {
          "type": "number",
          "description": "Global coefficient for Classifier-Free Guidance (CFG).",
          "required": "optional",
          "default": 1.25
        },
        "cfg_coef_txt": {
          "type": "number",
          "description": "CFG coefficient for the text condition.",
          "required": "optional",
          "default": 2.5
        },
        "ode_rtol": {
          "type": "number",
          "description": "Relative tolerance for the ODE solver.",
          "required": "optional",
          "default": 0.0001
        },
        "ode_atol": {
          "type": "number",
          "description": "Absolute tolerance for the ODE solver.",
          "required": "optional",
          "default": 0.0001
        },
        "ode_solver": {
          "type": "string",
          "description": "Type of ODE solver. Available options: 'euler', 'dopri5'.",
          "required": "optional",
          "default": "euler",
          "enum": [
            "euler",
            "dopri5"
          ]
        },
        "ode_steps": {
          "type": "number",
          "description": "Number of steps for the 'euler' solver.",
          "required": "optional",
          "default": 10
        },
        "output_dir": {
          "type": "string",
          "description": "Directory path to save the generated audio files. Defaults to 'output_audio'.",
          "required": "optional",
          "default": "output_audio"
        }
      },
      "examples": [
        {
          "task_description": "Generate music using text and chords",
          "Act": {
            "tool": "audiocraft_jasco_api",
            "arguments": {
              "model": "facebook/jasco-chords-drums-400M",
              "text": "Acoustic folk song with a gentle guitar and a simple beat.",
              "chords_sym": "(G, 0.0), (C, 4.0), (G, 8.0), (D, 12.0)",
              "output_dir": "generated_music"
            }
          }
        },
        {
          "task_description": "Generate music with melody and drums from files",
          "Act": {
            "tool": "audiocraft_jasco_api",
            "arguments": {
              "model": "facebook/jasco-chords-drums-melody-400M",
              "text": "Upbeat pop track with a catchy synth melody.",
              "chords_sym": "(Am, 0.0), (F, 2.0), (C, 4.0), (G, 6.0)",
              "melody_file_path": "path/to/your/melody.wav",
              "drums_file_path": "path/to/your/drums.wav",
              "drum_input_src": "file",
              "output_dir": "pop_track"
            }
          }
        }
      ]
    },
    "step_audio_tts_3b_api": {
      "description": "Voice cloning tool: Use a reference audio to clone its timbre and generate new speech.",
      "detailed_description": "This tool performs voice cloning text-to-speech (TTS) by using a reference audio to clone its timbre and generate new speech from the provided text. It utilizes the /generate_clone API endpoint. The inputs required are the target text for synthesis, a reference audio file for timbre cloning, and the corresponding text of the reference audio. The tool returns the file path of the generated audio.",
      "parameters": {
        "text": {
          "type": "string",
          "description": "The target text to be converted into speech.",
          "required": true,
          "example": "Hello, welcome to use this voice cloning tool."
        },
        "prompt_audio": {
          "type": "string",
          "description": "Path to the reference audio file used for cloning the timbre. Supports common audio formats like WAV, MP3, etc.",
          "required": true,
          "example": "path/to/sample.wav"
        },
        "prompt_text": {
          "type": "string",
          "description": "The text content corresponding to the 'prompt_audio' reference file.",
          "required": true,
          "example": "This is the text of the reference audio."
        },
        "output_path": {
          "type": "string",
          "description": "Path to save the generated audio file.",
          "required": "optional",
          "default": "generated_clone_audio.wav",
          "example": "output/cloned_speech.wav"
        }
      },
      "examples": [
        {
          "task_description": "Clone a voice from a reference audio and text to generate new speech, saving to a specified output path.",
          "Act": {
            "tool": "VoiceCloneTTS",
            "arguments": {
              "text": "Hello, welcome to use this voice cloning tool.",
              "prompt_audio": "path/to/sample.wav",
              "prompt_text": "This is the text of the reference audio.",
              "output_path": "output/cloned_speech.wav"
            }
          }
        }
      ]
    },
    "sparkTTS_tool_api": {
      "description": "A tool for text-to-speech, supporting voice cloning and custom voice creation.",
      "detailed_description": "The SparkTTS tool provides text-to-speech functionality with two main tasks. The 'voice_clone' task allows cloning a voice from an audio sample to read the given text. The 'voice_creation' task generates a custom voice based on specified gender, pitch, and speed parameters to read the text. For the 'voice_clone' task, the 'text', 'prompt_text', and 'prompt_audio_path' are required. For the 'voice_creation' task, 'text' is required, while 'gender', 'pitch', and 'speed' have default values. The function returns the path to the generated audio file upon successful execution.",
      "parameters": {
        "task": {
          "type": "string",
          "description": "The task to be performed, either 'voice_clone' or 'voice_creation'.",
          "required": true,
          "enum": [
            "voice_clone",
            "voice_creation"
          ],
          "example": "voice_clone"
        },
        "text": {
          "type": "string",
          "description": "The input text to be converted to speech.",
          "required": true,
          "example": "This is the cloned voice."
        },
        "output_path": {
          "type": "string",
          "description": "The path to save the generated audio file.",
          "required": true,
          "example": "./clone_output.wav"
        },
        "prompt_text": {
          "type": "string",
          "description": "The text corresponding to the prompt audio for voice cloning. Required when task is 'voice_clone'.",
          "required": "optional",
          "example": "This is the prompt audio's text."
        },
        "prompt_audio_path": {
          "type": "string",
          "description": "The path to the prompt audio file (e.g., .wav) for voice cloning. A sample rate of at least 16kHz is recommended. Required when task is 'voice_clone'.",
          "required": "optional",
          "example": "./sample.wav"
        },
        "gender": {
          "type": "string",
          "description": "The gender of the generated voice, can be 'male' or 'female'. Used only when task is 'voice_creation'.",
          "required": "optional",
          "default": "male",
          "enum": [
            "male",
            "female"
          ],
          "example": "female"
        },
        "pitch": {
          "type": "float",
          "description": "The pitch of the generated voice. Used only when task is 'voice_creation'.",
          "required": "optional",
          "default": 3.0,
          "example": 4.0
        },
        "speed": {
          "type": "float",
          "description": "The speed of the generated voice. Used only when task is 'voice_creation'.",
          "required": "optional",
          "default": 3.0,
          "example": 2.0
        }
      },
      "examples": [
        {
          "task_description": "Clone a voice using a prompt audio and text",
          "Act": {
            "tool": "SparkTTS_tool",
            "arguments": {
              "task": "voice_clone",
              "text": "This is the cloned voice.",
              "prompt_text": "This is the prompt audio's text.",
              "prompt_audio_path": "./sample.wav",
              "output_path": "./clone_output.wav"
            }
          }
        },
        {
          "task_description": "Create a custom female voice with specific pitch and speed",
          "Act": {
            "tool": "SparkTTS_tool",
            "arguments": {
              "task": "voice_creation",
              "text": "This is a custom voice.",
              "gender": "female",
              "pitch": 4.0,
              "speed": 2.0,
              "output_path": "./creation_output.wav"
            }
          }
        }
      ]
    },
    "yue_api": {
      "description": "YuE Music Generation Tool.",
      "detailed_description": "Calls the 'innova-ai/YuE-music-generator-demo' API on Hugging Face Space to generate music. Supports generation by specifying music genre, lyrics, or by providing an audio file as a prompt. The API returns three audio files: a mixed final track, a vocals-only track, and an instrumental-only track.",
      "parameters": {
        "genre_txt": {
          "type": "string",
          "description": "Text description of the music genre, e.g., 'Pop' or 'Lyrical Folk'.",
          "required": "optional",
          "example": "Pop"
        },
        "lyrics_txt": {
          "type": "string",
          "description": "The lyrics text for the music.",
          "required": "optional",
          "example": "Verse 1: The sun shines bright."
        },
        "num_segments": {
          "type": "integer",
          "description": "The number of music segments to generate.",
          "required": "optional",
          "default": 2,
          "example": 4
        },
        "duration": {
          "type": "integer",
          "description": "The duration of the generated song in seconds.",
          "required": "optional",
          "default": 30,
          "example": 60
        },
        "use_audio_prompt": {
          "type": "boolean",
          "description": "Whether to use the provided audio file as a generation prompt.",
          "required": "optional",
          "default": false,
          "example": true
        },
        "audio_prompt_path": {
          "type": "string",
          "description": "Path (local or URL) to the audio file to be used as a prompt. This parameter is required if use_audio_prompt is True.",
          "required": "optional",
          "example": "input/audio/my_prompt.wav"
        },
        "output_dir": {
          "type": "string",
          "description": "Directory path to save the three final generated audio files.",
          "required": "optional",
          "default": "yue_music_output",
          "example": "output/my_generated_music"
        }
      },
      "examples": [
        {
          "task_description": "Generate a 60-second pop song with specified lyrics and save it to a custom directory.",
          "Act": {
            "tool": "yue_api",
            "arguments": {
              "genre_txt": "Pop",
              "lyrics_txt": "Oh, dancing in the moonlight, feeling so free tonight.",
              "duration": 60,
              "output_dir": "output/pop_song_project"
            }
          }
        },
        {
          "task_description": "Generate new music based on an existing audio file as a prompt.",
          "Act": {
            "tool": "yue_api",
            "arguments": {
              "use_audio_prompt": true,
              "audio_prompt_path": "input/prompts/melody_idea.wav",
              "num_segments": 4,
              "duration": 45
            }
          }
        },
        {
          "task_description": "Generate a short instrumental piece by only specifying the genre.",
          "Act": {
            "tool": "yue_api",
            "arguments": {
              "genre_txt": "Lofi hip hop",
              "duration": 30
            }
          }
        }
      ]
    },
    "voicecraft_tts_and_edit_api": {
      "description": "Performs text-to-speech (TTS), audio editing, and long-form text synthesis using the VoiceCraft model.",
      "detailed_description": "Provides three modes of operation: 'TTS' for generating speech from text (with optional voice cloning from a reference audio), 'Edit' for replacing a segment of an audio file with new speech, and 'Long TTS' for synthesizing long-form text by splitting it into sentences and generating them sequentially. The tool requires 'mode' and 'transcript' for all operations. An 'audio_path' is necessary for 'Edit' and 'Long TTS' modes, and also optional for voice cloning in 'TTS' mode. For 'Long TTS', the 'selected_sentence' to be synthesized is also required. It returns a dictionary containing the path to the generated audio file and the inferred transcript.",
      "parameters": {
        "mode": {
          "type": "string",
          "description": "The operating mode. 'TTS' for text-to-speech, 'Edit' for audio editing, 'Long TTS' for long text synthesis.",
          "required": true,
          "enum": [
            "TTS",
            "Edit",
            "Long TTS"
          ],
          "example": "TTS"
        },
        "transcript": {
          "type": "string",
          "description": "The text to be synthesized or used for editing.",
          "required": true,
          "example": "The quick brown fox jumps over the lazy dog."
        },
        "audio_path": {
          "type": "string",
          "description": "Path to the input audio file. Required for 'Edit' and 'Long TTS' modes. Optional for 'TTS' mode to enable voice cloning.",
          "required": false,
          "example": "input/original_audio.wav"
        },
        "output_path": {
          "type": "string",
          "description": "Path to save the generated audio file.",
          "required": false,
          "default": "output.wav",
          "example": "output/generated_speech.wav"
        },
        "seed": {
          "type": "number",
          "description": "Random seed for reproducibility. Use -1.0 for a random seed.",
          "required": false,
          "default": -1.0,
          "example": 42
        },
        "smart_transcript": {
          "type": "boolean",
          "description": "Whether to enable smart transcription.",
          "required": false,
          "default": true,
          "example": true
        },
        "prompt_end_time": {
          "type": "number",
          "description": "In 'Edit' mode, the end time of the audio prompt.",
          "required": false,
          "default": 3.675,
          "example": 3.5
        },
        "edit_start_time": {
          "type": "number",
          "description": "In 'Edit' mode, the start time of the segment to be edited.",
          "required": false,
          "default": 3.83,
          "example": 4.0
        },
        "edit_end_time": {
          "type": "number",
          "description": "In 'Edit' mode, the end time of the segment to be edited.",
          "required": false,
          "default": 5.113,
          "example": 5.0
        },
        "left_margin": {
          "type": "number",
          "description": "Left margin for the audio processing.",
          "required": false,
          "default": 0.08,
          "example": 0.1
        },
        "right_margin": {
          "type": "number",
          "description": "Right margin for the audio processing.",
          "required": false,
          "default": 0.08,
          "example": 0.1
        },
        "temperature": {
          "type": "number",
          "description": "Controls the randomness of the generation. Higher values mean more randomness.",
          "required": false,
          "default": 1.0,
          "example": 0.95
        },
        "top_p": {
          "type": "number",
          "description": "Nucleus sampling threshold.",
          "required": false,
          "default": 0.9,
          "example": 0.9
        },
        "top_k": {
          "type": "number",
          "description": "Top-k sampling. Set to 0 to disable.",
          "required": false,
          "default": 0.0,
          "example": 5
        },
        "sample_batch_size": {
          "type": "number",
          "description": "Sample batch size, which can affect the speech rate.",
          "required": false,
          "default": 2.0,
          "example": 2
        },
        "stop_repetition": {
          "type": "integer",
          "description": "Level for stopping repetition. Must be a number.",
          "required": false,
          "default": 3,
          "enum": [
            -1,
            1,
            2,
            3,
            4
          ],
          "example": 3
        },
        "kvcache": {
          "type": "integer",
          "description": "Whether to use KV caching. 1 for yes, 0 for no. Must be a number.",
          "required": false,
          "default": 1,
          "enum": [
            0,
            1
          ],
          "example": 1
        },
        "split_text": {
          "type": "string",
          "description": "In 'Long TTS' mode, how to split the text.",
          "required": false,
          "default": "Newline",
          "enum": [
            "Newline",
            "Sentence"
          ],
          "example": "Sentence"
        },
        "selected_sentence": {
          "type": [
            "string",
            "null"
          ],
          "description": "In 'Long TTS' mode, the specific sentence to process. Required for this mode. Should be omitted or null for 'TTS' and 'Edit' modes.",
          "required": false,
          "default": null,
          "example": "This is the sentence to be synthesized."
        },
        "codec_audio_sr": {
          "type": "number",
          "description": "Codec audio sample rate.",
          "required": false,
          "default": 16000.0,
          "example": 16000
        },
        "codec_sr": {
          "type": "number",
          "description": "Codec sample rate.",
          "required": false,
          "default": 50.0,
          "example": 50
        },
        "silence_tokens": {
          "type": "string",
          "description": "A string representing the list of silence tokens.",
          "required": false,
          "default": "[1388,1898,131]",
          "example": "[1388,1898,131]"
        }
      },
      "examples": [
        {
          "task_description": "Generate speech from a piece of text using the TTS mode, cloning the voice from a provided audio file.",
          "Act": {
            "tool": "VoiceCraftTool",
            "arguments": {
              "mode": "TTS",
              "transcript": "Hello world, this is a demonstration of text-to-speech with voice cloning.",
              "audio_path": "input/my_voice.wav",
              "output_path": "output/hello_world_cloned.wav"
            }
          }
        },
        {
          "task_description": "Edit a specific portion of an existing audio file.",
          "Act": {
            "tool": "VoiceCraftTool",
            "arguments": {
              "mode": "Edit",
              "transcript": "The quick brown fox jumps over the lazy cat.",
              "audio_path": "input/original_speech.wav",
              "edit_start_time": 3.83,
              "edit_end_time": 5.113,
              "output_path": "output/edited_speech.wav"
            }
          }
        },
        {
          "task_description": "Generate a single sentence as part of a long-form text synthesis.",
          "Act": {
            "tool": "VoiceCraftTool",
            "arguments": {
              "mode": "Long TTS",
              "transcript": "This is the first sentence. This is the second sentence which we will generate now.",
              "audio_path": "input/long_tts_prompt.wav",
              "output_path": "output/long_tts_sentence.wav",
              "split_text": "Sentence",
              "selected_sentence": "This is the second sentence which we will generate now."
            }
          }
        }
      ]
    },
    "image2music_api": {
      "description": "Generates music from a local image file or a web image URL using the image-to-music-v2 model.",
      "detailed_description": "This tool creates a musical piece based on a provided image. The image can be a local file path or a URL. The user can specify the output directory and filename for the generated MP3 file. If not provided, a filename is automatically generated based on the input image name and the current timestamp, and it's saved in the 'outputs/music' directory. The tool supports several underlying models for music generation, with 'Riffusion' being the default, which is generally faster for testing.",
      "parameters": {
        "image_source": {
          "type": "string",
          "description": "The source of your image. This can be a local file path (e.g., \"C:/images/photo.png\") or a URL to an image on the web (e.g., \"https://example.com/image.jpg\"). Required.",
          "required": true,
          "example": "https://example.com/image.jpg"
        },
        "output_dir": {
          "type": "string",
          "description": "The folder where you want to save the generated music file. If not specified, it defaults to the 'outputs/music' folder in the program directory.",
          "required": "optional",
          "default": "outputs/music",
          "example": "my_generated_music"
        },
        "output_filename": {
          "type": "string",
          "description": "The desired name for the output music file (e.g., \"my_song.mp3\"). If omitted, a name will be automatically generated from the image name and a timestamp.",
          "required": "optional",
          "example": "my_composition.mp3"
        },
        "model": {
          "type": "string",
          "description": "The model to use for music generation. 'Riffusion' is generally faster and suitable for quick tests.",
          "required": "optional",
          "default": "Riffusion",
          "enum": [
            "ACE Step",
            "AudioLDM-2",
            "Riffusion",
            "Mustango",
            "Stable Audio Open"
          ],
          "example": "ACE Step"
        },
        "hf_token": {
          "type": "string",
          "description": "Your Hugging Face access token, required if you need to use a private model.",
          "required": "optional",
          "example": "hf_xxxxxxxxxxxxxxxxxxxxxx"
        }
      },
      "examples": [
        {
          "task_description": "Generate music from a local image file and save it to the default directory.",
          "Act": {
            "tool": "image2music_api",
            "arguments": {
              "image_source": "C:/Users/Admin/Pictures/sunset.png"
            }
          }
        },
        {
          "task_description": "Generate music from an image URL, select the 'AudioLDM-2' model, and specify a custom output path and filename.",
          "Act": {
            "tool": "image2music_api",
            "arguments": {
              "image_source": "https://www.example.com/images/mountain_view.jpg",
              "output_dir": "D:/my_music/landscapes",
              "output_filename": "mountain_sonata.mp3",
              "model": "AudioLDM-2"
            }
          }
        },
        {
          "task_description": "Create a piece of music from a web image using the 'Stable Audio Open' model, allowing the system to auto-generate the filename.",
          "Act": {
            "tool": "image2music_api",
            "arguments": {
              "image_source": "https://anothersite.net/art/abstract.gif",
              "model": "Stable Audio Open",
              "output_dir": "outputs/abstract_tunes"
            }
          }
        }
      ]
    }
  }

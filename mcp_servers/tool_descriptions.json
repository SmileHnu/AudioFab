{
  "query_tool": {
    "description": "Query detailed information about any tool, including parameter specifications, usage examples, and functionality.",
    "detailed_description": "An essential metadata discovery tool that provides comprehensive documentation about any available tool in the system. Before using a new tool, it's highly recommended to query its specifications first to understand parameter requirements, optional configurations, default values, and see concrete usage examples. This tool supports intelligent documentation retrieval with rich formatting to facilitate proper tool usage.",
    "parameters": {
      "tool_name": {
        "type": "string",
        "description": "Name of the tool to query information about",
        "required": true
      }
    },
    "examples": [
      {
        "task_description": "Query detailed information about the load_audio tool",
        "Act": {
          "tool": "query_tool",
          "arguments": {
            "tool_name": "load_audio"
          }
        }
      }
    ]
  },
  "list_available_tools": {
    "description": "List all available tools with their brief descriptions.",
    "detailed_description": "Provides a comprehensive inventory of all available tools in the system with their concise descriptions. This is the ideal starting point for tool discovery. For detailed information about specific tools, including parameter requirements and usage examples, use the query_tool with the specific tool name.",
    "parameters": {},
    "examples": [
      {
        "task_description": "List all available tools",
        "Act": {
          "tool": "list_available_tools",
          "arguments": {}
        }
      }
    ]
  },
  "search_tools_by_task": {
    "description": "Intelligently search for relevant tools based on natural language task descriptions.",
    "detailed_description": "An advanced AI-powered tool discovery system that identifies the most suitable tools for any given task using state-of-the-art natural language understanding. The system leverages multilingual semantic vector embeddings combined with optimized keyword matching to deeply comprehend the user's intent, even when there are no exact keyword matches. The intelligent ranking algorithm analyzes tool capabilities against the task requirements, providing a relevance-sorted list of the most appropriate tools with confidence scores. This system excels at understanding complex, multi-step tasks and recommending the optimal tool combinations for efficient workflow completion.",
    "parameters": {
      "task_description": {
        "type": "string",
        "description": "Natural language description of the task you want to accomplish",
        "required": true
      }
    },
    "examples": [
      {
        "task_description": "Search for audio resampling tools",
        "Act": {
          "tool": "search_tools_by_task",
          "arguments": {
            "task_description": "How can I change the sample rate of an audio file?"
          }
        }
      },
      {
        "task_description": "Find voice cloning tools",
        "Act": {
          "tool": "search_tools_by_task",
          "arguments": {
            "task_description": "I need a speech synthesis tool that can mimic a specific voice"
          }
        }
      },
      {
        "task_description": "Find emotion analysis tools",
        "Act": {
          "tool": "search_tools_by_task",
          "arguments": {
            "task_description": "I need to analyze the emotions expressed in a speech recording"
          }
        }
      }
    ]
  },
  "load_audio": {
    "description": "加载音频数据。",
    "detailed_description": "该工具用于加载音频文件并可选择性地重采样到指定采样率。它会将加载的音频保存为临时文件并返回相关信息。",
    "parameters": {
      "audio_path": {
        "type": "string",
        "description": "音频文件路径",
        "required": true
      },
      "target_sr": {
        "type": "integer",
        "description": "目标采样率（可选）",
        "required": "optional"
      }
    },
    "examples": [
      {
        "task_description": "加载音频文件",
        "Act": {
          "tool": "load_audio",
          "arguments": {
            "audio_path": "input/my_audio.wav"
          }
        }
      },
      {
        "task_description": "加载音频并重采样到16kHz",
        "Act": {
          "tool": "load_audio",
          "arguments": {
            "audio_path": "input/my_audio.wav",
            "target_sr": 16000
          }
        }
      }
    ]
  },
  "resample_audio": {
    "description": "重采样音频。",
    "detailed_description": "该工具用于将音频从一个采样率转换到另一个采样率。它会保存重采样后的音频并返回相关信息。",
    "parameters": {
      "audio_path": {
        "type": "string",
        "description": "音频文件路径",
        "required": true
      },
      "orig_sr": {
        "type": "integer",
        "description": "原始采样率",
        "required": true
      },
      "target_sr": {
        "type": "integer",
        "description": "目标采样率",
        "required": true
      }
    },
    "examples": [
      {
        "task_description": "将44.1kHz音频重采样到16kHz",
        "Act": {
          "tool": "resample_audio",
          "arguments": {
            "audio_path": "output/audio/processed_20231215_123456.wav",
            "orig_sr": 44100,
            "target_sr": 16000
          }
        }
      }
    ]
  },
  "compute_stft": {
    "description": "计算短时傅里叶变换。",
    "detailed_description": "该工具计算音频信号的短时傅里叶变换（STFT），并将结果保存为NumPy数组文件。",
    "parameters": {
      "audio_path": {
        "type": "string",
        "description": "音频文件路径",
        "required": true
      },
      "n_fft": {
        "type": "integer",
        "description": "FFT窗口大小",
        "required": "optional",
        "default": 2048
      },
      "hop_length": {
        "type": "integer",
        "description": "帧移",
        "required": "optional",
        "default": 512
      }
    },
    "examples": [
      {
        "task_description": "计算音频的STFT",
        "Act": {
          "tool": "compute_stft",
          "arguments": {
            "audio_path": "output/audio/processed_20231215_123456.wav",
            "n_fft": 2048,
            "hop_length": 512
          }
        }
      }
    ]
  },
  "compute_mfcc": {
    "description": "计算MFCC特征。",
    "detailed_description": "该工具计算音频的梅尔频率倒谱系数（MFCC）特征，这是一种常用于语音识别和音频分析的特征。",
    "parameters": {
      "audio_path": {
        "type": "string",
        "description": "音频文件路径",
        "required": true
      },
      "n_mfcc": {
        "type": "integer",
        "description": "MFCC系数数量",
        "required": "optional",
        "default": 13
      }
    },
    "examples": [
      {
        "task_description": "计算音频的MFCC特征",
        "Act": {
          "tool": "compute_mfcc",
          "arguments": {
            "audio_path": "output/audio/processed_20231215_123456.wav",
            "n_mfcc": 13
          }
        }
      }
    ]
  },
  "compute_mel_spectrogram": {
    "description": "计算梅尔频谱图并生成可视化图像。",
    "detailed_description": "该工具计算音频的梅尔频谱图，一种基于人类听觉感知的频谱表示，并可以选择性地生成可视化图像。在audio_processor中生成可视化，在dsp_processor中只生成数据文件。",
    "parameters": {
      "audio_path": {
        "type": "string",
        "description": "音频文件路径",
        "required": true
      },
      "n_mels": {
        "type": "integer",
        "description": "梅尔滤波器组数量",
        "required": "optional",
        "default": 128
      }
    },
    "examples": [
      {
        "task_description": "计算并可视化音频的梅尔频谱图",
        "Act": {
          "tool": "compute_mel_spectrogram",
          "arguments": {
            "audio_path": "output/audio/processed_20231215_123456.wav",
            "n_mels": 128
          }
        }
      }
    ]
  },
  "add_reverb": {
    "description": "添加混响效果。",
    "detailed_description": "该工具对音频添加混响效果，可用于模拟不同空间声学环境下的声音。",
    "parameters": {
      "audio_path": {
        "type": "string",
        "description": "音频文件路径",
        "required": true
      },
      "room_scale": {
        "type": "float",
        "description": "房间大小因子（0-1）",
        "required": "optional",
        "default": 0.8
      }
    },
    "examples": [
      {
        "task_description": "为音频添加混响效果",
        "Act": {
          "tool": "add_reverb",
          "arguments": {
            "audio_path": "output/audio/processed_20231215_123456.wav",
            "room_scale": 0.8
          }
        }
      }
    ]
  },
  "mix_audio": {
    "description": "混合多个音频。",
    "detailed_description": "该工具将多个音频文件混合成一个单一的音频文件，可以指定每个音频的混合权重。",
    "parameters": {
      "audio_paths": {
        "type": "array",
        "description": "音频文件路径列表",
        "required": true
      },
      "weights": {
        "type": "array",
        "description": "混合权重列表（可选）",
        "required": "optional"
      }
    },
    "examples": [
      {
        "task_description": "混合两个音频文件",
        "Act": {
          "tool": "mix_audio",
          "arguments": {
            "audio_paths": [
              "output/audio/processed_1.wav",
              "output/audio/processed_2.wav"
            ],
            "weights": [
              0.6,
              0.4
            ]
          }
        }
      }
    ]
  },
  "apply_fade": {
    "description": "应用淡入淡出效果。",
    "detailed_description": "该工具对音频应用淡入淡出效果，可以使音频的开始和结束更平滑。",
    "parameters": {
      "audio_path": {
        "type": "string",
        "description": "音频文件路径",
        "required": true
      },
      "fade_duration": {
        "type": "float",
        "description": "淡入淡出时长（秒）",
        "required": true
      }
    },
    "examples": [
      {
        "task_description": "为音频添加1秒的淡入淡出效果",
        "Act": {
          "tool": "apply_fade",
          "arguments": {
            "audio_path": "output/audio/processed_20231215_123456.wav",
            "fade_duration": 1.0
          }
        }
      }
    ]
  },
  "serve_local_audio": {
    "description": "将本地音频文件转换为可访问的URL。",
    "detailed_description": "该工具启动一个本地HTTP服务器，使音频文件可以通过网络URL访问，便于在网页或其他应用中播放。",
    "parameters": {
      "audio_path": {
        "type": "string",
        "description": "本地音频文件路径",
        "required": true
      },
      "port": {
        "type": "integer",
        "description": "HTTP服务器端口号",
        "required": "optional",
        "default": 8000
      }
    },
    "examples": [
      {
        "task_description": "将处理后的音频文件通过HTTP服务器提供访问",
        "Act": {
          "tool": "serve_local_audio",
          "arguments": {
            "audio_path": "output/audio/processed_20231215_123456.wav",
            "port": 8080
          }
        }
      }
    ]
  },
  "stop_audio_server": {
    "description": "停止音频文件上传服务器，并释放资源。",
    "detailed_description": "该工具停止由serve_local_audio启动的HTTP服务器，并清理相关的临时文件。",
    "parameters": {},
    "examples": [
      {
        "task_description": "停止正在运行的音频服务器",
        "Act": {
          "tool": "stop_audio_server",
          "arguments": {}
        }
      }
    ]
  },
  "convert_audio_format": {
    "description": "将音频从一种格式转换为另一种格式。",
    "detailed_description": "该工具支持多种音频格式之间的转换，如WAV、MP3、OGG、FLAC等，同时可以调整采样率、通道数和位深度。",
    "parameters": {
      "input_path": {
        "type": "string",
        "description": "输入音频文件路径",
        "required": true
      },
      "output_format": {
        "type": "string",
        "description": "输出格式（如 'wav', 'mp3', 'ogg', 'flac' 等）",
        "required": "optional",
        "default": "wav"
      },
      "sample_rate": {
        "type": "integer",
        "description": "目标采样率（可选）",
        "required": "optional"
      },
      "channels": {
        "type": "integer",
        "description": "目标通道数（可选，1=单声道，2=立体声）",
        "required": "optional"
      },
      "bit_depth": {
        "type": "integer",
        "description": "目标位深度（可选，仅适用于WAV格式）",
        "required": "optional"
      }
    },
    "examples": [
      {
        "task_description": "将MP3文件转换为44.1kHz、16位的WAV文件",
        "Act": {
          "tool": "convert_audio_format",
          "arguments": {
            "input_path": "input/music.mp3",
            "output_format": "wav",
            "sample_rate": 44100,
            "bit_depth": 16
          }
        }
      }
    ]
  },
  "trim_audio": {
    "description": "裁剪音频文件的指定时间区间。",
    "detailed_description": "该工具用于从音频文件中提取指定时间段的内容，并可选择性地应用淡入淡出效果以使过渡更平滑。",
    "parameters": {
      "input_path": {
        "type": "string",
        "description": "输入音频文件路径",
        "required": true
      },
      "start_time": {
        "type": "float",
        "description": "开始时间（秒）",
        "required": true
      },
      "end_time": {
        "type": "float",
        "description": "结束时间（秒）",
        "required": true
      },
      "fade_in": {
        "type": "float",
        "description": "淡入时长（秒，可选）",
        "required": "optional"
      },
      "fade_out": {
        "type": "float",
        "description": "淡出时长（秒，可选）",
        "required": "optional"
      }
    },
    "examples": [
      {
        "task_description": "从音频文件中提取10秒到30秒的部分，并应用0.5秒的淡入淡出",
        "Act": {
          "tool": "trim_audio",
          "arguments": {
            "input_path": "input/long_audio.wav",
            "start_time": 10.0,
            "end_time": 30.0,
            "fade_in": 0.5,
            "fade_out": 0.5
          }
        }
      }
    ]
  },
  "align_audio_lengths": {
    "description": "将多个音频文件对齐到相同长度。",
    "detailed_description": "该工具使用多种方法（如填充、裁剪、循环或拉伸）将一组音频文件调整到相同的时长，便于后续处理或混合。",
    "parameters": {
      "audio_paths": {
        "type": "array",
        "description": "音频文件路径列表（至少2个）",
        "required": true
      },
      "target_duration": {
        "type": "float",
        "description": "目标时长（秒，可选）。如果不指定，将使用最长音频的时长",
        "required": "optional"
      },
      "method": {
        "type": "string",
        "description": "对齐方法: 'pad'（填充）, 'trim'（裁剪）, 'loop'（循环）或 'stretch'（拉伸）",
        "required": "optional",
        "default": "pad"
      },
      "fade_duration": {
        "type": "float",
        "description": "淡入淡出时长（秒），用于平滑过渡",
        "required": "optional",
        "default": 0.1
      }
    },
    "examples": [
      {
        "task_description": "将两个音频文件对齐到15秒长度，使用填充方法并添加0.2秒淡入淡出",
        "Act": {
          "tool": "align_audio_lengths",
          "arguments": {
            "audio_paths": [
              "output/audio/vocal.wav",
              "output/audio/accompaniment.wav"
            ],
            "target_duration": 15.0,
            "method": "pad",
            "fade_duration": 0.2
          }
        }
      }
    ]
  },
  "get_gpu_info": {
    "description": "获取 GPU 信息。",
    "detailed_description": "该工具获取系统中可用的GPU信息，包括数量、型号、内存使用情况等。",
    "parameters": {},
    "examples": [
      {
        "task_description": "查询系统中的GPU信息",
        "Act": {
          "tool": "get_gpu_info",
          "arguments": {}
        }
      }
    ]
  },
  "set_gpu_device": {
    "description": "设置当前使用的 GPU 设备。",
    "detailed_description": "该工具用于切换当前活跃的GPU设备，在多GPU系统上进行操作时特别有用。",
    "parameters": {
      "device_id": {
        "type": "integer",
        "description": "GPU 设备 ID",
        "required": true
      }
    },
    "examples": [
      {
        "task_description": "将活跃GPU设备设置为ID为0的设备",
        "Act": {
          "tool": "set_gpu_device",
          "arguments": {
            "device_id": 0
          }
        }
      }
    ]
  },
  "load_numpy_file": {
    "description": "加载 .npy 格式的 NumPy 数组文件。",
    "detailed_description": "该工具用于加载NumPy数组文件并返回其维度、数据类型和基本统计信息等。",
    "parameters": {
      "file_path": {
        "type": "string",
        "description": "NumPy 文件路径",
        "required": true
      }
    },
    "examples": [
      {
        "task_description": "加载NumPy数组文件并查看其基本信息",
        "Act": {
          "tool": "load_numpy_file",
          "arguments": {
            "file_path": "output/tensors/stft_20231215_123456.npy"
          }
        }
      }
    ]
  },
  "load_torch_file": {
    "description": "加载 .pth 格式的 PyTorch 张量文件。",
    "detailed_description": "该工具用于加载PyTorch张量文件并返回其维度、数据类型和设备信息等。可以指定加载到CPU或GPU设备。",
    "parameters": {
      "file_path": {
        "type": "string",
        "description": "PyTorch 文件路径",
        "required": true
      },
      "to_device": {
        "type": "string",
        "description": "加载到的设备 ('cpu' 或 'cuda')",
        "required": "optional"
      }
    },
    "examples": [
      {
        "task_description": "加载PyTorch张量文件到CPU并查看其基本信息",
        "Act": {
          "tool": "load_torch_file",
          "arguments": {
            "file_path": "output/tensors/model_weights.pth",
            "to_device": "cpu"
          }
        }
      }
    ]
  },
  "convert_numpy_to_tensor": {
    "description": "将 NumPy 数组转换为 PyTorch 张量并保存。",
    "detailed_description": "该工具将NumPy数组文件加载并转换为PyTorch张量，可以指定目标设备，并将结果保存为.pth文件。",
    "parameters": {
      "numpy_path": {
        "type": "string",
        "description": "NumPy 文件路径",
        "required": true
      },
      "tensor_path": {
        "type": "string",
        "description": "输出的张量文件路径（可选，默认在相同位置生成）",
        "required": "optional"
      },
      "to_device": {
        "type": "string",
        "description": "转换后的设备（'cpu' 或 'cuda'）",
        "required": "optional"
      }
    },
    "examples": [
      {
        "task_description": "将NumPy数组转换为CUDA设备上的PyTorch张量",
        "Act": {
          "tool": "convert_numpy_to_tensor",
          "arguments": {
            "numpy_path": "output/tensors/mel_spec_20231215_123456.npy",
            "to_device": "cuda"
          }
        }
      }
    ]
  },
  "convert_tensor_to_numpy": {
    "description": "将 PyTorch 张量转换为 NumPy 数组并保存。",
    "detailed_description": "该工具将PyTorch张量文件加载并转换为NumPy数组，然后保存为.npy文件。",
    "parameters": {
      "tensor_path": {
        "type": "string",
        "description": "PyTorch 张量文件路径",
        "required": true
      },
      "numpy_path": {
        "type": "string",
        "description": "输出的 NumPy 文件路径（可选，默认在相同位置生成）",
        "required": "optional"
      }
    },
    "examples": [
      {
        "task_description": "将PyTorch张量转换为NumPy数组",
        "Act": {
          "tool": "convert_tensor_to_numpy",
          "arguments": {
            "tensor_path": "output/tensors/model_features.pth"
          }
        }
      }
    ]
  },
  "move_tensor_to_device": {
    "description": "将张量移动到指定设备（CPU 或 CUDA）。",
    "detailed_description": "该工具将PyTorch张量从一个设备移动到另一个设备（例如从CPU到GPU或反之），并保存结果。",
    "parameters": {
      "tensor_path": {
        "type": "string",
        "description": "张量文件路径",
        "required": true
      },
      "device": {
        "type": "string",
        "description": "目标设备 ('cpu' 或 'cuda[:id]')",
        "required": true
      },
      "output_path": {
        "type": "string",
        "description": "输出文件路径（可选）",
        "required": "optional"
      }
    },
    "examples": [
      {
        "task_description": "将张量从CPU移动到GPU设备",
        "Act": {
          "tool": "move_tensor_to_device",
          "arguments": {
            "tensor_path": "output/tensors/features.pth",
            "device": "cuda"
          }
        }
      }
    ]
  },
  "concatenate_tensors": {
    "description": "沿指定维度连接多个张量。",
    "detailed_description": "该工具将多个PyTorch张量文件加载并沿指定维度连接成一个新的张量，然后保存结果。",
    "parameters": {
      "tensor_paths": {
        "type": "array",
        "description": "张量文件路径列表",
        "required": true
      },
      "dim": {
        "type": "integer",
        "description": "连接的维度",
        "required": "optional",
        "default": 0
      },
      "output_path": {
        "type": "string",
        "description": "输出文件路径（可选）",
        "required": "optional"
      }
    },
    "examples": [
      {
        "task_description": "沿第0维连接两个特征张量",
        "Act": {
          "tool": "concatenate_tensors",
          "arguments": {
            "tensor_paths": [
              "output/tensors/features1.pth",
              "output/tensors/features2.pth"
            ],
            "dim": 0
          }
        }
      }
    ]
  },
  "split_tensor": {
    "description": "沿指定维度拆分张量。",
    "detailed_description": "该工具将PyTorch张量文件加载并沿指定维度拆分成多个小张量，然后将结果保存到指定目录。",
    "parameters": {
      "tensor_path": {
        "type": "string",
        "description": "张量文件路径",
        "required": true
      },
      "dim": {
        "type": "integer",
        "description": "拆分的维度",
        "required": "optional",
        "default": 0
      },
      "sections": {
        "type": "integer or array",
        "description": "拆分方式：整数表示等大小的块，数组表示按索引拆分",
        "required": "optional",
        "default": 2
      },
      "output_dir": {
        "type": "string",
        "description": "输出目录（可选）",
        "required": "optional"
      }
    },
    "examples": [
      {
        "task_description": "将张量沿第0维拆分为两部分",
        "Act": {
          "tool": "split_tensor",
          "arguments": {
            "tensor_path": "output/tensors/features.pth",
            "dim": 0,
            "sections": 2
          }
        }
      }
    ]
  },
  "save_tensor": {
    "description": "保存张量数据到 PyTorch .pth 文件。",
    "detailed_description": "该工具根据指定的形状和可选的值创建一个PyTorch张量，并将其保存为.pth文件。如果不提供值，则生成随机张量。",
    "parameters": {
      "tensor_data": {
        "type": "object",
        "description": "张量数据字典，包含形状、值和数据类型",
        "required": true
      },
      "output_path": {
        "type": "string",
        "description": "输出文件路径（可选）",
        "required": "optional"
      }
    },
    "examples": [
      {
        "task_description": "创建并保存一个3x3的单位矩阵张量",
        "Act": {
          "tool": "save_tensor",
          "arguments": {
            "tensor_data": {
              "shape": [
                3,
                3
              ],
              "values": [
                1,
                0,
                0,
                0,
                1,
                0,
                0,
                0,
                1
              ],
              "dtype": "float32"
            },
            "output_path": "output/tensors/identity_matrix.pth"
          }
        }
      }
    ]
  },
  "tensor_operations": {
    "description": "对张量执行基本操作。",
    "detailed_description": "该工具对PyTorch张量执行各种基本操作，如重塑、转置、克隆、加法、乘法、求均值、求和和求范数等，并保存结果。",
    "parameters": {
      "tensor_path": {
        "type": "string",
        "description": "张量文件路径",
        "required": true
      },
      "operation": {
        "type": "string",
        "description": "操作类型：reshape、transpose、clone、add、multiply、mean、sum、norm",
        "required": true
      },
      "params": {
        "type": "object",
        "description": "操作参数（可选）",
        "required": "optional"
      },
      "output_path": {
        "type": "string",
        "description": "输出文件路径（可选）",
        "required": "optional"
      }
    },
    "examples": [
      {
        "task_description": "将张量重塑为新的形状",
        "Act": {
          "tool": "tensor_operations",
          "arguments": {
            "tensor_path": "output/tensors/features.pth",
            "operation": "reshape",
            "params": {
              "shape": [
                1,
                -1
              ]
            }
          }
        }
      },
      {
        "task_description": "计算张量的均值",
        "Act": {
          "tool": "tensor_operations",
          "arguments": {
            "tensor_path": "output/tensors/features.pth",
            "operation": "mean"
          }
        }
      }
    ]
  },
  "AudioXTool": {
    "description": "Generate audio or video content using the AudioX multimodal model with support for text, audio, and video inputs.",
    "detailed_description": "AudioXTool leverages the AudioX model for multimodal audio/video generation. It can generate content from various input modalities including text prompts, audio references, and video references. The tool supports multiple use cases such as: Text-to-Audio (T2A), Audio-to-Audio (A2A) for audio extension or transformation, Video-to-Audio (V2A) for generating soundtracks for videos, and multimodal combinations (e.g., Text+Video-to-Audio). Invalid or missing inputs are handled gracefully with default prompts, ensuring the model always generates output. Generated audio can be optionally merged with the input video to create a complete audiovisual experience. The tool provides detailed control over generation parameters like duration, inference steps, sampler settings, and conditioning strength.",
    "parameters": {
      "text_prompt": {
        "type": "string",
        "description": "Text description for the content to generate. Can be used alone or combined with other modalities.",
        "required": "optional",
        "example": "A peaceful melody with soft piano and gentle rain in the background."
      },
      "audio_path": {
        "type": "string",
        "description": "Path to an audio file to use as a reference for generation. If invalid or empty, a default silent audio prompt will be used.",
        "required": "optional"
      },
      "video_path": {
        "type": "string",
        "description": "Path to a video file to use as a reference for generation. If invalid or empty, a default blank video prompt will be used.",
        "required": "optional"
      },
      "output_audio_path": {
        "type": "string",
        "description": "Custom path to save the generated audio file (WAV format). Default: output/audio/audiox_audio_[timestamp].wav",
        "required": "optional"
      },
      "output_video_path": {
        "type": "string",
        "description": "Custom path to save the generated video file (MP4 format) with the generated audio. Default: output/video/audiox_video_[timestamp].mp4. Video is only generated if a valid input video_path was provided.",
        "required": "optional"
      },
      "seconds_start": {
        "type": "integer",
        "description": "The start time (in seconds) for conditioning and generation window.",
        "required": "optional",
        "default": 0
      },
      "seconds_total": {
        "type": "integer",
        "description": "Total duration (in seconds) of the generated content.",
        "required": "optional",
        "default": 10
      },
      "steps": {
        "type": "integer",
        "description": "Number of inference steps. Higher values may improve quality but increase generation time.",
        "required": "optional",
        "default": 250
      },
      "cfg_scale": {
        "type": "float",
        "description": "Classifier-free guidance scale. Controls how strongly the model adheres to the conditioning inputs.",
        "required": "optional",
        "default": 7.0
      },
      "sigma_min": {
        "type": "float",
        "description": "Minimum sigma value for the sampler.",
        "required": "optional",
        "default": 0.3
      },
      "sigma_max": {
        "type": "float",
        "description": "Maximum sigma value for the sampler.",
        "required": "optional",
        "default": 500.0
      },
      "sampler_type": {
        "type": "string",
        "description": "Type of sampler to use for generation (e.g., \"dpmpp-3m-sde\").",
        "required": "optional",
        "default": "dpmpp-3m-sde"
      },
      "negative_prompt": {
        "type": "string",
        "description": "Text describing what you don't want in the generated content.",
        "required": "optional"
      },
      "seed": {
        "type": "integer",
        "description": "Random seed for reproducibility. If None, a random seed is used.",
        "required": "optional"
      },
      "device_selection": {
        "type": "string",
        "description": "Computing device to use for generation ('cuda' or 'cpu').",
        "required": "optional",
        "default": "cuda (if available) or cpu"
      }
    },
    "examples": [
      {
        "task_description": "Generate peaceful piano music with a text prompt",
        "Act": {
          "tool": "AudioXTool",
          "arguments": {
            "text_prompt": "A peaceful piano piece with soft strings in the background",
            "output_audio_path": "output/music/peaceful_piano.wav",
            "seconds_total": 15,
            "steps": 250,
            "cfg_scale": 7.0
          }
        }
      },
      {
        "task_description": "Generate audio for a video and merge them together",
        "Act": {
          "tool": "AudioXTool",
          "arguments": {
            "video_path": "input/video/silent_scene.mp4",
            "text_prompt": "Ambient nature sounds with gentle wind and distant birds",
            "output_audio_path": "output/audio/nature_sounds.wav",
            "output_video_path": "output/video/scene_with_audio.mp4"
          }
        }
      },
      {
        "task_description": "Extend and transform an existing audio sample",
        "Act": {
          "tool": "AudioXTool",
          "arguments": {
            "audio_path": "input/audio/short_melody.wav",
            "text_prompt": "Extend this melody with orchestral accompaniment",
            "output_audio_path": "output/audio/extended_orchestral.wav",
            "seconds_total": 30,
            "seed": 12345
          }
        }
      }
    ]
  },
  "ACEStepTool": {
    "description": "Generate high-quality music using the ACE-Step foundation model with support for text prompts, lyrics, and audio-to-audio generation.",
    "detailed_description": "ACEStepTool is a powerful music generation tool that leverages the ACE-Step foundation model to create original music across diverse genres. It supports multiple generation modes: Text2Music (creating music from text descriptions), Retake (adding variations to existing music), Repainting (selectively regenerating parts of music), Editing (modifying existing music by changing tags or lyrics), Extending (adding music to the beginning or end of an existing piece), and Audio2Audio (generating music based on a reference audio). The tool offers extensive customization options for both the creative aspects (like prompts and lyrics) and technical parameters (like diffusion settings and scheduler types). It also supports LoRA adaptations for specialized styles or genres.",
    "parameters": {
      "task": {
        "type": "string",
        "description": "The type of music generation task to perform.",
        "required": "optional",
        "default": "text2music",
        "enum": [
          "text2music",
          "retake",
          "repaint",
          "edit",
          "extend",
          "audio2audio"
        ]
      },
      "prompt": {
        "type": "string",
        "description": "Text description for the music to generate. Can include genres, moods, instruments, scene descriptions, etc.",
        "required": "optional",
        "example": "Epic orchestral music with powerful drums and emotional strings, suitable for a fantasy battle scene."
      },
      "lyrics": {
        "type": "string",
        "description": "Lyrics for the generated music. Can include structure tags like [verse], [chorus], and [bridge].",
        "required": "optional",
        "example": "[verse] Walking through the rain, memories flooding back\n[chorus] I remember everything, the good times and the bad"
      },
      "audio_prompt": {
        "type": "string",
        "description": "Path to a reference audio file for audio2audio generation. The generated music will be influenced by this audio.",
        "required": "optional"
      },
      "ref_audio_strength": {
        "type": "number",
        "description": "Strength of reference audio influence (0-1, higher = more influence).",
        "required": "optional",
        "default": 0.5
      },
      "output_path": {
        "type": "string",
        "description": "Custom path to save the generated audio file. If not provided, a default path will be used.",
        "required": "optional"
      },
      "format": {
        "type": "string",
        "description": "Output audio format.",
        "required": "optional",
        "default": "wav"
      },
      "audio_duration": {
        "type": "number",
        "description": "Duration of generated audio in seconds.",
        "required": "optional",
        "default": 30.0
      },
      "infer_step": {
        "type": "integer",
        "description": "Number of inference steps. Higher values produce better quality but take longer to process.",
        "required": "optional",
        "default": 60
      },
      "guidance_scale": {
        "type": "number",
        "description": "Classifier-free guidance scale. Higher values result in stronger adherence to the prompt.",
        "required": "optional",
        "default": 15.0
      },
      "scheduler_type": {
        "type": "string",
        "description": "Type of diffusion scheduler to use.",
        "required": "optional",
        "default": "euler",
        "enum": [
          "euler",
          "heun",
          "pingpong"
        ]
      },
      "cfg_type": {
        "type": "string",
        "description": "Type of classifier-free guidance method.",
        "required": "optional",
        "default": "apg",
        "enum": [
          "apg",
          "cfg",
          "cfg_star"
        ]
      },
      "omega_scale": {
        "type": "number",
        "description": "Scale for omega parameter in diffusion process.",
        "required": "optional",
        "default": 10.0
      },
      "seed": {
        "type": "integer",
        "description": "Random seed for reproducible generation. If not provided, a random seed will be used.",
        "required": "optional"
      },
      "guidance_interval": {
        "type": "number",
        "description": "Interval for applying guidance during generation.",
        "required": "optional",
        "default": 0.5
      },
      "guidance_interval_decay": {
        "type": "number",
        "description": "Decay rate for guidance scale during generation.",
        "required": "optional",
        "default": 0.0
      },
      "min_guidance_scale": {
        "type": "number",
        "description": "Minimum value for guidance scale after decay.",
        "required": "optional",
        "default": 3.0
      },
      "use_erg_tag": {
        "type": "boolean",
        "description": "Whether to use enhanced tag generation.",
        "required": "optional",
        "default": true
      },
      "use_erg_lyric": {
        "type": "boolean",
        "description": "Whether to use enhanced lyric generation.",
        "required": "optional",
        "default": true
      },
      "use_erg_diffusion": {
        "type": "boolean",
        "description": "Whether to use enhanced diffusion process.",
        "required": "optional",
        "default": true
      },
      "oss_steps": {
        "type": "string",
        "description": "Comma-separated list of one-step sampling steps.",
        "required": "optional"
      },
      "guidance_scale_text": {
        "type": "number",
        "description": "Guidance scale for text in double condition mode.",
        "required": "optional",
        "default": 0.0
      },
      "guidance_scale_lyric": {
        "type": "number",
        "description": "Guidance scale for lyrics in double condition mode.",
        "required": "optional",
        "default": 0.0
      },
      "retake_variance": {
        "type": "number",
        "description": "Variance for retake generation (0-1, higher = more different from original).",
        "required": "optional",
        "default": 0.5
      },
      "repaint_start": {
        "type": "integer",
        "description": "Start time (seconds) for repainting a specific section.",
        "required": "optional",
        "default": 0
      },
      "repaint_end": {
        "type": "integer",
        "description": "End time (seconds) for repainting a specific section.",
        "required": "optional",
        "default": 0
      },
      "src_audio_path": {
        "type": "string",
        "description": "Path to source audio for edit/repaint/extend tasks.",
        "required": "optional"
      },
      "edit_target_prompt": {
        "type": "string",
        "description": "Target prompt for edit task (the new prompt to transform the music towards).",
        "required": "optional"
      },
      "edit_target_lyrics": {
        "type": "string",
        "description": "Target lyrics for edit task (the new lyrics to transform the music towards).",
        "required": "optional",
        "example": "[verse] The city lights shine bright tonight\n[chorus] Under the stars we dance"
      },
      "edit_n_min": {
        "type": "number",
        "description": "Minimum normalized time step for edit diffusion.",
        "required": "optional",
        "default": 0.0
      },
      "edit_n_max": {
        "type": "number",
        "description": "Maximum normalized time step for edit diffusion.",
        "required": "optional",
        "default": 1.0
      },
      "edit_n_avg": {
        "type": "integer",
        "description": "Number of prediction averages for edit diffusion.",
        "required": "optional",
        "default": 1
      },
      "lora_name_or_path": {
        "type": "string",
        "description": "Path or name of a LoRA adaptation to use.If what you need to generate is a Chinese rap-style song, then use it; otherwise, don't use it.set it to 'none' if you don't want to use it.",
        "required": "optional",
        "default": "/home/chengz/LAMs/pre_train_models/models--ACE-Step--ACE-Step-v1-chinese-rap-LoRA"
      },
      "device_id": {
        "type": "integer",
        "description": "GPU device ID to use.",
        "required": "optional",
        "default": 0
      },
      "bf16": {
        "type": "boolean",
        "description": "Whether to use bfloat16 precision.",
        "required": "optional",
        "default": true
      },
      "torch_compile": {
        "type": "boolean",
        "description": "Whether to use torch.compile for optimization.",
        "required": "optional",
        "default": false
      },
      "cpu_offload": {
        "type": "boolean",
        "description": "Whether to offload model to CPU when not in use.",
        "required": "optional",
        "default": false
      },
      "overlapped_decode": {
        "type": "boolean",
        "description": "Whether to use overlapped decoding for long audio.",
        "required": "optional",
        "default": false
      },
      "debug": {
        "type": "boolean",
        "description": "Whether to print debug information.",
        "required": "optional",
        "default": false
      }
    },
    "examples": [
      {
        "task_description": "Generate epic orchestral music from a text prompt",
        "Act": {
          "tool": "ACEStepTool",
          "arguments": {
            "task": "text2music",
            "prompt": "Epic orchestral music with powerful drums and emotional strings, suitable for a fantasy battle scene",
            "audio_duration": 60.0,
            "guidance_scale": 15.0,
            "infer_step": 100
          }
        }
      },
      {
        "task_description": "Generate music with lyrics about a rainy day",
        "Act": {
          "tool": "ACEStepTool",
          "arguments": {
            "prompt": "Melancholic piano ballad with soft rain sounds in the background",
            "lyrics": "[verse] Walking through the rain, memories flooding back\n[chorus] I remember everything, the good times and the bad",
            "audio_duration": 45.0,
            "output_path": "output/music/rainy_day_ballad.wav"
          }
        }
      },
      {
        "task_description": "Generate a variation of an existing music piece",
        "Act": {
          "tool": "ACEStepTool",
          "arguments": {
            "task": "retake",
            "src_audio_path": "input/audio/original_song.wav",
            "retake_variance": 0.3,
            "seed": 12345
          }
        }
      },
      {
        "task_description": "Generate Chinese rap using a specialized LoRA model",
        "Act": {
          "tool": "ACEStepTool",
          "arguments": {
            "prompt": "Modern Chinese rap with heavy beats",
            "lyrics": "[verse] 我的生活充满了挑战\n[chorus] 但我从不放弃",
            "lora_name_or_path": "/home/chengz/LAMs/pre_train_models/models--ACE-Step--ACE-Step-v1-chinese-rap-LoRA",
            "audio_duration": 30.0
          }
        }
      },
      {
        "task_description": "Generate music inspired by a reference audio",
        "Act": {
          "tool": "ACEStepTool",
          "arguments": {
            "task": "audio2audio",
            "audio_prompt": "input/audio/reference.wav",
            "ref_audio_strength": 0.7,
            "audio_duration": 60.0
          }
        }
      },
      {
        "task_description": "Edit a specific section of an existing music piece",
        "Act": {
          "tool": "ACEStepTool",
          "arguments": {
            "task": "repaint",
            "src_audio_path": "input/audio/original_piece.wav",
            "repaint_start": 30,
            "repaint_end": 60,
            "prompt": "Add a dramatic crescendo with strings",
            "output_path": "output/music/edited_piece.wav"
          }
        }
      }
    ]
  },
  "MusicGenTool": {
    "description": "Generate music using Facebook's MusicGen model based on text prompts and optional melody.",
    "detailed_description": "MusicGenTool leverages Facebook Audiocraft's MusicGen model to generate high-quality music based on text descriptions and optional melody input. The tool can create music in various styles and genres described by natural language prompts. You can also provide a melody audio file to guide the generation process, creating music that follows the melodic structure of the reference. The maximum generation duration is limited to 30 seconds. The tool supports batch generation of multiple musical pieces at once.",
    "parameters": {
      "prompt": {
        "type": "string or list",
        "description": "Text description for the music to generate. Can be a single string or a list of strings for batch generation.",
        "required": true,
        "example": "80s pop track with bassy drums and synth"
      },
      "melody_path": {
        "type": "string",
        "description": "Path to a melody audio file to use as reference for the generation.",
        "required": "optional",
        "example": "input/melodies/piano_melody.wav"
      },
      "melody_sample_rate": {
        "type": "integer",
        "description": "Sample rate of the melody file if provided.",
        "required": "optional",
        "default": 44100
      },
      "output_path": {
        "type": "string",
        "description": "Custom path to save the generated audio. If not provided, a default path will be used.",
        "required": "optional"
      },
      "format": {
        "type": "string",
        "description": "Output audio format.",
        "required": "optional",
        "default": "wav"
      },
      "duration": {
        "type": "float",
        "description": "Duration of generated audio in seconds (maximum 30 seconds).",
        "required": "optional",
        "default": 10.0
      },
      "guidance_scale": {
        "type": "float",
        "description": "Classifier-free guidance scale (higher = more adherence to prompt).",
        "required": "optional",
        "default": 3.0
      },
      "seed": {
        "type": "integer",
        "description": "Random seed for reproducible generation.",
        "required": "optional"
      },
      "apply_loudness_normalization": {
        "type": "boolean",
        "description": "Whether to apply loudness normalization to the generated audio.",
        "required": "optional",
        "default": true
      },
      "model_path": {
        "type": "string",
        "description": "Path to the MusicGen model.",
        "required": "optional",
        "default": "/home/chengz/LAMs/pre_train_models/models--facebook--musicgen-melody"
      },
      "device": {
        "type": "string",
        "description": "Computing device to use ('cuda' or 'cpu').",
        "required": "optional",
        "default": "cuda"
      },
      "dtype": {
        "type": "string",
        "description": "Precision to use for model inference ('float16' or 'float32').",
        "required": "optional",
        "default": "float16"
      },
      "batch_size": {
        "type": "integer",
        "description": "Number of audio samples to generate in parallel.",
        "required": "optional",
        "default": 1
      }
    },
    "examples": [
      {
        "task_description": "Generate a pop music track with a specific style",
        "Act": {
          "tool": "MusicGenTool",
          "arguments": {
            "prompt": "80s pop track with bassy drums and synth",
            "output_path": "output/audio/80s_pop.wav",
            "duration": 15.0
          }
        }
      },
      {
        "task_description": "Generate music based on a reference melody",
        "Act": {
          "tool": "MusicGenTool",
          "arguments": {
            "prompt": "orchestral arrangement with dramatic strings",
            "melody_path": "input/melodies/piano_theme.wav",
            "melody_sample_rate": 44100,
            "guidance_scale": 4.0
          }
        }
      },
      {
        "task_description": "Generate multiple music tracks in different styles",
        "Act": {
          "tool": "MusicGenTool",
          "arguments": {
            "prompt": [
              "Jazz trio with saxophone solo",
              "Epic orchestral film score with dramatic drums"
            ],
            "model_path": "models/audiocraft/musicgen-medium",
            "guidance_scale": 4.0
          }
        }
      }
    ]
  },
  "AudioGenTool": {
    "description": "Generate audio (environmental sounds, effects) using Facebook's AudioGen model based on text prompts.",
    "detailed_description": "AudioGenTool utilizes Facebook Audiocraft's AudioGen model to generate environmental sounds, audio effects, and non-musical audio based on text descriptions. This tool is ideal for creating sound effects like animals, vehicles, natural phenomena, human activities, and other non-musical audio. AudioGen specializes in generating realistic audio elements that can be used in various applications like games, videos, and multimedia projects. The maximum generation duration is limited to 30 seconds. The tool includes advanced parameters for controlling the generation quality and sampling behavior.",
    "parameters": {
      "prompt": {
        "type": "string or list",
        "description": "Text description for the audio to generate. Can be a single string or a list of strings for batch generation.",
        "required": true,
        "example": "dog barking in the distance with birds chirping"
      },
      "output_path": {
        "type": "string",
        "description": "Custom path to save the generated audio. If not provided, a default path will be used.",
        "required": "optional"
      },
      "format": {
        "type": "string",
        "description": "Output audio format.",
        "required": "optional",
        "default": "wav"
      },
      "duration": {
        "type": "float",
        "description": "Duration of generated audio in seconds (maximum 30 seconds).",
        "required": "optional",
        "default": 5.0
      },
      "guidance_scale": {
        "type": "float",
        "description": "Classifier-free guidance scale (higher = more adherence to prompt).",
        "required": "optional",
        "default": 3.0
      },
      "temperature": {
        "type": "float",
        "description": "Temperature for sampling (higher = more variation and randomness).",
        "required": "optional",
        "default": 1.0
      },
      "top_k": {
        "type": "integer",
        "description": "Top-k sampling parameter. Limits sampling to the k most likely tokens.",
        "required": "optional",
        "default": 250
      },
      "top_p": {
        "type": "float",
        "description": "Top-p (nucleus) sampling parameter. 0.0 means disabled, uses top_k instead.",
        "required": "optional",
        "default": 0.0
      },
      "seed": {
        "type": "integer",
        "description": "Random seed for reproducible generation.",
        "required": "optional"
      },
      "apply_loudness_normalization": {
        "type": "boolean",
        "description": "Whether to apply loudness normalization to the generated audio.",
        "required": "optional",
        "default": true
      },
      "model_path": {
        "type": "string",
        "description": "Path to the AudioGen model.",
        "required": "optional",
        "default": "/home/chengz/LAMs/pre_train_models/models--facebook--audiogen-medium"
      },
      "device": {
        "type": "string",
        "description": "Computing device to use ('cuda' or 'cpu').",
        "required": "optional",
        "default": "cuda"
      },
      "dtype": {
        "type": "string",
        "description": "Precision to use for model inference ('float16' or 'float32').",
        "required": "optional",
        "default": "float16"
      },
      "extend_stride": {
        "type": "float",
        "description": "Stride length in seconds when generating audio longer than the native window (for audio > 10s).",
        "required": "optional",
        "default": 2.0
      },
      "batch_size": {
        "type": "integer",
        "description": "Number of audio samples to generate in parallel.",
        "required": "optional",
        "default": 1
      }
    },
    "examples": [
      {
        "task_description": "Generate realistic animal sounds",
        "Act": {
          "tool": "AudioGenTool",
          "arguments": {
            "prompt": "wolf howling in a forest at night",
            "output_path": "output/audio/wolf_howl.wav",
            "duration": 8.0
          }
        }
      },
      {
        "task_description": "Generate multiple environmental sound effects",
        "Act": {
          "tool": "AudioGenTool",
          "arguments": {
            "prompt": [
              "heavy rain with thunder",
              "ocean waves crashing on a rocky shore",
              "crackling campfire"
            ],
            "model_path": "models/audiocraft/audiogen-large",
            "duration": 10.0
          }
        }
      },
      {
        "task_description": "Generate mechanical sound effect with specific sampling settings",
        "Act": {
          "tool": "AudioGenTool",
          "arguments": {
            "prompt": "old steam locomotive starting up and moving",
            "duration": 15.0,
            "guidance_scale": 5.0,
            "temperature": 0.8,
            "top_k": 100,
            "seed": 12345
          }
        }
      }
    ]
  },
  "FunASRTool": {
    "description": "Process speech audio for recognition, analysis, and understanding using FunASR's suite of speech processing models.",
    "detailed_description": "FunASRTool is a comprehensive speech processing tool based on the FunASR toolkit. It provides multiple speech understanding capabilities including Automatic Speech Recognition (ASR), Inverse Text Normalization (ITN), Language Identification (LID), Voice Activity Detection (VAD), Punctuation Restoration, and Timestamp Prediction. The tool supports multiple languages including Mandarin Chinese, Cantonese, English, Japanese, and Korean. It also offers both standard batch processing and streaming recognition modes for real-time applications. Each task uses specialized models optimized for that particular capability, with automatic model selection based on the task and language specified.",
    "parameters": {
      "audio_path": {
        "type": "string",
        "description": "Path to the input audio file to process.",
        "required": true,
        "example": "input/audio/speech_sample.wav"
      },
      "task": {
        "type": "string",
        "description": "The speech processing task to perform. Options include: 'asr' (transcription), 'asr_itn' (transcription with text normalization), 'lid' (language identification), 'vad' (voice activity detection), 'punc' (punctuation restoration), 'timestamp' (word-level timestamp prediction), and 'streaming_asr' (streaming/real-time transcription).",
        "required": "optional",
        "default": "asr",
        "enum": [
          "asr",
          "asr_itn",
          "lid",
          "vad",
          "punc",
          "timestamp",
          "streaming_asr"
        ]
      },
      "output_path": {
        "type": "string",
        "description": "Custom path to save the output. If not provided, a default path will be used.",
        "required": "optional"
      },
      "output_format": {
        "type": "string",
        "description": "Format for saving the output results.",
        "required": "optional",
        "default": "json",
        "enum": [
          "json",
          "txt"
        ]
      },
      "language": {
        "type": "string",
        "description": "Language of the audio content. Options include: 'zh' (Mandarin Chinese), 'yue' (Cantonese), 'en' (English), 'ja' (Japanese), 'ko' (Korean), and 'auto' (automatic detection).",
        "required": "optional",
        "default": "auto",
        "enum": [
          "zh",
          "yue",
          "en",
          "ja",
          "ko",
          "auto"
        ]
      },
      "is_streaming": {
        "type": "boolean",
        "description": "Whether to use streaming mode for processing the audio. Only applicable when task is 'streaming_asr'.",
        "required": "optional",
        "default": false
      },
      "chunk_size": {
        "type": "array",
        "description": "Configuration for streaming ASR chunk processing, in format [0, chunk_size, look_ahead]. For example, [0, 10, 5] means 600ms chunk size (10*60ms) with 300ms (5*60ms) lookahead.",
        "required": "optional",
        "example": [
          0,
          10,
          5
        ]
      },
      "encoder_chunk_look_back": {
        "type": "integer",
        "description": "Number of encoder chunks to look back for streaming ASR.",
        "required": "optional",
        "default": 4
      },
      "decoder_chunk_look_back": {
        "type": "integer",
        "description": "Number of encoder chunks to look back for streaming ASR decoder cross-attention.",
        "required": "optional",
        "default": 1
      },
      "text_file": {
        "type": "string",
        "description": "Path to a text file containing transcription. Required only for timestamp prediction task.",
        "required": "optional",
        "example": "input/text/transcript.txt"
      },
      "model_name": {
        "type": "string",
        "description": "Custom model name to override the default automatic selection.",
        "required": "optional",
        "example": "paraformer-zh-streaming"
      },
      "model_revision": {
        "type": "string",
        "description": "Specific model revision to use.",
        "required": "optional"
      },
      "device": {
        "type": "string",
        "description": "Computing device to use for model inference.",
        "required": "optional",
        "default": "cuda"
      }
    },
    "examples": [
      {
        "task_description": "Transcribe Chinese speech to text",
        "Act": {
          "tool": "FunASRTool",
          "arguments": {
            "audio_path": "input/audio/chinese_speech.wav",
            "task": "asr",
            "language": "zh"
          }
        }
      },
      {
        "task_description": "Detect voice activity segments in an audio file",
        "Act": {
          "tool": "FunASRTool",
          "arguments": {
            "audio_path": "input/audio/meeting_recording.wav",
            "task": "vad",
            "output_format": "json"
          }
        }
      },
      {
        "task_description": "Identify the language spoken in an audio clip",
        "Act": {
          "tool": "FunASRTool",
          "arguments": {
            "audio_path": "input/audio/unknown_language.wav",
            "task": "lid"
          }
        }
      },
      {
        "task_description": "Generate word-level timestamps for a known transcription",
        "Act": {
          "tool": "FunASRTool",
          "arguments": {
            "audio_path": "input/audio/speech.wav",
            "task": "timestamp",
            "text_file": "input/text/transcript.txt",
            "language": "zh"
          }
        }
      },
      {
        "task_description": "Process audio in streaming mode to simulate real-time transcription",
        "Act": {
          "tool": "FunASRTool",
          "arguments": {
            "audio_path": "input/audio/long_speech.wav",
            "task": "streaming_asr",
            "is_streaming": true,
            "chunk_size": [
              0,
              10,
              5
            ],
            "encoder_chunk_look_back": 4,
            "decoder_chunk_look_back": 1
          }
        }
      }
    ]
  },
  "EmotionRecognitionTool": {
    "description": "Analyze emotions in speech audio using emotion2vec models to detect emotional states and characteristics.",
    "detailed_description": "EmotionRecognitionTool utilizes the emotion2vec_plus_large model to perform advanced speech emotion recognition. It can analyze an audio recording to determine the emotional content and characteristics of the speaker's voice. The tool supports both utterance-level analysis (analyzing the entire audio file as one unit) and second-by-second analysis for tracking emotional changes over time. It can also extract emotion embeddings that capture the multidimensional representation of emotions in the speech, which can be useful for further analysis or fine-tuning applications. Results typically include emotion categories and their confidence scores.",
    "parameters": {
      "audio_path": {
        "type": "string",
        "description": "Path to the input audio file containing speech to analyze for emotional content.",
        "required": true,
        "example": "input/audio/emotional_speech.wav"
      },
      "output_dir": {
        "type": "string",
        "description": "Directory to save results and embeddings. If not provided, a default timestamped directory will be created.",
        "required": "optional"
      },
      "granularity": {
        "type": "string",
        "description": "Level of analysis - 'utterance' for entire audio as one unit or 'second' for second-by-second emotion tracking.",
        "required": "optional",
        "default": "utterance",
        "enum": [
          "utterance",
          "second"
        ]
      },
      "extract_embedding": {
        "type": "boolean",
        "description": "Whether to extract and save emotion embeddings along with the emotion classifications.",
        "required": "optional",
        "default": false
      },
      "model_name": {
        "type": "string",
        "description": "Name or path of the emotion recognition model to use.",
        "required": "optional",
        "default": "/home/chengz/LAMs/pre_train_models/models--emotion2vec--emotion2vec_plus_large"
      },
      "device": {
        "type": "string",
        "description": "Computing device to use for model inference.",
        "required": "optional",
        "default": "cuda"
      }
    },
    "examples": [
      {
        "task_description": "Analyze the overall emotion in a speech recording",
        "Act": {
          "tool": "EmotionRecognitionTool",
          "arguments": {
            "audio_path": "input/audio/speech_sample.wav",
            "granularity": "utterance"
          }
        }
      },
      {
        "task_description": "Track emotional changes second-by-second in an audio clip",
        "Act": {
          "tool": "EmotionRecognitionTool",
          "arguments": {
            "audio_path": "input/audio/emotional_dialogue.wav",
            "granularity": "second",
            "output_dir": "output/emotion_analysis/dialogue1"
          }
        }
      },
      {
        "task_description": "Extract emotion embeddings along with classification results",
        "Act": {
          "tool": "EmotionRecognitionTool",
          "arguments": {
            "audio_path": "input/audio/speech.wav",
            "extract_embedding": true
          }
        }
      }
    ]
  },
  "YuEMusicGenerationTool": {
    "description": "Generate complete songs from lyrics with vocals using the YuE open-source music generation foundation model.",
    "detailed_description": "YuEMusicGenerationTool leverages YuE (乐), an open-source foundation model for full-song music generation, which transforms lyrics into complete songs with vocals. YuE generates high-quality music with synchronized vocals that match provided lyrics, creating compositions that can last several minutes and include both vocals and instrumental accompaniment.\n\nThe tool offers three primary modes of operation:\n1) CoT (Chain-of-Thought) mode: Generates music based solely on lyrics and genre tags without audio reference\n2) Dual-track ICL mode: Uses separate vocal and instrumental reference tracks to guide music generation\n3) Single-track ICL mode: Uses a single mixed/vocal/instrumental reference track for style guidance\n\nYuE supports multiple languages including English, Mandarin Chinese, Cantonese, Japanese, and Korean, with specialized models for each language. The lyrics must be properly structured with session labels (e.g., [verse], [chorus]) and appropriate spacing.\n\nGenre tagging is crucial for guiding the musical style, and should include elements like genre, instruments, mood, gender, and vocal timbre. For optimal results, each lyric segment should be limited in length.",
    "parameters": {
      "genre": {
        "type": "string",
        "description": "Space-separated tags describing the musical style, instruments, mood, gender, and timbre. Should include all five components for stable results.",
        "required": true,
        "example": "inspiring female uplifting pop airy vocal electronic bright vocal vocal"
      },
      "lyrics": {
        "type": "string",
        "description": "Complete lyrics divided into structured segments with labels (e.g., [verse], [chorus], [bridge]) prepended. Each session should be separated by two newline characters.",
        "required": true,
        "example": "[verse]\nStaring at the sunset, colors paint the sky\nThoughts of you keep swirling, can't deny\n\n[chorus]\nEvery road you take, I'll be one step behind\nEvery dream you chase, I'm reaching for the light"
      },
      "language": {
        "type": "string",
        "description": "Primary language of the lyrics. Different models are optimized for different languages.",
        "required": "optional",
        "default": "english",
        "enum": [
          "english",
          "chinese"
        ]
      },
      "reasoning_method": {
        "type": "string",
        "description": "Generation mode: 'cot' for Chain-of-Thought (no audio reference), or 'icl' for in-context learning with audio references.",
        "required": "optional",
        "default": "cot",
        "enum": [
          "cot",
          "icl"
        ]
      },
      "max_new_tokens": {
        "type": "integer",
        "description": "Maximum tokens to generate. Each segment is approximately 30 seconds with the default value.",
        "required": "optional",
        "default": 3000
      },
      "repetition_penalty": {
        "type": "float",
        "description": "Penalty for repetitive content in generation. Higher values (1.0-2.0) reduce repetition.",
        "required": "optional",
        "default": 1.1
      },
      "run_n_segments": {
        "type": "integer",
        "description": "Number of lyric segments to generate music for. Increase based on your available GPU memory if you want a full song.",
        "required": "optional",
        "default": 2
      },
      "output_file": {
        "type": "string",
        "description": "Custom path to save the generated audio file. If not provided, a default timestamped path will be used.",
        "required": "optional"
      },
      "use_audio_prompt": {
        "type": "boolean",
        "description": "Whether to use an audio file as a reference for the music style (Single-track ICL mode).",
        "required": "optional",
        "default": false
      },
      "audio_prompt_path": {
        "type": "string",
        "description": "Path to reference audio file for Single-track ICL mode. Used when use_audio_prompt=true.",
        "required": "conditional"
      },
      "prompt_start_time": {
        "type": "float",
        "description": "Start time in seconds to extract from the reference audio. Recommended to use the chorus section for better musicality.",
        "required": "optional",
        "default": 0.0
      },
      "prompt_end_time": {
        "type": "float",
        "description": "End time in seconds to extract from the reference audio. Around 30 seconds is recommended for ICL modes.",
        "required": "optional",
        "default": 30.0
      },
      "use_dual_tracks_prompt": {
        "type": "boolean",
        "description": "Whether to use separate vocal and instrumental tracks as references (Dual-track ICL mode).",
        "required": "optional",
        "default": false
      },
      "vocal_track_prompt_path": {
        "type": "string",
        "description": "Path to vocal reference track for dual-track ICL mode. Used when use_dual_tracks_prompt=true.",
        "required": "conditional"
      },
      "instrumental_track_prompt_path": {
        "type": "string",
        "description": "Path to instrumental reference track for dual-track ICL mode. Used when use_dual_tracks_prompt=true.",
        "required": "conditional"
      },
      "keep_intermediate": {
        "type": "boolean",
        "description": "Whether to keep intermediate files generated during the process.",
        "required": "optional",
        "default": false
      },
      "disable_offload_model": {
        "type": "boolean",
        "description": "Whether to disable model offloading to CPU. Enabling this uses more GPU memory but may be faster.",
        "required": "optional",
        "default": false
      },
      "cuda_idx": {
        "type": "integer",
        "description": "CUDA device index to use for generation.",
        "required": "optional",
        "default": 0
      },
      "seed": {
        "type": "integer",
        "description": "Random seed for reproducible generation.",
        "required": "optional",
        "default": 42
      },
      "rescale": {
        "type": "boolean",
        "description": "Whether to rescale output audio to avoid clipping.",
        "required": "optional",
        "default": true
      }
    },
    "examples": [
      {
        "task_description": "Generate a pop song with English lyrics",
        "Act": {
          "tool": "YuEMusicGenerationTool",
          "arguments": {
            "genre": "inspirational female pop electronic piano bright melody",
            "lyrics": "[verse]\nMorning light breaks through my window\nA brand new day begins to show\n\n[chorus]\nI'm ready for whatever comes my way\nGonna face the world and seize the day",
            "language": "english",
            "reasoning_method": "cot",
            "run_n_segments": 2
          }
        }
      },
      {
        "task_description": "Generate a Chinese song with audio reference",
        "Act": {
          "tool": "YuEMusicGenerationTool",
          "arguments": {
            "genre": "energetic male rock ballad electric-guitar drums powerful emotional",
            "lyrics": "[verse]\n站在高山之巅\n俯瞰这世界\n\n[chorus]\n让风吹过我的脸\n感受自由无边",
            "language": "chinese",
            "use_audio_prompt": true,
            "audio_prompt_path": "input/references/rock_sample.mp3",
            "prompt_start_time": 30.0,
            "prompt_end_time": 60.0,
            "output_file": "output/songs/chinese_rock.mp3"
          }
        }
      },
      {
        "task_description": "Generate a song with separate vocal and instrumental references",
        "Act": {
          "tool": "YuEMusicGenerationTool",
          "arguments": {
            "genre": "emotional female ballad piano strings intimate vulnerable",
            "lyrics": "[verse]\nTears falling like rain\nMemories bring back the pain\n\n[chorus]\nBut I'll rise again\nStronger than I've ever been",
            "use_dual_tracks_prompt": true,
            "vocal_track_prompt_path": "input/references/female_vocal.wav",
            "instrumental_track_prompt_path": "input/references/piano_backing.wav",
            "reasoning_method": "icl"
          }
        }
      }
    ]
  },
  "YuEETool": {
    "description": "Generate complete songs from lyrics with vocals using the YuE-E accelerated music generation model.",
    "detailed_description": "YuEETool is a faster, optimized version of YuEMusicGenerationTool that leverages YuE-E, an enhanced version of the YuE (乐) open-source music generation foundation model. It transforms lyrics into complete songs with vocals and offers the same functionality as YuEMusicGenerationTool but with significantly faster inference times, making it the preferred choice for music generation tasks.\n\nThe tool offers three primary modes of operation:\n1) CoT (Chain-of-Thought) mode: Generates music based solely on lyrics and genre tags without audio reference\n2) Dual-track ICL mode: Uses separate vocal and instrumental reference tracks to guide music generation\n3) Single-track ICL mode: Uses a single mixed/vocal/instrumental reference track for style guidance\n\nYuE-E supports multiple languages including English and Mandarin Chinese, with specialized models for each language. The lyrics must be properly structured with session labels (e.g., [verse], [chorus]) and appropriate spacing.\n\nGenre tagging is crucial for guiding the musical style, and should include elements like genre, instruments, mood, gender, and vocal timbre. For optimal results, each lyric segment should be limited in length.\n\nThe tool returns three audio files: the complete mixed version, the instrumental track only (itrack), and the vocal track only (vtrack). These are available in the return value as 'mixed_output_path', 'itrack_output_path', and 'vtrack_output_path' respectively.",
    "parameters": {
      "genre": {
        "type": "string",
        "description": "Space-separated tags describing the musical style, instruments, mood, gender, and timbre. Should include all five components for stable results.",
        "required": true,
        "example": "inspiring female uplifting pop airy vocal electronic bright vocal vocal"
      },
      "lyrics": {
        "type": "string",
        "description": "Complete lyrics divided into structured segments with labels (e.g., [verse], [chorus], [bridge]) prepended. Each session should be separated by two newline characters.",
        "required": true,
        "example": "[verse]\nStaring at the sunset, colors paint the sky\nThoughts of you keep swirling, can't deny\n\n[chorus]\nEvery road you take, I'll be one step behind\nEvery dream you chase, I'm reaching for the light"
      },
      "language": {
        "type": "string",
        "description": "Primary language of the lyrics. Different models are optimized for different languages.",
        "required": "optional",
        "default": "english",
        "enum": [
          "english",
          "chinese"
        ]
      },
      "reasoning_method": {
        "type": "string",
        "description": "Generation mode: 'cot' for Chain-of-Thought (no audio reference), or 'icl' for in-context learning with audio references.",
        "required": "optional",
        "default": "cot",
        "enum": [
          "cot",
          "icl"
        ]
      },
      "max_new_tokens": {
        "type": "integer",
        "description": "Maximum tokens to generate. Each segment is approximately 30 seconds with the default value.",
        "required": "optional",
        "default": 3000
      },
      "repetition_penalty": {
        "type": "float",
        "description": "Penalty for repetitive content in generation. Higher values (1.0-2.0) reduce repetition.",
        "required": "optional",
        "default": 1.1
      },
      "run_n_segments": {
        "type": "integer",
        "description": "Number of lyric segments to generate music for. Increase based on your available GPU memory if you want a full song.",
        "required": "optional",
        "default": 2
      },
      "output_file": {
        "type": "string",
        "description": "Custom path to save the generated audio file. If not provided, a default timestamped path will be used.",
        "required": "optional"
      },
      "stage1_use_exl2": {
        "type": "boolean",
        "description": "Use exllamav2 to load and run stage 1 model for faster inference.",
        "required": "optional",
        "default": true
      },
      "stage2_use_exl2": {
        "type": "boolean",
        "description": "Use exllamav2 to load and run stage 2 model for faster inference.",
        "required": "optional",
        "default": true
      },
      "stage2_batch_size": {
        "type": "integer",
        "description": "Non-exl2 batch size used in Stage 2 inference. Higher values increase memory usage but may accelerate generation.",
        "required": "optional",
        "default": 4
      },
      "stage1_cache_size": {
        "type": "integer",
        "description": "Cache size used in Stage 1 inference. Larger sizes improve speed at the cost of memory.",
        "required": "optional",
        "default": 16384
      },
      "stage2_cache_size": {
        "type": "integer",
        "description": "Exl2 cache size used in Stage 2 inference. Larger sizes improve speed at the cost of memory.",
        "required": "optional",
        "default": 32768
      },
      "stage1_cache_mode": {
        "type": "string",
        "description": "Cache mode for Stage 1 (FP16, Q8, Q6, Q4). Lower precision modes use less memory but may affect quality.",
        "required": "optional",
        "default": "FP16"
      },
      "stage2_cache_mode": {
        "type": "string",
        "description": "Cache mode for Stage 2 (FP16, Q8, Q6, Q4). Lower precision modes use less memory but may affect quality.",
        "required": "optional",
        "default": "FP16"
      },
      "stage1_no_guidance": {
        "type": "boolean",
        "description": "Disable classifier-free guidance for stage 1. May increase generation speed at the cost of quality.",
        "required": "optional",
        "default": false
      },
      "use_audio_prompt": {
        "type": "boolean",
        "description": "Whether to use an audio file as a reference for the music style (Single-track ICL mode).",
        "required": "optional",
        "default": false
      },
      "audio_prompt_path": {
        "type": "string",
        "description": "Path to reference audio file for Single-track ICL mode. Used when use_audio_prompt=true.",
        "required": "conditional"
      },
      "prompt_start_time": {
        "type": "float",
        "description": "Start time in seconds to extract from the reference audio. Recommended to use the chorus section for better musicality.",
        "required": "optional",
        "default": 0.0
      },
      "prompt_end_time": {
        "type": "float",
        "description": "End time in seconds to extract from the reference audio. Around 30 seconds is recommended for ICL modes.",
        "required": "optional",
        "default": 30.0
      },
      "use_dual_tracks_prompt": {
        "type": "boolean",
        "description": "Whether to use separate vocal and instrumental tracks as references (Dual-track ICL mode).",
        "required": "optional",
        "default": false
      },
      "vocal_track_prompt_path": {
        "type": "string",
        "description": "Path to vocal reference track for dual-track ICL mode. Used when use_dual_tracks_prompt=true.",
        "required": "conditional"
      },
      "instrumental_track_prompt_path": {
        "type": "string",
        "description": "Path to instrumental reference track for dual-track ICL mode. Used when use_dual_tracks_prompt=true.",
        "required": "conditional"
      },
      "keep_intermediate": {
        "type": "boolean",
        "description": "Whether to keep intermediate files generated during the process.",
        "required": "optional",
        "default": false
      },
      "disable_offload_model": {
        "type": "boolean",
        "description": "Whether to disable model offloading to CPU. Enabling this uses more GPU memory but may be faster.",
        "required": "optional",
        "default": false
      },
      "cuda_idx": {
        "type": "integer",
        "description": "CUDA device index to use for generation.",
        "required": "optional",
        "default": 0
      },
      "seed": {
        "type": "integer",
        "description": "Random seed for reproducible generation. If not provided, a random seed will be used.",
        "required": "optional"
      },
      "rescale": {
        "type": "boolean",
        "description": "Whether to rescale output audio to avoid clipping.",
        "required": "optional",
        "default": true
      }
    },
    "examples": [
      {
        "task_description": "Generate a pop song with English lyrics using the faster YuE-E model",
        "Act": {
          "tool": "YuEETool",
          "arguments": {
            "genre": "inspirational female pop electronic piano bright melody",
            "lyrics": "[verse]\nMorning light breaks through my window\nA brand new day begins to show\n\n[chorus]\nI'm ready for whatever comes my way\nGonna face the world and seize the day",
            "language": "english",
            "reasoning_method": "cot",
            "run_n_segments": 2
          }
        }
      },
      {
        "task_description": "Generate a Chinese song with audio reference using the accelerated model",
        "Act": {
          "tool": "YuEETool",
          "arguments": {
            "genre": "energetic male rock ballad electric-guitar drums powerful emotional",
            "lyrics": "[verse]\n站在高山之巅\n俯瞰这世界\n\n[chorus]\n让风吹过我的脸\n感受自由无边",
            "language": "chinese",
            "use_audio_prompt": true,
            "audio_prompt_path": "input/references/rock_sample.mp3",
            "prompt_start_time": 30.0,
            "prompt_end_time": 60.0,
            "output_file": "output/songs/chinese_rock.mp3"
          }
        }
      },
      {
        "task_description": "Generate a song with separate vocal and instrumental references using optimized settings",
        "Act": {
          "tool": "YuEETool",
          "arguments": {
            "genre": "emotional female ballad piano strings intimate vulnerable",
            "lyrics": "[verse]\nTears falling like rain\nMemories bring back the pain\n\n[chorus]\nBut I'll rise again\nStronger than I've ever been",
            "use_dual_tracks_prompt": true,
            "vocal_track_prompt_path": "input/references/female_vocal.wav",
            "instrumental_track_prompt_path": "input/references/piano_backing.wav",
            "reasoning_method": "icl",
            "stage1_cache_mode": "Q8",
            "stage2_cache_mode": "Q8"
          }
        }
      }
    ]
  },
  "VoiceCraftTool": {
    "description": "Edit speech audio by substituting, inserting, or deleting words while preserving the original voice.If the original text corresponding to the speech to be edited is not provided, you need to first obtain this original text using a speech recognition tool.",
    "detailed_description": "VoiceCraftTool enables zero-shot speech editing in English, allowing you to naturally modify the content of speech recordings while maintaining the speaker's voice and style. The tool supports three types of edits: substitution (replacing words), insertion (adding new words), and deletion (removing words). It provides fine-grained control over the editing process through various parameters.",
    "parameters": {
      "audio_path": {
        "type": "string",
        "description": "Path to the input audio file to edit (WAV format recommended)",
        "required": true
      },
      "edit_type": {
        "type": "string",
        "description": "Type of edit to perform: 'substitution' (replace words), 'insertion' (add words), or 'deletion' (remove words)",
        "required": true,
        "enum": ["substitution", "insertion", "deletion"]
      },
      "original_transcript": {
        "type": "string",
        "description": "Transcript of the original audio",
        "required": true
      },
      "target_transcript": {
        "type": "string",
        "description": "Desired transcript after editing. Must match the original except for the parts being edited",
        "required": true
      },
      "left_margin": {
        "type": "float",
        "description": "Additional time margin (in seconds) before the edited segment,Margin to the right of the editing segment,Default: 0.08",
        "required": false,
        "default": 0.08
      },
      "right_margin": {
        "type": "float",
        "description": "Additional time margin (in seconds) after the edited segment,Margin to the right of the editing segment,Default: 0.08",
        "required": false,
        "default": 0.08
      },
      "temperature": {
        "type": "float",
        "description": "Controls randomness in generation (higher = more random)",
        "required": false,
        "default": 1.0
      },
      "top_k": {
        "type": "integer",
        "description": "Number of highest probability vocabulary tokens to keep for sampling. -1 means no top-k filtering",
        "required": false,
        "default": -1
      },
      "top_p": {
        "type": "float",
        "description": "Nucleus sampling parameter (higher = more diversity)",
        "required": false,
        "default": 0.8
      },
      "stop_repetition": {
        "type": "integer",
        "description": "Controls repetition. When the number of consecutive repetition of a token is bigger than this, stop it. -1 for speech editing. -1 means do not adjust prob of silence tokens. ",
        "required": false,
        "default": -1
      },
      "kvcache": {
        "type": "boolean",
        "description": "Whether to use key-value caching for faster inference",
        "required": false,
        "default": true
      },
      "silence_tokens": {
        "type": "string",
        "description": "List of token IDs that represent silence, in string format",
        "required": false,
        "default": "[1388,1898,131]"
      },
      "output_path": {
        "type": "string",
        "description": "Custom path to save the edited audio file (WAV format)",
        "required": false
      },
      "device": {
        "type": "integer",
        "description": "CUDA device ID to use for inference,CUDA device ID (just need ID number eg. 0, 1, 2, etc.) to use for inference.",
        "required": false,
        "default": 0
      },
      "seed": {
        "type": "integer",
        "description": "Random seed for reproducibility",
        "required": false,
        "default": 42
      }
    },
    "examples": [
      {
        "task_description": "Replace words in a speech recording",
        "Act": {
          "tool": "VoiceCraftTool",
          "arguments": {
            "audio_path": "input/audio/speech_sample.wav",
            "edit_type": "substitution",
            "original_transcript": "I need to go to the store today.",
            "target_transcript": "I need to go to the mall today."
          }
        }
      },
      {
        "task_description": "Insert new words into a speech recording",
        "Act": {
          "tool": "VoiceCraftTool",
          "arguments": {
            "audio_path": "input/audio/business_presentation.wav",
            "edit_type": "insertion",
            "original_transcript": "The company revenue increased last quarter.",
            "target_transcript": "The company revenue significantly increased last quarter."
          }
        }
      },
      {
        "task_description": "Remove words from a speech recording",
        "Act": {
          "tool": "VoiceCraftTool",
          "arguments": {
            "audio_path": "input/audio/podcast_intro.wav",
            "edit_type": "deletion",
            "original_transcript": "Welcome to our brand new podcast.",
            "target_transcript": "Welcome to our podcast.",
            "temperature": 0.8,
            "top_p": 0.9
          }
        }
      }
    ]
  },
  "SparkTTSTool": {
    "description": "Generate highly natural speech with zero-shot voice cloning and controllable synthesis features using Spark-TTS.",
    "detailed_description": "SparkTTSTool leverages the Spark-TTS model, a state-of-the-art text-to-speech system built on large language model (LLM) architecture. It offers exceptional voice quality with zero-shot voice cloning (no training required for new voices) and voice parameter control options that can create voices with specific characteristics.\n\nThe tool operates in two primary modes:\n1) Voice cloning mode: Copies voice characteristics from a reference audio sample\n2) Controllable TTS mode: Creates voices with specified gender, pitch, and speed settings\n\nVoice control is achieved through three main parameters:\n- Gender: male or female\n- Pitch: very_low, low, moderate, high, or very_high\n- Speed: very_low, low, moderate, high, or very_high\n\nSpark-TTS excels at both Chinese and English synthesis with seamless code-switching capabilities, making it ideal for bilingual applications.",
    "parameters": {
      "text": {
        "type": "string",
        "description": "The text to convert to speech. Supports both Chinese and English, including mixed-language text.",
        "required": true,
        "example": "这是一个演示，demonstrating bilingual text-to-speech capabilities."
      },
      "prompt_text": {
        "type": "string",
        "description": "Transcript of the reference audio for voice cloning. Required when using prompt_speech_path.",
        "required": "conditional",
        "example": "Hello, this is a sample voice."
      },
      "prompt_speech_path": {
        "type": "string",
        "description": "Path to the reference audio file for voice cloning. The model will clone this voice for the synthesized speech.",
        "required": "conditional",
        "example": "input/prompts/reference_voice.wav"
      },
      "gender": {
        "type": "string",
        "description": "Gender of the synthesized voice. Used for controllable TTS mode.",
        "required": "optional",
        "enum": [
          "male",
          "female"
        ],
        "example": "female"
      },
      "pitch": {
        "type": "string",
        "description": "Pitch level of the synthesized voice. Used for controllable TTS mode.",
        "required": "optional",
        "enum": [
          "very_low",
          "low",
          "moderate",
          "high",
          "very_high"
        ],
        "example": "high"
      },
      "speed": {
        "type": "string",
        "description": "Speaking rate of the synthesized voice. Used for controllable TTS mode.",
        "required": "optional",
        "enum": [
          "very_low",
          "low",
          "moderate",
          "high",
          "very_high"
        ],
        "example": "moderate"
      },
      "output_path": {
        "type": "string",
        "description": "Custom path to save the generated audio (WAV format)",
        "required": "optional",
        "default": "/home/chengz/LAMs/mcp_chatbot-audio/output/audio"
      },
      "device": {
        "type": "integer",
        "description": "CUDA device ID to use for inference (0, 1, etc.).",
        "required": "optional",
        "default": 0
      },
      "verbose": {
        "type": "boolean",
        "description": "Whether to print detailed information during processing.",
        "required": "optional",
        "default": false
      }
    },
    "examples": [
      {
        "task_description": "Clone a voice from a reference audio sample",
        "Act": {
          "tool": "SparkTTSTool",
          "arguments": {
            "text": "I'm speaking with a cloned voice that matches the reference audio.",
            "prompt_text": "Hello, this is my voice sample for cloning.",
            "prompt_speech_path": "input/prompts/user_voice_sample.wav"
          }
        }
      },
      {
        "task_description": "Generate bilingual speech with voice cloning",
        "Act": {
          "tool": "SparkTTSTool",
          "arguments": {
            "text": "这是中英文混合的例子，demonstrating seamless code-switching capabilities.",
            "prompt_text": "This is a reference voice for demonstration.",
            "prompt_speech_path": "input/prompts/speaker_sample.wav",
            "output_path": "output/bilingual_speech.wav"
          }
        }
      },
      {
        "task_description": "Create a speech with specific voice characteristics",
        "Act": {
          "tool": "SparkTTSTool",
          "arguments": {
            "text": "This speech has controlled voice characteristics without requiring a reference voice.",
            "gender": "female",
            "pitch": "high",
            "speed": "moderate"
          }
        }
      },
      {
        "task_description": "Generate male voice with low pitch and slow speed",
        "Act": {
          "tool": "SparkTTSTool",
          "arguments": {
            "text": "This is an example of speech with specific voice characteristics.",
            "gender": "male",
            "pitch": "low",
            "speed": "very_low"
          }
        }
      },
      {
        "task_description": "Voice cloning with custom output path",
        "Act": {
          "tool": "SparkTTSTool",
          "arguments": {
            "text": "This sample uses voice cloning with a custom output path.",
            "prompt_text": "Reference voice for cloning.",
            "prompt_speech_path": "input/prompts/reference.wav",
            "output_path": "output/custom_output.wav"
          }
        }
      }
    ]
  },
  "AudioSepTool": {
    "description": "Separate specific sounds from audio files using textual descriptions with the AudioSep model.",
    "detailed_description": "AudioSepTool is an implementation of the 'Separate Anything You Describe' framework (AudioSep), a foundation model for open-domain sound separation based on natural language queries. It can extract specific sounds from mixed audio by using text descriptions of the target sound. The model demonstrates strong performance in separating audio events, musical instruments, speech, and other sound sources. AudioSep processes audio at 32 kHz sampling rate and supports chunk-based processing for memory efficiency with longer audio files.",
    "parameters": {
      "audio_file": {
        "type": "string",
        "description": "Path to the input audio file to process. The file will be resampled to 32 kHz internally.",
        "required": true,
        "example": "input/audio/mixed_sounds.wav"
      },
      "text": {
        "type": "string",
        "description": "Textual description of the sound you want to separate from the audio. Be specific about the sound characteristics.",
        "required": true,
        "example": "dog barking"
      },
      "output_file": {
        "type": "string",
        "description": "Custom path to save the separated audio file (WAV format). If not provided, a default path will be used.",
        "required": "optional",
        "example": "output/audio/separated_dog_barking.wav"
      },
      "use_chunk": {
        "type": "boolean",
        "description": "Whether to use chunked processing for memory efficiency. Recommended for longer audio files.",
        "required": "optional",
        "default": false
      },
      "device": {
        "type": "string",
        "description": "Computing device to use for inference ('cuda' or 'cpu').",
        "required": "optional",
        "default": "cuda"
      }
    },
    "examples": [
      {
        "task_description": "Separate a dog barking from an audio file",
        "Act": {
          "tool": "AudioSepTool",
          "arguments": {
            "audio_file": "input/audio/backyard_sounds.wav",
            "text": "dog barking",
            "output_file": "output/audio/separated_dog_barking.wav"
          }
        }
      },
      {
        "task_description": "Extract piano sounds from a music recording",
        "Act": {
          "tool": "AudioSepTool",
          "arguments": {
            "audio_file": "input/audio/band_playing.wav",
            "text": "piano melody",
            "output_file": "output/audio/separated_piano.wav"
          }
        }
      },
      {
        "task_description": "Separate speech from a noisy environment",
        "Act": {
          "tool": "AudioSepTool",
          "arguments": {
            "audio_file": "input/audio/noisy_interview.wav",
            "text": "human speaking, speech, voice",
            "use_chunk": true,
            "output_file": "output/audio/separated_speech.wav"
          }
        }
      }
    ]
  },
  "AudioSeparatorTool": {
    "description": "Separate audio into multiple stems such as vocals, instrumental, drums, bass, guitar, piano, and other.",
    "detailed_description": "AudioSeparatorTool separates audio tracks into their component stems using pre-trained neural networks. It supports two primary models: UVR-MDX-NET-Inst_HQ_3.onnx for basic vocals/instrumental separation, and htdemucs_6s.yaml for detailed multi-stem separation including vocals, drums, bass, guitar, piano, and other elements. The tool automatically names output files based on the model used, incorporating timestamps for easy identification. For UVR models, it produces 'vocals_[timestamp]_output' and 'instrumental_[timestamp]_output' files, while for htdemucs models, it produces timestamped stem files for all six stems. The tool tracks processing time and reports it in the output.",
    "parameters": {
      "audio_path": {
        "type": "string",
        "description": "Path to the input audio file to process (WAV, MP3, FLAC, etc.)",
        "required": true
      },
      "model_name": {
        "type": "string",
        "description": "Model to use for separation: UVR-MDX-NET-Inst_HQ_3.onnx (vocals/instrumental) or htdemucs_6s.yaml (multi-stem)",
        "required": "optional",
        "default": "UVR-MDX-NET-Inst_HQ_3.onnx"
      },
      "output_dir": {
        "type": "string",
        "description": "Directory to save separated stems. If not provided, a default directory will be used",
        "required": "optional"
      },
      "output_format": {
        "type": "string",
        "description": "Format of output audio files (WAV, MP3, FLAC, etc.)",
        "required": "optional",
        "default": "WAV"
      }
    },
    "examples": [
      {
        "task_description": "Basic separation into vocals and instrumental with timestamped filenames",
        "Act": {
          "tool": "AudioSeparatorTool",
          "arguments": {
            "audio_path": "input/audio/my_song.mp3"
          }
        }
      },
      {
        "task_description": "Multi-stem separation with 6 stems and timestamped filenames",
        "Act": {
          "tool": "AudioSeparatorTool",
          "arguments": {
            "audio_path": "input/audio/band_recording.wav",
            "model_name": "htdemucs_6s.yaml"
          }
        }
      },
      {
        "task_description": "Separate with custom output format",
        "Act": {
          "tool": "AudioSeparatorTool",
          "arguments": {
            "audio_path": "input/audio/song.flac",
            "output_format": "MP3"
          }
        }
      }
    ]
  },
  "Qwen2AudioTool": {
    "description": "Process audio with Qwen2-Audio model for comprehensive audio understanding, analysis, and  tasks. can analyzer audio/music style emotion and do all understanding/analysis tasks.but  you need to combine it with the specific task instructions. ",
    "detailed_description": "A powerful audio processing tool based on the Qwen2-Audio large speech language model. It supports a wide range of audio understanding tasks covering speech, music, and general audio analysis according to the AIR-Bench benchmark. The model excels at detailed audio analysis through natural language instructions, enabling applications from speech transcription and translation to emotion recognition, speaker analysis, music classification, and more.",
    "parameters": {
      "task": {
        "type": "string",
        "description": "Task to perform, including basic tasks (transcribe, chat, evaluate) and specialized AIR-Bench tasks for speech and audio analysis",
        "required": true,
        "choices": [
          "transcribe", "chat", "evaluate", 
          "speech_grounding", "language_identification", "speaker_gender", 
          "emotion_recognition", "speaker_age", "speech_entity", 
          "intent_classification", "speaker_verification", "synthesized_voice_detection",
          "audio_grounding", "vocal_classification", "acoustic_scene", 
          "sound_qa", "music_instruments", "music_genre", 
          "music_note_pitch", "music_note_velocity", "music_qa", "music_emotion"
        ]
      },
      "audio_path": {
        "type": "string",
        "description": "Path to input audio file",
        "required": true
      },
      "text": {
        "type": "string",
        "description": "Additional text input or context for the task",
        "required": false
      },
      "reference_audio_path": {
        "type": "string",
        "description": "Path to reference audio for comparison tasks (e.g., speaker verification)",
        "required": false
      },
      "prompt": {
        "type": "string",
        "description": "Text prompt to guide the model's response or specify task details",
        "required": false
      },
      "evaluation_criteria": {
        "type": "string",
        "description": "Custom criteria for audio evaluation",
        "required": false
      },
      "evaluation_prompt_name": {
        "type": "string",
        "description": "Name of predefined evaluation prompt to use",
        "required": false
      },
      "target_language": {
        "type": "string",
        "description": "Target language for translation tasks",
        "required": false
      },
      "temperature": {
        "type": "number",
        "description": "Controls randomness in generation (higher = more random)",
        "required": false,
        "default": 0.7
      },
      "top_p": {
        "type": "number",
        "description": "Nucleus sampling parameter",
        "required": false,
        "default": 0.9
      },
      "max_new_tokens": {
        "type": "integer",
        "description": "Maximum number of tokens to generate in the response",
        "required": false,
        "default": 2048
      },
      "do_sample": {
        "type": "boolean",
        "description": "Whether to use sampling in generation",
        "required": false,
        "default": true
      },
      "output_path": {
        "type": "string",
        "description": "Custom path to save output",
        "required": false
      },
      "output_format": {
        "type": "string",
        "description": "Format for saving output (json or txt)",
        "required": false,
        "default": "json",
        "choices": ["json", "txt"]
      },
      "device": {
        "type": "string",
        "description": "Computing device for inference (cuda or cpu)",
        "required": false,
        "default": "cuda"
      }
    },
    "examples": [
      {
        "task_description": "Transcribe speech to text",
        "Act": {
          "tool": "Qwen2AudioTool",
          "arguments": {
            "task": "transcribe",
            "audio_path": "input/speech.wav"
          }
        }
      },
      {
        "task_description": "Analyze the emotions in speech",
        "Act": {
          "tool": "Qwen2AudioTool",
          "arguments": {
            "task": "emotion_recognition",
            "audio_path": "input/speech.wav"
          }
        }
      },
      {
        "task_description": "Identify the speaker's gender",
        "Act": {
          "tool": "Qwen2AudioTool",
          "arguments": {
            "task": "speaker_gender",
            "audio_path": "input/speech.wav"
          }
        }
      },
      {
        "task_description": "Analyze the acoustic scene",
        "Act": {
          "tool": "Qwen2AudioTool",
          "arguments": {
            "task": "acoustic_scene",
            "audio_path": "input/ambience.wav"
          }
        }
      },
      {
        "task_description": "Classify musical instruments",
        "Act": {
          "tool": "Qwen2AudioTool",
          "arguments": {
            "task": "music_instruments",
            "audio_path": "input/music.wav"
          }
        }
      },
      {
        "task_description": "Verify if two speech samples are from the same speaker",
        "Act": {
          "tool": "Qwen2AudioTool",
          "arguments": {
            "task": "speaker_verification",
            "audio_path": "input/speech1.wav",
            "reference_audio_path": "input/speech2.wav"
          }
        }
      },
      {
        "task_description": "Open-ended chat about audio content",
        "Act": {
          "tool": "Qwen2AudioTool",
          "arguments": {
            "task": "chat",
            "audio_path": "input/audio/group_discussion.wav",
            "prompt": "Identify all speakers in this recording, analyze the conversation dynamics including who speaks most frequently, who tends to interrupt others, and the overall tone of the discussion. Summarize the key points made by each participant."
          }
        }
      }
    ]
  },
  "TIGERSpeechSeparationTool": {
    "description": "Separate speech from audio mixtures.",
    "detailed_description": "TIGERSpeechSeparationTool separates mixed speech signals into individual speaker tracks. The tool processes input audio, resampling it to the required 16kHz sample rate if necessary, and outputs individual WAV files for each detected speaker.",
    "parameters": {
      "audio_path": {
        "type": "string",
        "description": "Path to the input audio file containing mixed speech to separate.",
        "required": true,
        "example": "input/audio/mixed_speech.wav"
      },
      "output_dir": {
        "type": "string",
        "description": "Directory to save the separated speech files. Each speaker will be saved as a separate WAV file.",
        "required": "optional",
        "example": "output/separated_speakers"
      }
    },
    "examples": [
      {
        "task_description": "Separate speech from a recording with multiple speakers",
        "Act": {
          "tool": "TIGERSpeechSeparationTool",
          "arguments": {
            "audio_path": "input/audio/conversation.wav"
          }
        }
      }
    ]
  },
  "Hallo2Tool": {
    "description": "Generate audio-driven portrait animations from a single image and audio input.",
    "detailed_description": "Hallo2Tool transforms a single portrait image into a realistic talking video synchronized with any audio input. It uses state-of-the-art technology to create natural head movements, facial expressions, and accurate lip synchronization. The system can process long-duration audio (up to one hour) and maintain high quality throughout. The generated animations preserve the identity and style of the source portrait while creating realistic movements. It's ideal for creating virtual presenters, dubbing videos into different languages, or bringing still portraits to life for education, storytelling, or entertainment purposes.",
    "parameters": {
      "source_image": {
        "type": "string",
        "description": "Path to the source portrait image. The portrait should face forward and have the face as the main focus.",
        "required": true
      },
      "driving_audio": {
        "type": "string",
        "description": "Path to the audio file that will drive the animation. Supported formats: WAV.",
        "required": true
      },
      "pose_weight": {
        "type": "number",
        "description": "Weight for head pose motion. Higher values increase head movement (0-1).",
        "required": "optional",
        "default": 0.3
      },
      "face_weight": {
        "type": "number",
        "description": "Weight for facial expressions. Higher values enhance facial expressions (0-1).",
        "required": "optional",
        "default": 0.7
      },
      "lip_weight": {
        "type": "number",
        "description": "Weight for lip sync accuracy. Higher values improve lip synchronization (0-1).",
        "required": "optional",
        "default": 1.0
      },
      "face_expand_ratio": {
        "type": "number",
        "description": "Expand ratio for the detected face region. Higher values include more context around the face.",
        "required": "optional",
        "default": 1.5
      },
      "output_path": {
        "type": "string",
        "description": "Custom path to save the generated video file (MP4 format).",
        "required": "optional"
      },
      "config_path": {
        "type": "string",
        "description": "Path to a custom configuration file. If not provided, default config will be used.",
        "required": "false",
        "default": "/home/chengz/LAMs/mcp_chatbot-audio/models/hallo2/configs/inference/long.yaml"

      },
      "device": {
        "type": "string",
        "description": "Computing device to use for inference ('cuda' or 'cpu').",
        "required": "optional",
        "enum": [
          "cuda",
          "cpu"
        ]
      }
    },
    "examples": [
      {
        "task_description": "Generate a basic portrait animation with default settings",
        "Act": {
          "tool": "Hallo2Tool",
          "arguments": {
            "source_image": "input/portraits/person.jpg",
            "driving_audio": "input/audio/speech.wav"
          }
        }
      },
      {
        "task_description": "Create an animation with custom motion settings",
        "Act": {
          "tool": "Hallo2Tool",
          "arguments": {
            "source_image": "input/portraits/presenter.png",
            "driving_audio": "input/audio/presentation.wav",
            "pose_weight": 0.4,
            "face_weight": 0.8,
            "lip_weight": 1.0,
            "output_path": "output/videos/presentation.mp4"
          }
        }
      },
      {
        "task_description": "Generate a portrait animation with minimal head movement but strong lip sync",
        "Act": {
          "tool": "Hallo2Tool",
          "arguments": {
            "source_image": "input/portraits/speaker.jpg",
            "driving_audio": "input/audio/lecture.wav",
            "pose_weight": 0.1,
            "face_weight": 0.5,
            "lip_weight": 1.0,
            "face_expand_ratio": 1.8
          }
        }
      }
    ]
  },
  "Hallo2VideoEnhancementTool": {
    "description": "Enhance portrait videos with high-resolution processing and face restoration.",
    "detailed_description": "Hallo2VideoEnhancementTool improves the quality of videos using advanced AI-based upscaling and face restoration techniques. It's especially designed to enhance portrait videos by applying specialized processing to facial regions while also upscaling the entire frame. The tool can increase resolution (up to 4K), enhance facial details, and improve overall video quality. It uses a modified version of CodeFormer to provide high-quality results with adjustable fidelity control to balance between quality enhancement and preserving the original appearance.",
    "parameters": {
      "input_video": {
        "type": "string",
        "description": "Path to the input video file to enhance.",
        "required": true,
        "example": "input/videos/original_video.mp4"
      },
      "fidelity_weight": {
        "type": "number",
        "description": "Balance between quality and fidelity (0-1). Lower values prioritize quality, higher values preserve fidelity to the original.",
        "required": "optional",
        "default": 0.5,
        "example": 0.7
      },
      "upscale": {
        "type": "integer",
        "description": "Upscaling factor for the image (2, 3, or 4).",
        "required": "optional",
        "default": 2,
        "enum": [
          2,
          3,
          4
        ]
      },
      "output_path": {
        "type": "string",
        "description": "Custom path to save the enhanced video. If not provided, a default path will be used.",
        "required": "optional",
        "example": "output/videos/enhanced_video"
      },
      "bg_upsampler": {
        "type": "string",
        "description": "Background upsampler to use. Set to 'None' to disable background upsampling.",
        "required": "optional",
        "default": "realesrgan",
        "enum": [
          "realesrgan",
          "None"
        ]
      },
      "face_upsample": {
        "type": "boolean",
        "description": "Whether to apply additional upsampling to the face regions.",
        "required": "optional",
        "default": true
      },
      "detection_model": {
        "type": "string",
        "description": "Face detection model to use. Larger models are more accurate but slower.",
        "required": "optional",
        "default": "retinaface_resnet50",
        "enum": [
          "retinaface_resnet50",
          "retinaface_mobile0.25",
          "YOLOv5l",
          "YOLOv5n"
        ]
      },
      "bg_tile": {
        "type": "integer",
        "description": "Tile size for background upsampling. Smaller values use less memory but may reduce quality.",
        "required": "optional",
        "default": 400,
        "example": 200
      },
      "only_center_face": {
        "type": "boolean",
        "description": "Whether to only enhance the center face in the video.",
        "required": "optional",
        "default": false
      }
    },
    "examples": [
      {
        "task_description": "Enhance a video with default settings",
        "Act": {
          "tool": "Hallo2VideoEnhancementTool",
          "arguments": {
            "input_video": "output/videos/animation.mp4"
          }
        }
      },
      {
        "task_description": "Create a high-resolution (4x) version with balanced fidelity",
        "Act": {
          "tool": "Hallo2VideoEnhancementTool",
          "arguments": {
            "input_video": "output/videos/portrait_animation.mp4",
            "upscale": 4,
            "fidelity_weight": 0.7,
            "output_path": "output/videos/enhanced_4k"
          }
        }
      },
      {
        "task_description": "Enhance only facial details with lower memory usage",
        "Act": {
          "tool": "Hallo2VideoEnhancementTool",
          "arguments": {
            "input_video": "output/videos/talking_head.mp4",
            "bg_upsampler": "None",
            "face_upsample": true,
            "bg_tile": 200,
            "detection_model": "retinaface_mobile0.25"
          }
        }
      }
    ]
  },
  "read_file": {
    "description": "读取指定类型的所有文件内容，支持md, txt, json文件。",
    "detailed_description": "该工具可以读取指定目录下所有指定类型的文件内容。对于json文件会返回解析后的字典对象，对于md和txt文件会返回合并后的文本内容。每个文件的内容会以文件名作为分隔符。",
    "parameters": {
      "directory_path": {
        "type": "string",
        "description": "要读取的文件所在目录路径",
        "required": true
      },
      "file_type": {
        "type": "string",
        "description": "要读取的文件类型，支持md、txt、json",
        "required": "optional",
        "default": "md",
        "enum": [
          "md",
          "txt",
          "json"
        ]
      }
    },
    "examples": [
      {
        "task_description": "读取目录下所有markdown文件",
        "Act": {
          "tool": "read_file",
          "arguments": {
            "directory_path": "docs",
            "file_type": "md"
          }
        }
      },
      {
        "task_description": "读取目录下所有json文件",
        "Act": {
          "tool": "read_file",
          "arguments": {
            "directory_path": "config",
            "file_type": "json"
          }
        }
      }
    ]
  },
  "write_file": {
    "description": "写入（或新建）指定类型文件，支持md、txt、json。",
    "detailed_description": "该工具可以创建新文件或覆盖已存在的文件。支持写入markdown、txt和json格式的文件。对于json文件，会自动将内容解析为json对象后写入。如果文件已存在且未设置覆盖，则会返回错误信息。",
    "parameters": {
      "directory_path": {
        "type": "string",
        "description": "要写入文件的目录路径",
        "required": true
      },
      "filename": {
        "type": "string",
        "description": "文件名（可以带后缀，如果不带后缀会自动添加.md后缀）",
        "required": true
      },
      "content": {
        "type": "string",
        "description": "要写入的文件内容（对于json文件必须是可解析的json格式字符串）",
        "required": true
      },
      "overwrite": {
        "type": "boolean",
        "description": "是否覆盖已存在的文件",
        "required": "optional",
        "default": false
      }
    },
    "examples": [
      {
        "task_description": "创建新的markdown文件",
        "Act": {
          "tool": "write_file",
          "arguments": {
            "directory_path": "docs",
            "filename": "readme",
            "content": "# 项目说明\n\n这是一个示例文档。"
          }
        }
      },
      {
        "task_description": "创建json配置文件并覆盖已存在的文件",
        "Act": {
          "tool": "write_file",
          "arguments": {
            "directory_path": "config",
            "filename": "settings.json",
            "content": "{\"name\": \"updated\", \"value\": 456}",
            "overwrite": true
          }
        }
      }
    ]
  },
  "modify_file": {
    "description": "修改（覆盖）单个已存在的.md/.txt/.json文件。",
    "detailed_description": "该工具用于修改已存在的文件内容。支持修改markdown、txt和json格式的文件。对于json文件，会自动将新内容解析为json对象后写入。文件必须已经存在，否则会返回错误信息。",
    "parameters": {
      "file_path": {
        "type": "string",
        "description": "要修改的文件的完整路径",
        "required": true
      },
      "new_content": {
        "type": "string",
        "description": "新的文件内容（对于json文件必须是可解析的json格式字符串）",
        "required": true
      }
    },
    "examples": [
      {
        "task_description": "修改markdown文件内容",
        "Act": {
          "tool": "modify_file",
          "arguments": {
            "file_path": "docs/readme.md",
            "new_content": "# 更新后的文档\n\n这是更新后的内容。"
          }
        }
      },
      {
        "task_description": "修改json配置文件",
        "Act": {
          "tool": "modify_file",
          "arguments": {
            "file_path": "config/settings.json",
            "new_content": "{\"name\": \"updated\", \"value\": 456}"
          }
        }
      }
    ]
  },
  "CosyVoice2Tool": {
    "description": "Advanced text-to-speech synthesis using the CosyVoice2 model with extensive capabilities for voice cloning, cross-lingual synthesis, and style control.",
    "detailed_description": "CosyVoice2Tool is a powerful speech synthesis tool based on Alibaba's CosyVoice2 model. It provides advanced text-to-speech capabilities with exceptional natural voice quality and versatile control options. The tool supports multiple core functionalities:\n\n1) Zero-shot In-context Generation: Clone any voice from a reference audio sample without prior training\n2) Cross-lingual In-context Generation: Synthesize text in a different language while preserving the voice characteristics from a reference audio\n3) Mixed-lingual In-context Generation: Support for multiple languages within the same sentence with seamless transitions\n4) Instructed Voice Generation: Control voice characteristics through natural language instructions (requires prompt_audio_path)\n5) Dialect Control: Support for various Chinese dialects including Cantonese, Shanghai dialect, Shandong dialect, etc.\n6) Fine-grained Control: Advanced control over pronunciation and stylistic elements\n7) Speaking Style Control: Modulate emotional expression and speaking rate\n8) Voice Conversion: Transform source audio to match the voice characteristics of a target speaker\n\nThe system supports over 90 languages including English, Mandarin Chinese, Japanese, Korean, Cantonese, German, Spanish, Russian, French, Portuguese, Italian, Arabic, Hindi, Vietnamese, Thai, and many more. Language tags such as \"<|zh|>\", \"<|en|>\", \"<|jp|>\", \"<|yue|>\", \"<|ko|>\" can be used to indicate the language for cross-lingual synthesis.",
    "parameters": {
      "text": {
        "type": "string",
        "description": "Text content to synthesize. Required for all modes except Voice Conversion. For multilingual text, language tags like \"<|zh|>\" (Chinese), \"<|en|>\" (English), \"<|jp|>\" (Japanese), \"<|yue|>\" (Cantonese), \"<|ko|>\" (Korean), etc. can be included to specify language sections. The model supports over 90 languages in total.",
        "required": "optional"
      },
      "source_audio_path": {
        "type": "string",
        "description": "Path to the source audio file for Voice Conversion mode. This audio will be transformed to match the voice characteristics specified by prompt_audio_path while preserving the content.",
        "required": "optional"
      },
      "prompt_audio_path": {
        "type": "string",
        "description": "Path to a reference audio file providing voice characteristics for cloning, cross-lingual synthesis, or as the target voice for conversion. This parameter is required for instructed voice generation mode (when use_instruct_mode=true).",
        "required": "optional"
      },
      "prompt_text": {
        "type": "string",
        "description": "Transcript of the prompt_audio_path. Recommended for Zero-shot mode to improve the accuracy of voice cloning. The text should match the speech content in the prompt audio.",
        "required": "optional"
      },
      "speaker_id": {
        "type": "string",
        "description": "ID of a pre-defined speaker (if supported by the model). Used for consistent voice output across multiple synthesis sessions.",
        "required": "optional"
      },
      "zero_shot_speaker_id": {
        "type": "string",
        "description": "ID for a previously cached zero-shot speaker embedding. This allows reusing voice characteristics extracted from a previous session without providing the audio again.",
        "required": "optional"
      },
      "cross_lingual_synthesis": {
        "type": "boolean",
        "description": "When set to true, performs cross-lingual synthesis using the voice from prompt_audio_path while generating text in a different language. Requires both 'text' and 'prompt_audio_path' parameters.",
        "required": "optional",
        "default": false
      },
      "use_instruct_mode": {
        "type": "boolean",
        "description": "When set to true, enables Instructed Voice Generation mode for emotional, styled speech using the inference_instruct2 method. This mode requires prompt_audio_path to be provided as it needs a reference voice to apply the instructions to.",
        "required": "optional",
        "default": false
      },
      "instruct_text": {
        "type": "string",
        "description": "Instructions for voice style when use_instruct_mode is true. Examples:\n- Emotion instructions: \"用开心的语气说\" (speak happily), \"用伤心的语气说\" (speak sadly), \"用恐惧的情感表达\" (express fear)\n- Dialect instructions: \"用粤语说这句话\" (speak in Cantonese), \"用上海话说\" (speak in Shanghai dialect), \"使用山东话说\" (use Shandong dialect)\n- Character instructions: \"一个忧郁的诗人，言语中总是透露出一丝哀愁和浪漫\" (a melancholic poet, always revealing a hint of sorrow and romance in speech)\n- Speaking style: \"Speaking very fast\", \"Speaking with patience\"",
        "required": "optional"
      },
      "speed": {
        "type": "number",
        "description": "Controls the speech rate of the generated audio. Default is 1.0 (normal speed), values greater than 1.0 produce faster speech, and values less than 1.0 produce slower speech. Range is typically 0.5-2.0.",
        "required": "optional",
        "default": 1.0
      },
      "stream_output": {
        "type": "boolean",
        "description": "When set to true, processes audio in streaming mode, generating and yielding segments incrementally rather than waiting for the entire generation to complete. This can be useful for low-latency applications.",
        "required": "optional",
        "default": false
      },
      "use_text_frontend": {
        "type": "boolean",
        "description": "Whether to use text normalization preprocessing. When true, converts numerals, dates, and other special text forms into spoken form. Set to false for direct processing of raw text input when special handling is not desired.",
        "required": "optional",
        "default": true
      },
      "language_tag": {
        "type": "string",
        "description": "Language tag to prepend to the text if not already present. Examples: \"<|zh|>\" (Chinese), \"<|en|>\" (English), \"<|jp|>\" (Japanese), \"<|yue|>\" (Cantonese), \"<|ko|>\" (Korean). The model supports over 90 languages in total including German, Spanish, Russian, French, Portuguese, Italian, Arabic, Hindi, Vietnamese, Thai, and many more.",
        "required": "optional"
      },
      "output_path": {
        "type": "string",
        "description": "Custom path to save the generated audio file (WAV format). If not provided, a default path with timestamp will be generated automatically in the output/audio directory.",
        "required": "optional"
      },
      "device_hint": {
        "type": "string",
        "description": "Computing device to use for inference. Options are \"cuda\" for GPU acceleration or \"cpu\" for CPU processing. CUDA is recommended for faster performance when available.",
        "required": "optional",
        "default": "cuda"
      },
      "model_fp16": {
        "type": "boolean",
        "description": "When set to true, uses FP16 (half-precision) for model computation, which reduces memory usage and potentially increases speed at a slight cost to precision. Requires CUDA-capable GPU.",
        "required": "optional",
        "default": false
      },
      "model_jit": {
        "type": "boolean",
        "description": "When set to true, uses JIT (Just-In-Time) compilation to optimize model execution, which can improve inference speed. Requires CUDA-capable GPU.",
        "required": "optional",
        "default": false
      },
      "model_trt": {
        "type": "boolean",
        "description": "When set to true, uses TensorRT acceleration for the model, which can significantly improve inference speed on compatible hardware. Requires CUDA-capable GPU with TensorRT support.",
        "required": "optional",
        "default": false
      },
      "use_flow_cache": {
        "type": "boolean",
        "description": "When set to true, uses flow cache for faster inference, which can reduce computation time by caching intermediate results. This option is particularly useful for generating multiple audio segments with the same voice characteristics.",
        "required": "optional",
        "default": false
      }
    },
    "examples": [
      {
        "task_description": "Zero-shot voice cloning",
        "Act": {
          "tool": "CosyVoice2Tool",
          "arguments": {
            "text": "Hello world, this is a synthesized voice.",
            "prompt_audio_path": "reference_voice.wav",
            "prompt_text": "This is my reference voice sample."
          }
        }
      },
      {
        "task_description": "Cross-lingual synthesis",
        "Act": {
          "tool": "CosyVoice2Tool",
          "arguments": {
            "text": "<|en|>This is spoken in English with a Chinese voice.",
            "prompt_audio_path": "chinese_voice.wav",
            "cross_lingual_synthesis": true
          }
        }
      },
      {
        "task_description": "Instructed voice generation (emotional)",
        "Act": {
          "tool": "CosyVoice2Tool",
          "arguments": {
            "text": "Life is full of wonderful surprises.",
            "prompt_audio_path": "reference_voice.wav",
            "use_instruct_mode": true,
            "instruct_text": "用开心的语气说"
          }
        }
      },
      {
        "task_description": "Dialect control",
        "Act": {
          "tool": "CosyVoice2Tool",
          "arguments": {
            "text": "我们今天去哪里吃饭？",
            "prompt_audio_path": "reference_voice.wav",
            "use_instruct_mode": true,
            "instruct_text": "用粤语说这句话"
          }
        }
      },
      {
        "task_description": "Voice conversion",
        "Act": {
          "tool": "CosyVoice2Tool",
          "arguments": {
            "source_audio_path": "source_voice.wav",
            "prompt_audio_path": "target_voice.wav"
          }
        }
      }
    ]
  },
  "ClearVoiceTool": {
    "description": "Unified speech enhancement, separation, super-resolution, and target speaker extraction using the ClearerVoice Studio models. note:Achieving SOTA performance on these tasks.It is recommended to first obtain the sampling rate of the input audio, as this can help in selecting the appropriate model.",
    "detailed_description": "ClearVoiceTool provides a unified interface for advanced audio processing tasks, including speech enhancement, speech separation, speech super-resolution, and audio-visual target speaker extraction.\n\nSupported tasks and model selection guidelines:\n\n1. Speech Enhancement (Remove noise and improve speech quality):\n   - Use 'MossFormer2_SE_48K' for high-fidelity 48kHz audio enhancement. Recommended for music, studio recordings, or any scenario where preserving audio quality is critical.\n   - Use 'MossFormerGAN_SE_16K' for general 16kHz speech enhancement, especially in noisy environments or for telephony/meeting recordings.\n   - Use 'FRCRN_SE_16K' for lightweight, fast 16kHz enhancement when computational resources are limited or for real-time applications.\n\n2. Speech Separation (Separate multiple speakers from a single audio track):\n   - Use 'MossFormer2_SS_16K' for separating two or more speakers in 16kHz mixed speech recordings, such as meeting or interview audio.\n\n3. Speech Super-Resolution (Increase audio resolution and restore high-frequency details):\n   - Use 'MossFormer2_SR_48K' to convert low-resolution speech (e.g., 16kHz) to high-resolution 48kHz, restoring clarity and detail. Ideal for upscaling legacy or telephony audio.\n\n4. Audio-Visual Target Speaker Extraction (Extract the voice of a specific speaker from audio or video):\n   - Use 'AV_MossFormer2_TSE_16K' to extract the target speaker's voice from a video or audio file containing multiple speakers, especially when visual cues are available.\n\nChoose the model according to your input audio type, sample rate, and the specific enhancement or separation goal. For best results, match the model's expected sample rate (16kHz or 48kHz) to your input file. The tool supports both single-file and batch directory processing. Processed results are saved to the specified output directory.",
    "parameters": {
      "task": {
        "type": "string",
        "description": "The audio processing task to perform. One of: 'speech_enhancement', 'speech_separation', 'speech_super_resolution', 'target_speaker_extraction'",
        "required": true
      },
      "model_name": {
        "type": "string",
        "description": "The model to use for the selected task.\n- For speech_enhancement: MossFormer2_SE_48K, MossFormerGAN_SE_16K, FRCRN_SE_16K\n- For speech_separation: MossFormer2_SS_16K\n- For speech_super_resolution: MossFormer2_SR_48K\n- For target_speaker_extraction: AV_MossFormer2_TSE_16K",
        "required": true
      },
      "input_path": {
        "type": "string",
        "description": "Path to the input audio or video file. Required unless batch_process is true.",
        "required": false
      },
      "output_path": {
        "type": "string",
        "description": "Directory to save the processed output. If not provided, a default directory will be used.",
        "required": false
      },
      "online_write": {
        "type": "boolean",
        "description": "Whether to automatically save the processed audio. Default: true.",
        "required": false,
        "default": true
      },
      "batch_process": {
        "type": "boolean",
        "description": "If true, process all supported files in a directory. Requires input_directory.",
        "required": false,
        "default": false
      },
      "input_directory": {
        "type": "string",
        "description": "Directory containing files to process in batch mode. Required if batch_process is true.",
        "required": false
      },
      "device": {
        "type": "string",
        "description": "Computing device to use ('cuda' or 'cpu'). Default: 'cuda'",
        "required": false,
        "default": "cuda"
      }
    },
    "examples": [
      {
        "task_description": "Speech enhancement with MossFormer2_SE_48K",
        "Act": {
          "tool": "ClearVoiceTool",
          "arguments": {
            "task": "speech_enhancement",
            "model_name": "MossFormer2_SE_48K",
            "input_path": "noisy.wav",
            "output_path": "enhanced_output"
          }
        }
      },
      {
        "task_description": "Speech separation with MossFormer2_SS_16K",
        "Act": {
          "tool": "ClearVoiceTool",
          "arguments": {
            "task": "speech_separation",
            "model_name": "MossFormer2_SS_16K",
            "input_path": "mixed.wav",
            "output_path": "separated_output"
          }
        }
      },
      {
        "task_description": "Batch process a directory for speech enhancement",
        "Act": {
          "tool": "ClearVoiceTool",
          "arguments": {
            "task": "speech_enhancement",
            "model_name": "MossFormerGAN_SE_16K",
            "batch_process": true,
            "input_directory": "noisy_dir",
            "output_path": "enhanced_dir"
          }
        }
      },
      {
        "task_description": "Target speaker extraction from video",
        "Act": {
          "tool": "ClearVoiceTool",
          "arguments": {
            "task": "target_speaker_extraction",
            "model_name": "AV_MossFormer2_TSE_16K",
            "input_path": "video.mp4",
            "output_path": "extracted_speaker"
          }
        }
      }
    ]
  },
  "WhisperASRTool": {
    "description": "High-quality automatic speech recognition and translation using OpenAI Whisper large-v3 model with multi-language support.",
    "detailed_description": "WhisperASRTool is a powerful speech recognition and translation tool based on OpenAI's Whisper large-v3 model. It provides state-of-the-art automatic speech recognition (ASR) capabilities with support for 99 languages, along with speech translation to English. The tool uses the sequential long-form algorithm to handle audio files longer than 30 seconds efficiently. Key features include multi-language auto-detection, high-quality transcription with timestamp generation (sentence or word level), batch processing for multiple files, and comprehensive parameter control for optimal results. The tool supports both transcription (converting speech to text in the same language) and translation (converting speech to English text) tasks. It leverages local model files for fast processing and can work with various audio formats including WAV, MP3, M4A, FLAC, and OGG.",
    "parameters": {
      "audio_path": {
        "type": "string",
        "description": "Path to the input audio file to process. Supports various formats: WAV, MP3, M4A, FLAC, OGG.",
        "required": true,
        "example": "input/audio/speech_sample.wav"
      },
      "task": {
        "type": "string", 
        "description": "Task to perform: 'transcribe' converts speech to text in the same language, 'translate' converts speech to English text.",
        "required": "optional",
        "default": "transcribe",
        "enum": ["transcribe", "translate"]
      },
      "language": {
        "type": "string",
        "description": "Source language of the audio (optional, auto-detected if not specified). Examples: 'english', 'chinese', 'spanish', 'french', 'japanese', 'korean', etc. Use null or 'auto' for automatic language detection.",
        "required": "optional",
        "default": null,
        "example": "chinese"
      },
      "max_new_tokens": {
        "type": "integer",
        "description": "Maximum number of tokens to generate. Default: 440. Must be less than 448 to stay within model limits.",
        "required": "optional",
        "default": 440
      },
      "num_beams": {
        "type": "integer",
        "description": "Number of beams for beam search (1 for greedy decoding, higher values for better quality but slower speed).",
        "required": "optional",
        "default": 1
      },
      "temperature": {
        "type": "array",
        "description": "Temperature for sampling, supports fallback strategy. Can be a single float or tuple like [0.0, 0.2, 0.4, 0.6, 0.8, 1.0] for progressive fallback if the model fails at lower temperatures.",
        "required": "optional",
        "default": [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]
      },
      "compression_ratio_threshold": {
        "type": "number",
        "description": "Threshold for zlib compression ratio in token space. Higher values are more restrictive.",
        "required": "optional",
        "default": 1.35
      },
      "logprob_threshold": {
        "type": "number",
        "description": "Log probability threshold for token acceptance. Lower values are more restrictive.",
        "required": "optional",
        "default": -1.0
      },
      "no_speech_threshold": {
        "type": "number",
        "description": "Threshold for no-speech detection. Higher values are more likely to detect silence.",
        "required": "optional",
        "default": 0.6
      },
      "condition_on_prev_tokens": {
        "type": "boolean",
        "description": "Whether to condition generation on previous tokens. Useful for maintaining consistency.",
        "required": "optional",
        "default": false
      },
      "return_timestamps": {
        "type": "string",
        "description": "Timestamp generation mode: false for no timestamps, true for sentence-level timestamps, 'word' for word-level timestamps.",
        "required": "optional",
        "default": false,
        "enum": [false, true, "word"]
      },
      "batch_size": {
        "type": "integer",
        "description": "Batch size for processing multiple files simultaneously.",
        "required": "optional",
        "default": 1
      },
      "model_path": {
        "type": "string",
        "description": "Path to the local Whisper model directory.",
        "required": "optional",
        "default": "/home/chengz/LAMs/pre_train_models/models--openai--whisper-large-v3"
      },
      "torch_dtype": {
        "type": "string",
        "description": "PyTorch data type ('float16' for GPU with reduced memory usage, 'float32' for CPU or full precision).",
        "required": "optional",
        "default": "float16",
        "enum": ["float16", "float32"]
      },
      "low_cpu_mem_usage": {
        "type": "boolean",
        "description": "Whether to use low CPU memory usage during model loading.",
        "required": "optional",
        "default": true
      },
      "use_safetensors": {
        "type": "boolean",
        "description": "Whether to use safetensors format for faster and safer model loading.",
        "required": "optional",
        "default": true
      },
      "output_path": {
        "type": "string",
        "description": "Custom path to save the transcription output. If not provided, saves to 'whisper_output/whisper_result_{timestamp}.{format}'",
        "required": "optional"
      },
      "output_format": {
        "type": "string",
        "description": "Output format: 'json' for structured data with metadata, 'txt' for plain text transcription only.",
        "required": "optional",
        "default": "json",
        "enum": ["json", "txt"]
      },
      "device": {
        "type": "string",
        "description": "Computing device: 'auto' for automatic selection, 'cuda' for GPU, 'cpu' for CPU, or specific device like 'cuda:1'.",
        "required": "optional",
        "default": "auto"
      }
    },
    "examples": [
      {
        "task_description": "Basic transcription with auto language detection",
        "Act": {
          "tool": "WhisperASRTool",
          "arguments": {
            "audio_path": "input/audio/speech.wav"
          }
        }
      },
      {
        "task_description": "Chinese speech transcription with timestamps",
        "Act": {
          "tool": "WhisperASRTool",
          "arguments": {
            "audio_path": "input/audio/chinese_speech.wav",
            "language": "chinese",
            "return_timestamps": true,
            "output_format": "json"
          }
        }
      },
      {
        "task_description": "Translate foreign speech to English",
        "Act": {
          "tool": "WhisperASRTool",
          "arguments": {
            "audio_path": "input/audio/foreign_speech.wav",
            "task": "translate",
            "return_timestamps": "word"
          }
        }
      },
      {
        "task_description": "High-quality transcription with optimized parameters",
        "Act": {
          "tool": "WhisperASRTool",
          "arguments": {
            "audio_path": "input/audio/meeting.wav",
            "language": "english",
            "return_timestamps": "word",
            "temperature": 0.0,
            "num_beams": 3,
            "output_format": "json"
          }
        }
      },
      {
        "task_description": "Batch processing multiple audio files",
        "Act": {
          "tool": "WhisperASRTool",
          "arguments": {
            "audio_path": ["file1.wav", "file2.wav", "file3.wav"],
            "batch_size": 2,
            "task": "transcribe",
            "output_format": "json"
          }
        }
      }
    ]
  },
  "AudioSRTool": {
    "description": "Enhance audio quality through versatile super-resolution, upscaling any audio to 48kHz high-quality output.",
    "detailed_description": "AudioSRTool provides state-of-the-art audio super-resolution capabilities using the AudioSR model from the versatile_audio_super_resolution project. This tool can enhance the quality of any type of audio (music, speech, environmental sounds, etc.) by upscaling it to 48kHz high-quality output, regardless of the input sample rate. The tool supports two specialized models: 'basic' for general audio enhancement and 'speech' for speech-specific optimization. AudioSR works by using advanced deep learning techniques to reconstruct high-frequency components that may have been lost due to compression, low sampling rates, or other degradation factors. The enhanced audio maintains natural sound characteristics while significantly improving clarity and fidelity. The computing device (GPU/CPU) is automatically selected based on system availability, with CUDA GPU preferred for faster processing when available. This tool is particularly useful for restoring old recordings, improving compressed audio, or preparing audio for high-quality applications.",
    "parameters": {
      "audio_file": {
        "type": "string",
        "description": "Path to the input audio file to enhance. Supports various audio formats including WAV, MP3, FLAC, M4A, etc.",
        "required": true,
        "example": "input/audio/low_quality_recording.wav"
      },
      "output_file": {
        "type": "string",
        "description": "Path to save the enhanced audio file. If not provided, a timestamped filename will be generated automatically in the format 'audiosr_{model}_{input_name}_{timestamp}.wav'",
        "required": "optional",
        "example": "output/enhanced_audio.wav"
      },
      "model_name": {
        "type": "string",
        "description": "Model to use for audio enhancement. 'basic' is optimized for general audio types (music, environmental sounds), while 'speech' is specifically optimized for speech audio.",
        "required": "optional",
        "default": "basic",
        "enum": ["basic", "speech"],
        "example": "speech"
      }
    },
    "examples": [
      {
        "task_description": "Enhance general audio quality using basic model",
        "Act": {
          "tool": "AudioSRTool",
          "arguments": {
            "audio_file": "input/music_recording.wav",
            "output_file": "output/enhanced_music.wav",
            "model_name": "basic"
          }
        }
      },
      {
        "task_description": "Enhance speech audio with automatic output naming",
        "Act": {
          "tool": "AudioSRTool",
          "arguments": {
            "audio_file": "input/phone_call.wav",
            "model_name": "speech"
          }
        }
      },
      {
        "task_description": "Upscale compressed audio to high quality",
        "Act": {
          "tool": "AudioSRTool",
          "arguments": {
            "audio_file": "input/compressed_audio.mp3",
            "output_file": "output/restored_hq_audio.wav",
            "model_name": "basic"
          }
        }
      }
    ]
  },
  "DiffRhythmTool": {
    "description": "Generate high-quality full-length music and songs with vocals using the DiffRhythm diffusion-based music generation system.",
    "detailed_description": "DiffRhythmTool is the first open-source diffusion-based music generation model capable of creating complete full-length songs with both vocals and instrumental accompaniment. Developed by ASLP Lab, DiffRhythm leverages advanced diffusion architecture to produce high-quality music that seamlessly integrates lyrics with musical composition.\n\n**🎯 What This Tool Can Do:**\n\n**1. Complete Song Generation**\n• Generate full songs with vocals, lyrics, and instrumental accompaniment\n• Create instrumental-only music without lyrics\n• Support various musical genres (pop, rock, jazz, classical, electronic, etc.)\n• Produce professional-quality audio with natural-sounding vocals\n\n**2. Style Control & Transfer**\n• Use text prompts to describe desired musical style, mood, and instruments\n• Transfer style from reference audio files to new compositions\n• Combine lyrical content with specific musical characteristics\n• Create music in the style of existing songs while generating new melodies\n\n**3. Advanced Music Production Features**\n• Edit specific time segments of existing songs while preserving other parts\n• Generate multiple variations and automatically select the best quality output\n• Precise lyrics-to-music synchronization using LRC timestamp format\n• Support for complex song structures (verses, choruses, bridges)\n\n**4. Content Creation Applications**\n• Music for videos, podcasts, and multimedia projects\n• Demo creation for songwriters and musicians\n• Background music for games, apps, and presentations\n• AI-assisted music composition and production\n• Educational content for music theory and composition\n\n**🎼 Model Selection Guide:**\n\n**When to Use DiffRhythm-v1.2** (Recommended for most cases):\n✅ **Duration**: Up to 1 minute 35 seconds (95 seconds)\n✅ **Quality**: Latest model with best audio quality and most stable results\n✅ **Speed**: Faster inference (~40 seconds generation time)\n✅ **Memory**: Lower VRAM requirements (8GB+ recommended)\n✅ **Best For**:\n   • Short songs and music demos\n   • Social media content (TikTok, Instagram, YouTube Shorts)\n   • Commercial jingles and advertising music\n   • Podcast intros/outros\n   • Quick music prototyping and experimentation\n   • When you need multiple iterations quickly\n   • Most professional music production tasks\n\n**When to Use DiffRhythm-full** (For extended compositions):\n✅ **Duration**: Up to 4 minutes 45 seconds (285 seconds)\n✅ **Quality**: Good quality with extended generation capability\n✅ **Speed**: Slower inference (~50+ seconds generation time)\n✅ **Memory**: Higher VRAM requirements (12GB+ recommended)\n✅ **Best For**:\n   • Complete song compositions\n   • Full-length musical pieces\n   • Album tracks and extended musical content\n   • Background music for longer videos\n   • Ambient and atmospheric music\n   • When song structure requires multiple sections (verse-chorus-bridge-outro)\n   • Final production versions of compositions\n\n**🛠️ Technical Features & Optimizations:**\n\n**Memory Management**:\n• **Chunked Decoding**: Reduces VRAM usage by processing audio in segments\n• **Batch Generation**: Create multiple variations to improve output quality\n• **Model Offloading**: Automatic memory management during generation\n\n**Input Flexibility**:\n• **LRC Format Support**: Precise timestamp-based lyrics synchronization\n• **Multi-Modal Input**: Text prompts, reference audio, or existing song editing\n• **Style Transfer**: Extract and apply musical characteristics from reference tracks\n\n**Output Quality**:\n• **44.1kHz Sample Rate**: Professional audio quality\n• **Stereo Output**: Full stereo soundscape with proper mixing\n• **Format Support**: WAV output for maximum quality preservation\n\n**🎵 Practical Use Cases:**\n\n**Content Creators**:\n• Generate original background music for YouTube videos\n• Create unique music for podcasts and streaming content\n• Produce custom songs for social media campaigns\n\n**Musicians & Producers**:\n• Rapid prototyping of song ideas with lyrics\n• Style exploration and genre experimentation\n• Creating demos for client presentations\n• Generating inspiration for musical arrangements\n\n**Businesses & Brands**:\n• Custom jingles and advertising music\n• Background music for corporate videos\n• Unique audio branding elements\n• Event and presentation soundtracks\n\n**Developers & Game Designers**:\n• Dynamic background music for applications\n• Game soundtrack generation\n• Interactive audio content creation\n\n**Educational & Research**:\n• Music composition teaching aids\n• AI music research and experimentation\n• Demonstrating music theory concepts\n\n**💡 Pro Tips for Best Results:**\n\n**Model Selection Strategy**:\n• Start with v1.2 for testing and iteration due to faster generation\n• Use 'full' model only when you specifically need longer duration\n• Consider your hardware capabilities when choosing models\n\n**Prompt Engineering**:\n• Be specific about genre, instruments, and mood in text prompts\n• Include tempo descriptors (slow, moderate, fast, energetic)\n• Mention specific instruments for desired arrangements\n• Describe emotional characteristics (happy, melancholic, epic, peaceful)\n\n**Memory Optimization**:\n• Always enable 'chunked=true' for systems with 8GB or less VRAM\n• Use batch_infer_num=2-3 for better quality when hardware allows\n• Monitor system resources during generation\n\n**Quality Enhancement**:\n• Use reference audio from high-quality sources for style transfer\n• Ensure LRC timestamps are accurate for proper lyrics synchronization\n• Generate multiple versions and select the best output\n\nThe tool automatically handles model loading, audio processing, and output generation, making professional-quality music creation accessible through simple API calls.",
    "parameters": {
      "model_version": {
        "type": "string",
        "description": "Model version to use: 'v1.2' for latest quality with 1m35s max length, or 'full' for extended 4m45s generation capability.",
        "required": "optional",
        "default": "v1.2",
        "enum": ["v1.2", "full"],
        "example": "v1.2"
      },
      "lrc_path": {
        "type": "string",
        "description": "Path to lyrics file in LRC format with timestamps. LRC format example: '[00:00.00]First line\\n[00:05.00]Second line'",
        "required": "optional",
        "example": "lyrics/my_song.lrc"
      },
      "lrc_text": {
        "type": "string",
        "description": "Direct lyrics input in LRC format with timestamps. Used when lrc_path is not provided. Format: '[mm:ss.ff]Lyric line'",
        "required": "optional",
        "example": "[00:00.00]Hello world\\n[00:05.00]This is a test song\\n[00:10.00]AI music creation"
      },
      "ref_prompt": {
        "type": "string",
        "description": "Text prompt describing the desired musical style, genre, instruments, and mood. Cannot be used with ref_audio_path.",
        "required": "conditional",
        "example": "energetic pop song with electronic elements and upbeat rhythm"
      },
      "ref_audio_path": {
        "type": "string",
        "description": "Path to reference audio file for style transfer. The model will extract and apply the musical style from this audio. Cannot be used with ref_prompt.",
        "required": "conditional",
        "example": "reference/jazz_style.wav"
      },
      "chunked": {
        "type": "boolean",
        "description": "Enable chunked decoding to reduce VRAM usage. Recommended for systems with 8GB or less VRAM, especially for longer generations.",
        "required": "optional",
        "default": false
      },
      "batch_infer_num": {
        "type": "integer",
        "description": "Number of song variations to generate in parallel. The system randomly selects one as the final output. Higher values increase quality but require more computation.",
        "required": "optional",
        "default": 1,
        "example": 2
      },
      "edit": {
        "type": "boolean",
        "description": "Enable edit mode to modify specific sections of an existing song. Requires ref_song and edit_segments parameters.",
        "required": "optional",
        "default": false
      },
      "ref_song": {
        "type": "string",
        "description": "Path to existing song file to edit. Required when edit=true. The song will be used as a base for selective modification.",
        "required": "conditional",
        "example": "songs/original_composition.wav"
      },
      "edit_segments": {
        "type": "string",
        "description": "JSON string defining time segments to edit in format '[[start1,end1],[start2,end2]]'. Use -1 for audio start/end. Required when edit=true.",
        "required": "conditional",
        "example": "[[0,30],[60,-1]]"
      },
      "output_path": {
        "type": "string",
        "description": "Custom output path for the generated audio file. If not provided, saves to default music output directory with timestamp.",
        "required": "optional",
        "example": "output/music/my_generated_song.wav"
      },
      "verbose": {
        "type": "boolean",
        "description": "Enable detailed logging and progress information during generation process.",
        "required": "optional",
        "default": false
      }
    },
    "examples": [
      {
        "task_description": "Generate a short high-quality song with lyrics and text style prompt",
        "Act": {
          "tool": "DiffRhythmTool",
          "arguments": {
            "model_version": "v1.2",
            "lrc_text": "[00:00.00]Staring at the stars tonight\\n[00:05.00]Dreams are taking flight\\n[00:10.00]Music fills the air\\n[00:15.00]Magic everywhere",
            "ref_prompt": "dreamy pop ballad with soft piano and gentle strings",
            "chunked": true
          }
        }
      },
      {
        "task_description": "Generate full-length song using reference audio style",
        "Act": {
          "tool": "DiffRhythmTool",
          "arguments": {
            "model_version": "full",
            "lrc_path": "lyrics/complete_song.lrc",
            "ref_audio_path": "reference/jazz_standard.wav",
            "chunked": true,
            "output_path": "output/music/jazz_composition.wav"
          }
        }
      },
      {
        "task_description": "Create instrumental music without lyrics",
        "Act": {
          "tool": "DiffRhythmTool",
          "arguments": {
            "model_version": "v1.2",
            "ref_prompt": "epic orchestral instrumental with powerful drums and soaring strings",
            "batch_infer_num": 3,
            "chunked": true
          }
        }
      },
      {
        "task_description": "Edit specific sections of existing song",
        "Act": {
          "tool": "DiffRhythmTool",
          "arguments": {
            "model_version": "v1.2",
            "edit": true,
            "ref_song": "songs/original_track.wav",
            "edit_segments": "[[30,60],[90,-1]]",
            "ref_prompt": "add electric guitar solo with rock energy",
            "chunked": true
          }
        }
      },
      {
        "task_description": "Generate upbeat pop song with electronic elements",
        "Act": {
          "tool": "DiffRhythmTool",
          "arguments": {
            "model_version": "v1.2",
            "lrc_text": "[00:00.00]Turn up the volume loud\\n[00:04.00]Dance until the morning light\\n[00:08.00]Electronic beats surround\\n[00:12.00]Everything will be alright",
            "ref_prompt": "upbeat electronic pop with synthesizers, strong bass, and energetic drums",
            "batch_infer_num": 2,
            "chunked": true,
            "verbose": true
          }
        }
      },
      {
        "task_description": "Create atmospheric ambient music",
        "Act": {
          "tool": "DiffRhythmTool",
          "arguments": {
            "model_version": "full",
            "ref_prompt": "ambient atmospheric soundscape with soft pads, gentle reverb, and ethereal textures",
            "chunked": true,
            "output_path": "output/ambient/atmospheric_piece.wav"
          }
        }
      },
      {
        "task_description": "Generate YouTube Shorts background music",
        "Act": {
          "tool": "DiffRhythmTool",
          "arguments": {
            "model_version": "v1.2",
            "ref_prompt": "trendy upbeat background music for social media, catchy and energetic",
            "chunked": true,
            "batch_infer_num": 2,
            "output_path": "output/social_media/shorts_bg.wav"
          }
        }
      },
      {
        "task_description": "Create podcast intro music with specific branding",
        "Act": {
          "tool": "DiffRhythmTool",
          "arguments": {
            "model_version": "v1.2",
            "lrc_text": "[00:00.00]Welcome to the show\\n[00:03.00]Tech Talk Today\\n[00:06.00]Innovation and insights",
            "ref_prompt": "professional tech podcast intro, modern electronic with corporate feel",
            "chunked": true,
            "output_path": "output/podcasts/tech_talk_intro.wav"
          }
        }
      },
      {
        "task_description": "Generate children's song with educational lyrics",
        "Act": {
          "tool": "DiffRhythmTool",
          "arguments": {
            "model_version": "v1.2",
            "lrc_text": "[00:00.00]A is for apple, red and sweet\\n[00:04.00]B is for ball, so fun to beat\\n[00:08.00]C is for cat, with whiskers neat\\n[00:12.00]Learning the alphabet is such a treat",
            "ref_prompt": "cheerful children's educational song with acoustic guitar and xylophone",
            "chunked": true,
            "batch_infer_num": 2
          }
        }
      },
      {
        "task_description": "Create game background music with loop structure",
        "Act": {
          "tool": "DiffRhythmTool",
          "arguments": {
            "model_version": "full",
            "ref_prompt": "fantasy adventure game background music, orchestral with mystical elements, seamless loop",
            "chunked": true,
            "output_path": "output/games/fantasy_background.wav"
          }
        }
      },
      {
        "task_description": "Generate commercial jingle with brand message",
        "Act": {
          "tool": "DiffRhythmTool",
          "arguments": {
            "model_version": "v1.2",
            "lrc_text": "[00:00.00]Fresh coffee every morning\\n[00:03.00]Café Dreams, your perfect start\\n[00:06.00]Quality you can taste",
            "ref_prompt": "catchy commercial jingle, upbeat and memorable, acoustic with light percussion",
            "chunked": true,
            "batch_infer_num": 3
          }
        }
      },
      {
        "task_description": "Create meditation and relaxation music",
        "Act": {
          "tool": "DiffRhythmTool",
          "arguments": {
            "model_version": "full",
            "ref_prompt": "peaceful meditation music with soft piano, nature sounds, and gentle ambient tones",
            "chunked": true,
            "output_path": "output/wellness/meditation_track.wav"
          }
        }
      }
    ]
  },
  "cosyvoice2tool_api": {
    "description": "CosyVoice2 Text-to-Speech Tool",
    "detailed_description": "Uses the CosyVoice2 model to convert text into realistic speech. You can provide a reference audio as a voice style prompt (3-second fast cloning), or use text instructions to control the generation effect (natural language control).",
    "parameters": {
      "tts_text": {
        "type": "string",
        "description": "The text content to be synthesized into speech.",
        "required": true,
        "example": "Hello, this is a test of the CosyVoice2 text-to-speech tool."
      },
      "mode": {
        "type": "string",
        "description": "Inference mode. Use 'cosy' for 3s-fast-cloning with a prompt wav, or 'instruct' for natural language control.",
        "required": "optional",
        "default": "cosy",
        "enum": ["cosy", "instruct"],
        "example": "cosy"
      },
      "prompt_text": {
        "type": "string",
        "description": "Prompt text, used only in certain modes.",
        "required": "optional",
        "example": "A gentle and soothing voice."
      },
      "prompt_wav_path": {
        "type": "string",
        "description": "Path to the reference audio file, providing a voice style sample for 'cosy' mode.",
        "required": "optional",
        "example": "input/reference_voice.wav"
      },
      "instruct_text": {
        "type": "string",
        "description": "Instructional text used to control the generation effect in 'instruct' mode.",
        "required": "optional",
        "example": "Read the text in a happy and excited tone."
      },
      "seed": {
        "type": "number",
        "description": "Random inference seed to control the randomness of the generated result.",
        "required": "optional",
        "default": 0,
        "example": 42.0
      },
      "output_path": {
        "type": "string",
        "description": "The path to save the generated audio file.",
        "required": "optional",
        "default": "output.wav",
        "example": "output/generated_speech.wav"
      }
    },
    "examples": [
      {
        "task_description": "Synthesize speech using a reference voice audio (voice cloning).",
        "Act": {
          "tool": "cosyvoice2tool_api",
          "arguments": {
            "tts_text": "The quick brown fox jumps over the lazy dog.",
            "mode": "cosy",
            "prompt_wav_path": "input/source_voice.wav",
            "output_path": "output/cloned_voice.wav"
          }
        }
      },
      {
        "task_description": "Synthesize speech using natural language instructions.",
        "Act": {
          "tool": "cosyvoice2tool_api",
          "arguments": {
            "tts_text": "Welcome to the world of tomorrow!",
            "mode": "instruct",
            "instruct_text": "Speak in a deep, booming announcer voice.",
            "output_path": "output/announcer_voice.wav"
          }
        }
      },
      {
        "task_description": "Perform a basic text-to-speech conversion with a specific random seed.",
        "Act": {
          "tool": "cosyvoice2tool_api",
          "arguments": {
            "tts_text": "This is a simple text-to-speech conversion.",
            "seed": 123.0,
            "output_path": "output/simple_tts.wav"
          }
        }
      }
    ]
  },
  "AudioX_api": {
    "description": "AudioX Audio Generation Tool",
    "detailed_description": "Uses the AudioX model to generate high-quality audio based on text prompts, video, or audio prompts. This tool can synthesize audio from various inputs, including text descriptions, reference video files, and audio prompts, offering detailed control over the generation process through numerous parameters.",
    "parameters": {
      "prompt": {
        "type": "string",
        "description": "A text prompt describing the audio content to be generated.",
        "required": "optional",
        "default": "",
        "example": "A cinematic explosion with debris sounds"
      },
      "negative_prompt": {
        "type": "string",
        "description": "A negative prompt describing features that should not appear in the generated result.",
        "required": "optional",
        "default": null,
        "example": "low quality, muffled, noisy"
      },
      "video_file_path": {
        "type": "string",
        "description": "Path to the video file to be used as a generation reference.",
        "required": "optional",
        "default": null,
        "example": "input/source_video.mp4"
      },
      "audio_prompt_file_path": {
        "type": "string",
        "description": "Path to the audio prompt file to be used as a generation reference.",
        "required": "optional",
        "default": null,
        "example": "input/reference_audio.wav"
      },
      "audio_prompt_path": {
        "type": "string",
        "description": "Path to the audio prompt to be used as a generation reference.",
        "required": "optional",
        "default": null,
        "example": "input/reference_prompt.wav"
      },
      "seconds_start": {
        "type": "float",
        "description": "The start time in seconds of the video to use for generation.",
        "required": "optional",
        "default": 0,
        "example": 5.0
      },
      "seconds_total": {
        "type": "float",
        "description": "The total duration in seconds of the audio to be generated.",
        "required": "optional",
        "default": 10,
        "example": 15.5
      },
      "cfg_scale": {
        "type": "float",
        "description": "CFG scale parameter, controlling how closely the generation follows the prompt.",
        "required": "optional",
        "default": 7,
        "example": 8.5
      },
      "steps": {
        "type": "float",
        "description": "Number of sampling steps, affecting generation quality and time.",
        "required": "optional",
        "default": 100,
        "example": 150
      },
      "preview_every": {
        "type": "float",
        "description": "Sets the preview frequency during generation.",
        "required": "optional",
        "default": 0,
        "example": 10
      },
      "seed": {
        "type": "string",
        "description": "Random seed for generation. Set to '-1' for a random seed.",
        "required": "optional",
        "default": "-1",
        "example": "12345"
      },
      "sampler_type": {
        "type": "string",
        "description": "The type of sampler to use.",
        "required": "optional",
        "default": "dpmpp-3m-sde",
        "enum": ["dpmpp-2m-sde", "dpmpp-3m-sde"],
        "example": "dpmpp-2m-sde"
      },
      "sigma_min": {
        "type": "float",
        "description": "The minimum sigma value for the sampler.",
        "required": "optional",
        "default": 0.03,
        "example": 0.05
      },
      "sigma_max": {
        "type": "float",
        "description": "The maximum sigma value for the sampler.",
        "required": "optional",
        "default": 500,
        "example": 400
      },
      "cfg_rescale": {
        "type": "float",
        "description": "CFG rescale amount.",
        "required": "optional",
        "default": 0,
        "example": 0.5
      },
      "use_init": {
        "type": "boolean",
        "description": "Whether to use an initial audio file for generation.",
        "required": "optional",
        "default": false,
        "example": true
      },
      "init_audio_path": {
        "type": "string",
        "description": "Path to the initial audio file.",
        "required": "optional",
        "default": null,
        "example": "input/initial_sound.wav"
      },
      "init_noise_level": {
        "type": "float",
        "description": "The noise level to apply to the initial audio.",
        "required": "optional",
        "default": 0.1,
        "example": 0.2
      },
      "output_audio_path": {
        "type": "string",
        "description": "Path to save the output audio file.",
        "required": "optional",
        "default": "output_audio.wav",
        "example": "output/generated_sound.wav"
      },
      "output_video_path": {
        "type": "string",
        "description": "Path to save the output video file.",
        "required": "optional",
        "default": "output_video.mp4",
        "example": "output/generated_video.mp4"
      }
    },
    "examples": [
      {
        "task_description": "Generate audio from a text prompt",
        "Act": {
          "tool": "AudioX_api",
          "arguments": {
            "prompt": "sound of a heavy rainstorm with thunder",
            "seconds_total": 15,
            "output_audio_path": "output/rainstorm.wav"
          }
        }
      },
      {
        "task_description": "Generate audio conditioned on a video file",
        "Act": {
          "tool": "AudioX_api",
          "arguments": {
            "prompt": "footsteps on a gravel path",
            "video_file_path": "input/walking_video.mp4",
            "seconds_total": 20,
            "output_audio_path": "output/footsteps_audio.wav",
            "output_video_path": "output/walking_video_with_audio.mp4"
          }
        }
      },
      {
        "task_description": "Generate audio using an initial audio file and advanced settings",
        "Act": {
          "tool": "AudioX_api",
          "arguments": {
            "prompt": "futuristic vehicle engine sound",
            "use_init": true,
            "init_audio_path": "input/base_engine_sound.wav",
            "init_noise_level": 0.2,
            "steps": 150,
            "cfg_scale": 9.0,
            "sampler_type": "dpmpp-2m-sde",
            "output_audio_path": "output/futuristic_engine.wav"
          }
        }
      }
    ]
  },
  "Qwen2audio_api": {
    "description": "Qwen2-Audio-7B-Instruct Multimodal Dialogue Tool",
    "detailed_description": "Engage in multimodal conversations using both text and audio with the Qwen2-Audio-7B-Instruct model. You can send text or audio for dialogue and also manage the conversation history.",
    "parameters": {
        "prompt": {
            "type": "string",
            "description": "Text prompt, the user's text input.",
            "required": "optional",
            "default": "",
            "example": "Please describe what you hear in this audio."
        },
        "audio_file_path": {
            "type": "string",
            "description": "Path to the input audio file, the user's audio input.",
            "required": "optional",
            "example": "input/audio/user_query.wav"
        },
        "chatbot_history": {
            "type": "list",
            "description": "The chat history, formatted as a list. If None, a new conversation is started.",
            "required": "optional",
            "example": "[{\"role\": \"user\", \"content\": \"What is this sound?\"}, {\"role\": \"assistant\", \"content\": \"It sounds like a bird chirping.\"}]"
        },
        "action": {
            "type": "string",
            "description": "The type of action to perform. Options are 'chat' (to continue the conversation), 'regenerate' (to regenerate the last response), and 'reset' (to reset the conversation).",
            "required": "optional",
            "default": "chat",
            "enum": ["chat", "regenerate", "reset"],
            "example": "regenerate"
        },
        "save_history": {
            "type": "boolean",
            "description": "Whether to save the conversation history to a file.",
            "required": "optional",
            "default": true,
            "example": false
        },
        "output_file": {
            "type": "string",
            "description": "The file path to save the conversation history.",
            "required": "optional",
            "default": "conversation_history.json",
            "example": "output/my_conversation.json"
        }
    },
    "examples": [
        {
          "task_description": "Start a new multimodal chat with an audio file and a text prompt.",
          "Act": {
              "tool": "Qwen2audio_api",
              "arguments": {
                  "prompt": "What kind of bird is singing in this audio?",
                  "audio_file_path": "input/bird_song.mp3",
                  "action": "chat",
                  "output_file": "output/bird_chat.json"
              }
            }
        },
        {
            "task_description": "Continue a text-only conversation without sending a new audio file.",
            "Act": {
                "tool": "Qwen2audio_api",
                "arguments": {
                    "prompt": "Thank you. Can you tell me more about that species?",
                    "chatbot_history": "[...]",
                    "action": "chat"
                }
            }
        },
        {
            "task_description": "Regenerate the last response from the assistant.",
            "Act": {
                "tool": "Qwen2audio_api",
                "arguments": {
                    "chatbot_history": "[...]",
                    "action": "regenerate"
                }
            }
        },
        {
            "task_description": "Reset the conversation and start fresh.",
            "Act": {
                "tool": "Qwen2audio_api",
                "arguments": {
                    "action": "reset"
                }
            }
        }
    ]
  },
  "clearervoice_api": {
    "description": "A multi-task audio processing tool based on the alibabasglab/ClearVoice model, capable of speech enhancement, separation, super-resolution, and audio-visual speaker extraction.",
    "detailed_description": "This tool encapsulates four core capabilities from the alibabasglab/ClearVoice Hugging Face Space, allowing selection of different tasks via the 'task' parameter, which automatically calls the corresponding API endpoint. Supported tasks include: 1. 'enhancement' for noise reduction and speech enhancement with selectable sample rates. 2. 'separation' to split audio into two tracks, like vocals and background. 3. 'super_resolution' to upscale low-sample-rate audio, with an option to apply enhancement. 4. 'av_extraction' to extract an audio track from a video file.",
    "parameters": {
      "task": {
        "type": "string",
        "description": "The audio processing task to perform. Select from 'enhancement', 'separation', 'super_resolution', or 'av_extraction'.",
        "required": "optional",
        "default": "enhancement",
        "enum": [
          "enhancement",
          "separation",
          "super_resolution",
          "av_extraction"
        ],
        "example": "separation"
      },
      "input_path": {
        "type": "string",
        "description": "Path or URL to the input audio or video file (e.g., WAV, MP3, MP4). Required for all tasks.",
        "required": true,
        "example": "input/meeting_recording.wav"
      },
      "sr": {
        "type": "string",
        "description": "Used only for the 'enhancement' task. Specifies the sample rate model to use.",
        "required": "optional",
        "default": "48000 Hz",
        "enum": [
          "16000 Hz",
          "48000 Hz"
        ],
        "example": "16000 Hz"
      },
      "apply_se": {
        "type": "boolean",
        "description": "Used only for the 'super_resolution' task. If true, applies speech enhancement after upscaling.",
        "required": "optional",
        "default": true,
        "example": false
      },
      "output_audio_path": {
        "type": "string",
        "description": "Path to save the output audio for 'enhancement', 'super_resolution', or the first track of the 'separation' task.",
        "required": "optional",
        "default": "output.wav",
        "example": "output/enhanced_speech.wav"
      },
      "output_audio_path2": {
        "type": "string",
        "description": "Path to save the second output audio track for the 'separation' task (e.g., background noise).",
        "required": "optional",
        "default": "output_2.wav",
        "example": "output/background_track.wav"
      },
      "output_dir": {
        "type": "string",
        "description": "Directory to save the output files for the 'av_extraction' task.",
        "required": "optional",
        "default": "av_outputs",
        "example": "output/extracted_clips/"
      }
    },
    "examples": [
      {
        "task_description": "Enhance a noisy speech recording using the 48kHz model.",
        "Act": {
          "tool": "ClearerVoiceTool",
          "arguments": {
            "task": "enhancement",
            "input_path": "input/noisy_podcast.wav",
            "output_audio_path": "output/podcast_enhanced.wav",
            "sr": "48000 Hz"
          }
        }
      },
      {
        "task_description": "Separate a speaker's voice from background music.",
        "Act": {
          "tool": "ClearerVoiceTool",
          "arguments": {
            "task": "separation",
            "input_path": "input/interview_with_music.wav",
            "output_audio_path": "output/interview_voice.wav",
            "output_audio_path2": "output/interview_background.wav"
          }
        }
      },
      {
        "task_description": "Upscale a low-quality recording and apply enhancement.",
        "Act": {
          "tool": "ClearerVoiceTool",
          "arguments": {
            "task": "super_resolution",
            "input_path": "input/old_recording_8khz.wav",
            "output_audio_path": "output/restored_recording_48khz.wav",
            "apply_se": true
          }
        }
      }
    ]
  },
  "diffrhythm_api": {
    "description": "A tool for music generation and utility tasks using the DiffRhythm platform.",
    "detailed_description": "This tool encapsulates multiple endpoints of the dskill/DiffRhythm Web platform, offering a suite of capabilities. It allows for the generation of timed LRC snippets from themes and tags (theme_tags), the conversion of raw lyrics into an aligned LRC format (lyrics_lrc), and the generation of instrumental music from LRC files and text prompts (infer_music). It also includes a reserved function to query or update an internal lambda value (lambda_val). The music generation process is highly customizable, with parameters to control the diffusion process such as seed, steps, and CFG strength, as well as the output file format and the ODE solver method.",
    "parameters": {
      "task": {
        "type": "string",
        "description": "The specific task to perform. 'theme_tags': generate LRC from a theme. 'lyrics_lrc': convert lyrics to LRC. 'lambda_val': query the internal lambda value. 'infer_music': generate music.",
        "required": "optional",
        "default": "infer_music",
        "enum": [
          "theme_tags",
          "lyrics_lrc",
          "lambda_val",
          "infer_music"
        ]
      },
      "theme": {
        "type": "string",
        "description": "The theme description, used for the 'theme_tags' task.",
        "required": "optional",
        "example": "A rainy night in a cyberpunk city"
      },
      "tags_gen": {
        "type": "string",
        "description": "Comma-separated tags for generation, used for the 'theme_tags' task.",
        "required": "optional",
        "example": "electronic, ambient, slow tempo, neon lights"
      },
      "language": {
        "type": "string",
        "description": "Language for the 'theme_tags' task.",
        "required": "optional",
        "default": "en",
        "enum": [
          "en",
          "cn"
        ]
      },
      "tags_lyrics": {
        "type": "string",
        "description": "Tags associated with the lyrics, used for the 'lyrics_lrc' task.",
        "required": "optional",
        "example": "pop, upbeat"
      },
      "lyrics_input": {
        "type": "string",
        "description": "The raw lyric string (with line breaks) to be converted to LRC format for the 'lyrics_lrc' task.",
        "required": "optional",
        "example": "Verse 1\nWalking down the lonely road\nChorus\nI see the sunshine"
      },
      "lrc": {
        "type": "string",
        "description": "The aligned LRC content or a path to an LRC file, required for the 'infer_music' task.",
        "required": "optional",
        "example": "[00:10.50]Hello world\n[00:12.00]This is a test"
      },
      "text_prompt": {
        "type": "string",
        "description": "Text prompt describing the desired music style, required for the 'infer_music' task.",
        "required": "optional",
        "example": "A beautiful, soothing, and emotional piece of music, with a soft piano and strings, 80 bpm."
      },
      "seed": {
        "type": "number",
        "description": "Seed for the diffusion process to ensure reproducibility. Used in the 'infer_music' task.",
        "required": "optional",
        "default": 0
      },
      "randomize_seed": {
        "type": "boolean",
        "description": "If true, ignores the provided seed and uses a random one. Used in the 'infer_music' task.",
        "required": "optional",
        "default": true
      },
      "steps": {
        "type": "number",
        "description": "Number of inference steps for the diffusion model. Used in the 'infer_music' task.",
        "required": "optional",
        "default": 32
      },
      "cfg_strength": {
        "type": "number",
        "description": "Classifier-Free Guidance strength. Higher values adhere more strictly to the prompt. Used in the 'infer_music' task.",
        "required": "optional",
        "default": 4
      },
      "file_type": {
        "type": "string",
        "description": "The output format for the generated audio file. Used in the 'infer_music' task.",
        "required": "optional",
        "default": "mp3",
        "enum": [
          "mp3",
          "wav",
          "ogg"
        ]
      },
      "odeint_method": {
        "type": "string",
        "description": "The ODE solver method for the inference process. Used in the 'infer_music' task.",
        "required": "optional",
        "default": "euler",
        "enum": [
          "euler",
          "midpoint",
          "rk4",
          "implicit_adams"
        ]
      },
      "output_music_path": {
        "type": "string",
        "description": "Path to save the final generated music file. Used in the 'infer_music' task.",
        "required": "optional",
        "default": "diff_music_output.mp3",
        "example": "output/music/final_track.mp3"
      }
    },
    "examples": [
      {
        "task_description": "Generate a piece of music based on LRC content and a text prompt.",
        "Act": {
          "tool": "diffrhythm_api",
          "arguments": {
            "task": "infer_music",
            "lrc": "[00:05.00]Epic orchestral music\n[00:10.00]Fantasy cinematic theme",
            "text_prompt": "Epic orchestral music, fantasy, cinematic, powerful brass section, 120 bpm",
            "cfg_strength": 5,
            "steps": 40,
            "output_music_path": "output/epic_soundtrack.mp3"
          }
        }
      },
      {
        "task_description": "Generate an LRC timing file from a theme and descriptive tags in Chinese.",
        "Act": {
          "tool": "diffrhythm_api",
          "arguments": {
            "task": "theme_tags",
            "theme": "赛博朋克都市的雨夜",
            "tags_gen": "电子, 氛围, 慢节奏, 霓虹灯",
            "language": "cn"
          }
        }
      },
      {
        "task_description": "Convert a block of raw English lyrics into an LRC formatted string.",
        "Act": {
          "tool": "diffrhythm_api",
          "arguments": {
            "task": "lyrics_lrc",
            "tags_lyrics": "Acoustic, ballad, heartfelt",
            "lyrics_input": "The stars are out tonight\nI feel you by my side\nAnd everything's alright"
          }
        }
      }
    ]
  },
  "ACE_Step_api": {
    "description": "An integrated tool for end-to-end music generation, editing, and extension from text.",
    "detailed_description": "ACE-Step is an end-to-end web platform for text-to-music generation and subsequent audio fine-tuning, repainting, and extension. This tool encapsulates nine key endpoints, allowing users to complete the entire pipeline from scratch to multi-round editing using a single 'task' parameter and its corresponding arguments. Supported tasks include 'text2music' for initial generation, 'retake' for resampling, 'repaint' to redraw a specific time segment, 'edit' for modifying lyrics or style, and 'extend' to lengthen the audio. Tasks can be chained together; for example, the output of 'text2music' can be fed into 'repaint' or 'extend' for further modifications. The tool also provides utility functions like 'sample_data' to fetch hyperparameter templates and 'get_audio' to retrieve the audio from any stage of the process.",
    "parameters": {
      "task": {
        "type": "string",
        "description": "The primary operation to perform. Determines which endpoint is called and which other parameters are required.",
        "required": true,
        "default": "text2music",
        "enum": [
          "text2music",
          "retake",
          "repaint",
          "edit",
          "extend",
          "sample_data",
          "get_audio",
          "edit_type"
        ]
      },
      "input_json_1": {
        "type": [
          "object",
          "array",
          "string",
          "number",
          "boolean"
        ],
        "description": "The primary JSON data from a previous step. Required for 'retake', 'repaint', 'edit', and 'extend' tasks, where it usually represents the parameters of the music to be modified.",
        "required": "optional"
      },
      "input_json_2": {
        "type": [
          "object",
          "array",
          "string",
          "number",
          "boolean"
        ],
        "description": "The secondary JSON data for complex tasks. Required for 'repaint', 'edit', and 'extend' tasks.",
        "required": "optional"
      },
      "input_audio_path": {
        "type": "string",
        "description": "Path to an input audio file, used when a task requires an uploaded audio source (e.g., repaint, edit, extend from an upload).",
        "required": "optional"
      },
      "prompt": {
        "type": "string",
        "description": "Music style tags, separated by commas. Required for 'text2music' task. Example: 'pop, 120BPM, energetic'.",
        "required": "optional",
        "default": ""
      },
      "lyrics": {
        "type": "string",
        "description": "The lyrics for the music. Required for 'text2music' task. Can be multi-line text.",
        "required": "optional",
        "default": ""
      },
      "infer_step": {
        "type": "number",
        "description": "Number of topology steps. Higher values increase quality but are slower. Range: 1-100.",
        "required": "optional",
        "default": 27
      },
      "guidance_scale": {
        "type": "number",
        "description": "The CFG Scale for inference. Range: 1-30.",
        "required": "optional",
        "default": 15
      },
      "scheduler_type": {
        "type": "string",
        "description": "The type of sampler to use.",
        "required": "optional",
        "default": "euler",
        "enum": [
          "euler",
          "heun"
        ]
      },
      "cfg_type": {
        "type": "string",
        "description": "The CFG scheme to use.",
        "required": "optional",
        "default": "apg",
        "enum": [
          "cfg",
          "apg",
          "cfg_star"
        ]
      },
      "omega_scale": {
        "type": "number",
        "description": "Granularity control. Range: 0-20.",
        "required": "optional",
        "default": 10
      },
      "manual_seeds": {
        "type": "string",
        "description": "Manual seeds for reproducibility, in the format '42,17'. If null or empty, seeds are chosen randomly.",
        "required": "optional",
        "default": null
      },
      "guidance_interval": {
        "type": "number",
        "description": "CFG trigger interval. Range: 0-1.",
        "required": "optional",
        "default": 0.5
      },
      "guidance_interval_decay": {
        "type": "number",
        "description": "Decay for the guidance interval.",
        "required": "optional",
        "default": 0.0
      },
      "min_guidance_scale": {
        "type": "number",
        "description": "Minimum guidance scale.",
        "required": "optional",
        "default": 3
      },
      "use_erg_tag": {
        "type": "boolean",
        "description": "Enable the ERG enhancement module for tags.",
        "required": "optional",
        "default": true
      },
      "use_erg_lyric": {
        "type": "boolean",
        "description": "Enable the ERG enhancement module for lyrics.",
        "required": "optional",
        "default": true
      },
      "use_erg_diffusion": {
        "type": "boolean",
        "description": "Enable the ERG enhancement module for diffusion.",
        "required": "optional",
        "default": true
      },
      "oss_steps": {
        "type": "string",
        "description": "Comma-separated OSS stage steps, e.g., '0.2,0.6'. If null, uses default.",
        "required": "optional",
        "default": null
      },
      "guidance_scale_text": {
        "type": "number",
        "description": "Specific guidance scale for text.",
        "required": "optional",
        "default": 0.0
      },
      "guidance_scale_lyric": {
        "type": "number",
        "description": "Specific guidance scale for lyrics.",
        "required": "optional",
        "default": 0.0
      },
      "retake_variance": {
        "type": "number",
        "description": "Random perturbation amplitude for 'retake' and 'repaint' tasks. Range: 0-1.",
        "required": "optional",
        "default": 0.2
      },
      "retake_seeds": {
        "type": "string",
        "description": "Comma-separated seeds for resampling in 'retake' task. Required for 'retake'.",
        "required": "optional",
        "default": ""
      },
      "repaint_start": {
        "type": "number",
        "description": "Start time in seconds for the repaint region.",
        "required": "optional",
        "default": 0.0
      },
      "repaint_end": {
        "type": "number",
        "description": "End time in seconds for the repaint region.",
        "required": "optional",
        "default": 30.0
      },
      "repaint_source": {
        "type": "string",
        "description": "Source audio for the 'repaint' task.",
        "required": "optional",
        "default": "text2music",
        "enum": [
          "text2music",
          "last_repaint",
          "upload"
        ]
      },
      "edit_type": {
        "type": "string",
        "description": "The type of edit to perform in the 'edit' task.",
        "required": "optional",
        "default": "only_lyrics",
        "enum": [
          "only_lyrics",
          "remix"
        ]
      },
      "edit_prompt": {
        "type": "string",
        "description": "New style tags for the 'edit' task.",
        "required": "optional",
        "default": ""
      },
      "edit_lyrics": {
        "type": "string",
        "description": "New lyrics for the 'edit' task.",
        "required": "optional",
        "default": ""
      },
      "edit_n_min": {
        "type": "number",
        "description": "Minimum random range for 'remix' editing.",
        "required": "optional",
        "default": 0.6
      },
      "edit_n_max": {
        "type": "number",
        "description": "Maximum random range for 'remix' editing.",
        "required": "optional",
        "default": 1.0
      },
      "left_extend_length": {
        "type": "number",
        "description": "Duration in seconds to extend to the left for the 'extend' task.",
        "required": "optional",
        "default": 0.0
      },
      "right_extend_length": {
        "type": "number",
        "description": "Duration in seconds to extend to the right for the 'extend' task.",
        "required": "optional",
        "default": 30.0
      },
      "extend_source": {
        "type": "string",
        "description": "Source audio for the 'extend' or 'edit' task.",
        "required": "optional",
        "default": "text2music",
        "enum": [
          "text2music",
          "last_extend",
          "upload"
        ]
      },
      "extend_seeds": {
        "type": "string",
        "description": "Comma-separated seeds for the 'extend' task. Required for 'extend'.",
        "required": "optional",
        "default": ""
      },
      "lambda_stage": {
        "type": "string",
        "description": "Specifies which stage's source audio to retrieve for the 'get_audio' task.",
        "required": "optional",
        "default": "text2music",
        "enum": [
          "text2music",
          "last_repaint",
          "upload",
          "last_edit",
          "last_extend"
        ]
      },
      "output_audio_path": {
        "type": "string",
        "description": "Path to save the output audio file.",
        "required": "optional",
        "default": "ace_step_output.wav"
      },
      "output_json_path": {
        "type": "string",
        "description": "Path to save the output parameters JSON file.",
        "required": "optional",
        "default": "ace_step_params.json"
      }
    },
    "examples": [
      {
        "task_description": "Generate music from text and lyrics.",
        "Act": {
          "tool": "ACE_Step_api",
          "arguments": {
            "task": "text2music",
            "prompt": "lofi, chill, instrumental",
            "lyrics": "Verse 1: Coding through the late night hours.",
            "output_audio_path": "output/lofi_song.wav",
            "output_json_path": "output/lofi_song_params.json"
          }
        }
      },
      {
        "task_description": "Retake a previously generated music piece with new seeds for variation.",
        "Act": {
          "tool": "ACE_Step_api",
          "arguments": {
            "task": "retake",
            "input_json_1": "output/lofi_song_params.json",
            "retake_variance": 0.3,
            "retake_seeds": "123,456",
            "output_audio_path": "output/lofi_song_retake.wav",
            "output_json_path": "output/lofi_song_retake_params.json"
          }
        }
      },
      {
        "task_description": "Extend a music piece by 10 seconds to the right.",
        "Act": {
          "tool": "ACE_Step_api",
          "arguments": {
            "task": "extend",
            "input_json_1": "output/lofi_song_retake_params.json",
            "input_json_2": {},
            "right_extend_length": 10.0,
            "extend_seeds": "789,101",
            "extend_source": "text2music",
            "output_audio_path": "output/lofi_extended.wav",
            "output_json_path": "output/lofi_extended_params.json"
          }
        }
      },
      {
        "task_description": "Fetch hyperparameter templates for quick tuning.",
        "Act": {
          "tool": "ACE_Step_api",
          "arguments": {
            "task": "sample_data"
          }
        }
      }
    ]
  },
  "SenseVoice_api": {
    "description": "A speech understanding tool based on SenseVoice-Small for multi-task processing.",
    "detailed_description": "Based on the megatrump/SenseVoice Hugging Face Space (API endpoint /model_inference), this tool performs multiple speech-related tasks in a single call: 1. Automatic Speech Recognition (ASR), 2. Language Identification (LID), 3. Speech Emotion Recognition (SER), and 4. Acoustic Event Detection (AED). The model supports several languages: zh (Chinese), en (English), yue (Cantonese), ja (Japanese), and ko (Korean). Select 'auto' to automatically detect the language. Recommended input duration is ≤30 seconds. This tool returns a string containing the recognized result. Example: '🎼[music] Hello, I'm in a good mood today. 😊'",
    "parameters": {
      "input_wav_path": {
        "type": "string",
        "description": "The path to the local audio file (e.g., .wav, .mp3).",
        "required": true,
        "example": "input/audio/recording_to_transcribe.wav"
      },
      "language": {
        "type": "string",
        "description": "Language code for the input audio. 'auto' enables automatic detection.",
        "required": false,
        "default": "auto",
        "enum": [
          "auto",
          "zh",
          "en",
          "yue",
          "ja",
          "ko",
          "nospeech"
        ],
        "example": "en"
      }
    },
    "examples": [
      {
        "task_description": "Transcribe an English audio file.",
        "Act": {
          "tool": "SenseVoice_api",
          "arguments": {
            "input_wav_path": "path/to/english_speech.wav",
            "language": "en"
          }
        }
      },
      {
        "task_description": "Transcribe an audio file with auto language detection.",
        "Act": {
          "tool": "SenseVoice_api",
          "arguments": {
            "input_wav_path": "path/to/unknown_language.wav"
          }
        }
      },
      {
        "task_description": "Transcribe a Japanese audio file.",
        "Act": {
          "tool": "SenseVoice_api",
          "arguments": {
            "input_wav_path": "path/to/japanese_podcast.wav",
            "language": "ja"
          }
        }
      }
    ]
  },
  "whisper_large_v3_turbo_api": {
    "description": "Performs audio transcription or translation using the hf-audio/whisper-large-v3-turbo model.",
    "detailed_description": "This tool supports three input methods: local audio files, online audio file URLs, and YouTube video URLs. When 'audio_path' is provided, the /predict endpoint is called. When 'yt_url' is provided, the /predict_2 endpoint is called. The 'audio_path' and 'yt_url' parameters are mutually exclusive; please provide only one. Supported tasks include 'transcribe' (speech-to-text) and 'translate' (translates audio to English). The result is returned as transcribed or translated text. If 'output_path' is specified, the result will be saved to the file and the file path will be returned.",
    "parameters": {
      "audio_path": {
        "type": "string",
        "description": "Path to the local audio file or a directly accessible audio file URL. This is mutually exclusive with 'yt_url'.",
        "required": "optional",
        "example": "path/to/audio.wav"
      },
      "yt_url": {
        "type": "string",
        "description": "The URL of a YouTube video. This is mutually exclusive with 'audio_path'.",
        "required": "optional",
        "example": "https://www.youtube.com/watch?v=example"
      },
      "task": {
        "type": "string",
        "description": "The task to be performed. Can be 'transcribe' (default) or 'translate'.",
        "required": "optional",
        "default": "transcribe",
        "enum": [
          "transcribe",
          "translate"
        ],
        "example": "transcribe"
      },
      "output_path": {
        "type": "string",
        "description": "Optional path to save the result file. If provided, the function writes the result to this file and returns the path; otherwise, it returns the text result directly.",
        "required": "optional",
        "example": "output/result.txt"
      }
    },
    "examples": [
      {
        "task_description": "Transcribe a local audio file and save the result.",
        "Act": {
          "tool": "whisper_large_v3_turbo_api",
          "arguments": {
            "audio_path": "path/to/your/audio.mp3",
            "task": "transcribe",
            "output_path": "output/transcription.txt"
          }
        }
      },
      {
        "task_description": "Translate the audio from a YouTube video into English.",
        "Act": {
          "tool": "whisper_large_v3_turbo_api",
          "arguments": {
            "yt_url": "https://www.youtube.com/watch?v=example_video_id",
            "task": "translate"
          }
        }
      },
      {
        "task_description": "Transcribe an audio file from a URL and return the text directly.",
        "Act": {
          "tool": "whisper_large_v3_turbo_api",
          "arguments": {
            "audio_path": "https://example.com/audio.mp3"
          }
        }
      }
    ]
  },
  "tiger_api": {
    "description": "TIGER audio extraction tool, capable of separating audio tracks from audio or video files.",
    "detailed_description": "This tool utilizes the TIGER model to perform various audio separation tasks. Supported API endpoints/tasks: '/separate_dnr' separates dialogue, sound effects, and music from audio files. '/separate_speakers' separates up to 4 speakers from an audio file. '/separate_dnr_video' separates dialogue, sound effects, and music from a video file and returns the separated videos. '/separate_speakers_video' separates up to 4 speakers from a video file and returns the separated videos. The tool accepts a path to a single audio or video file and returns a list containing the paths of all output files.",
    "parameters": {
      "input_file_path": {
        "type": "string",
        "description": "Path to the input audio or video file.",
        "required": true,
        "example": "path/to/audio.wav"
      },
      "task": {
        "type": "string",
        "description": "The task to be performed. Must be one of the four valid API endpoints.",
        "required": true,
        "enum": [
          "/separate_dnr",
          "/separate_speakers",
          "/separate_dnr_video",
          "/separate_speakers_video"
        ],
        "example": "/separate_speakers"
      },
      "output_dir": {
        "type": "string",
        "description": "The directory path to save the output files.",
        "required": "optional",
        "default": "output",
        "example": "results/speakers"
      }
    },
    "examples": [
      {
        "task_description": "Separate up to 4 speakers from an audio file.",
        "Act": {
          "tool": "tiger_audio_extraction_api",
          "arguments": {
            "input_file_path": "path/to/audio.wav",
            "task": "/separate_speakers",
            "output_dir": "results/speakers"
          }
        }
      },
      {
        "task_description": "Separate dialogue, sound effects, and music from a video file.",
        "Act": {
          "tool": "tiger_audio_extraction_api",
          "arguments": {
            "input_file_path": "path/to/video.mp4",
            "task": "/separate_dnr_video",
            "output_dir": "results/dnr"
          }
        }
      },
      {
        "task_description": "Separate dialogue, sound effects, and music from an audio file.",
        "Act": {
          "tool": "tiger_audio_extraction_api",
          "arguments": {
            "input_file_path": "path/to/meeting.mp3",
            "task": "/separate_dnr"
          }
        }
      }
    ]
  },
  "audio_super_resolution_api": {
    "description": "Uses the Nick088/Audio-SR model for audio super-resolution.",
    "detailed_description": "This tool enhances the quality of an audio file by increasing its resolution. It uses a pre-trained model to generate a higher-quality version of the input audio. Supported models: 'basic', 'speech'. Input: Path to a local audio file (e.g., .wav, .mp3). Returns: The file path of the enhanced output audio.",
    "parameters": {
      "audio_file_path": {
        "type": "string",
        "description": "Required. The path to the input audio file to be enhanced.",
        "required": true,
        "example": "input/audio/low_quality.wav"
      },
      "output_path": {
        "type": "string",
        "description": "Required. The path to save the resulting enhanced audio file.",
        "required": true,
        "example": "output/enhanced_audio.wav"
      },
      "model_name": {
        "type": "string",
        "description": "The model to use. Options are 'basic' or 'speech'.",
        "required": "optional",
        "default": "basic",
        "enum": [
          "basic",
          "speech"
        ],
        "example": "speech"
      },
      "guidance_scale": {
        "type": "float",
        "description": "The scale used to guide the generation process.",
        "required": "optional",
        "default": 3.5,
        "example": 4.0
      },
      "ddim_steps": {
        "type": "integer",
        "description": "Number of DDIM diffusion model steps.",
        "required": "optional",
        "default": 50,
        "example": 75
      },
      "seed": {
        "type": "integer",
        "description": "Random seed for reproducing results.",
        "required": "optional",
        "default": 42,
        "example": 123
      }
    },
    "examples": [
      {
        "task_description": "Enhance a speech recording using the 'speech' model and custom settings.",
        "Act": {
          "tool": "audio_super_resolution_api",
          "arguments": {
            "audio_file_path": "input/conference_call.mp3",
            "output_path": "output/enhanced_conference_call.wav",
            "model_name": "speech",
            "guidance_scale": 4.0,
            "ddim_steps": 75,
            "seed": 123
          }
        }
      },
      {
        "task_description": "Enhance a general audio file using default optional parameters.",
        "Act": {
          "tool": "audio_super_resolution_api",
          "arguments": {
            "audio_file_path": "input/music_demo.wav",
            "output_path": "output/enhanced_music_demo.wav",
            "model_name": "basic"
          }
        }
      },
      {
        "task_description": "Upscale an audio file with only the required parameters specified.",
        "Act": {
          "tool": "audio_super_resolution_api",
          "arguments": {
            "audio_file_path": "path/to/my_audio.wav",
            "output_path": "path/to/my_enhanced_audio.wav"
          }
        }
      }
    ]
  },
  "index_tts_1_5_api": {
    "description": "A Text-to-Speech (TTS) tool that clones a voice from a reference audio to generate speech for the target text.",
    "detailed_description": "This tool provides Text-to-Speech (TTS) functionality by cloning a voice from a reference audio file to synthesize speech for a given text. It utilizes the '/gen_single' API endpoint. The process involves providing a path to a reference audio (e.g., .wav, .mp3) and the text to be converted. The tool then generates a new audio file with the voice from the reference audio speaking the provided text. The final path of the successfully generated audio file is returned.",
    "parameters": {
      "prompt_audio_path": {
        "type": "string",
        "description": "The file path to the reference audio for voice cloning. This is a required parameter.",
        "required": true,
        "example": "my_reference_voice.wav"
      },
      "target_text": {
        "type": "string",
        "description": "The target text to be converted into speech. This is a required parameter.",
        "required": true,
        "example": "Hello, welcome to this speech synthesis tool!"
      },
      "output_path": {
        "type": "string",
        "description": "The path to save the generated audio file. Defaults to 'generated_audio.wav'.",
        "required": false,
        "default": "generated_audio.wav",
        "example": "output/result.wav"
      }
    },
    "returns": {
      "type": "string",
      "description": "The final path of the successfully generated audio file, or None if the operation fails."
    },
    "examples": [
      {
        "task_description": "Generate speech by cloning a voice from a reference audio and saving it to a specific output path.",
        "Act": {
          "tool": "index_tts_api",
          "arguments": {
            "prompt_audio_path": "input/reference_voice.wav",
            "target_text": "Hello, this is a demonstration of voice cloning.",
            "output_path": "output/cloned_voice_audio.wav"
          }
        }
      },
      {
        "task_description": "Use a reference audio to generate speech for a target text, using the default output path.",
        "Act": {
          "tool": "index_tts_api",
          "arguments": {
            "prompt_audio_path": "samples/my_voice.mp3",
            "target_text": "This tool makes it easy to generate speech in any voice."
          }
        }
      }
    ]
  },
  "audiocraft_jasco_api": {
    "description": "Audiocraft Music Generation Tool",
    "detailed_description": "Calls the /predict_full API endpoint of Tonic/audiocraft to generate music based on text, chords, melody, and drums. Supported input types: text descriptions, chord progression strings, melody audio files, and drum audio files. Returns a tuple containing the local saved paths of the two generated audio files (Jasco Stem 1, Jasco Stem 2). Note: Melody and drum files are optional, but their use depends on the selected model. For example, models containing 'melody' in their name require melody_file_path.",
    "parameters": {
      "model": {
        "type": "string",
        "description": "The name of the model to use. Available options: 'facebook/jasco-chords-drums-400M', 'facebook/jasco-chords-drums-1B', 'facebook/jasco-chords-drums-melody-400M', 'facebook/jasco-chords-drums-melody-1B'.",
        "required": "optional",
        "default": "facebook/jasco-chords-drums-melody-400M",
        "enum": [
          "facebook/jasco-chords-drums-400M",
          "facebook/jasco-chords-drums-1B",
          "facebook/jasco-chords-drums-melody-400M",
          "facebook/jasco-chords-drums-melody-1B"
        ]
      },
      "text": {
        "type": "string",
        "description": "Text prompt describing the music style, instruments, etc.",
        "required": "optional",
        "default": "Strings, woodwind, orchestral, symphony."
      },
      "chords_sym": {
        "type": "string",
        "description": "Chord progression string in the format `(CHORD, START_TIME_IN_SECONDS)`.",
        "required": "optional",
        "default": "(C, 0.0), (D, 2.0), (F, 4.0), (Ab, 6.0), (Bb, 7.0), (C, 8.0)"
      },
      "melody_file_path": {
        "type": "string",
        "description": "Local path to the melody reference audio file. Required for models that use melody.",
        "required": "optional",
        "default": ""
      },
      "drums_file_path": {
        "type": "string",
        "description": "Local path to the drums reference audio file. Used when `drum_input_src` is 'file'.",
        "required": "optional",
        "default": ""
      },
      "drums_mic_path": {
        "type": "string",
        "description": "Local path to the drum audio file recorded via microphone. Used when `drum_input_src` is 'mic'.",
        "required": "optional",
        "default": ""
      },
      "drum_input_src": {
        "type": "string",
        "description": "The source of the drum input. Available options: 'file', 'mic'.",
        "required": "optional",
        "default": "file",
        "enum": [
          "file",
          "mic"
        ]
      },
      "cfg_coef_all": {
        "type": "number",
        "description": "Global coefficient for Classifier-Free Guidance (CFG).",
        "required": "optional",
        "default": 1.25
      },
      "cfg_coef_txt": {
        "type": "number",
        "description": "CFG coefficient for the text condition.",
        "required": "optional",
        "default": 2.5
      },
      "ode_rtol": {
        "type": "number",
        "description": "Relative tolerance for the ODE solver.",
        "required": "optional",
        "default": 0.0001
      },
      "ode_atol": {
        "type": "number",
        "description": "Absolute tolerance for the ODE solver.",
        "required": "optional",
        "default": 0.0001
      },
      "ode_solver": {
        "type": "string",
        "description": "Type of ODE solver. Available options: 'euler', 'dopri5'.",
        "required": "optional",
        "default": "euler",
        "enum": [
          "euler",
          "dopri5"
        ]
      },
      "ode_steps": {
        "type": "number",
        "description": "Number of steps for the 'euler' solver.",
        "required": "optional",
        "default": 10
      },
      "output_dir": {
        "type": "string",
        "description": "Directory path to save the generated audio files. Defaults to 'output_audio'.",
        "required": "optional",
        "default": "output_audio"
      }
    },
    "examples": [
      {
        "task_description": "Generate music using text and chords",
        "Act": {
          "tool": "audiocraft_jasco_api",
          "arguments": {
            "model": "facebook/jasco-chords-drums-400M",
            "text": "Acoustic folk song with a gentle guitar and a simple beat.",
            "chords_sym": "(G, 0.0), (C, 4.0), (G, 8.0), (D, 12.0)",
            "output_dir": "generated_music"
          }
        }
      },
      {
        "task_description": "Generate music with melody and drums from files",
        "Act": {
          "tool": "audiocraft_jasco_api",
          "arguments": {
            "model": "facebook/jasco-chords-drums-melody-400M",
            "text": "Upbeat pop track with a catchy synth melody.",
            "chords_sym": "(Am, 0.0), (F, 2.0), (C, 4.0), (G, 6.0)",
            "melody_file_path": "path/to/your/melody.wav",
            "drums_file_path": "path/to/your/drums.wav",
            "drum_input_src": "file",
            "output_dir": "pop_track"
          }
        }
      }
    ]
  },
  "step_audio_tts_3b_api": {
    "description": "Voice cloning tool: Use a reference audio to clone its timbre and generate new speech.",
    "detailed_description": "This tool performs voice cloning text-to-speech (TTS) by using a reference audio to clone its timbre and generate new speech from the provided text. It utilizes the /generate_clone API endpoint. The inputs required are the target text for synthesis, a reference audio file for timbre cloning, and the corresponding text of the reference audio. The tool returns the file path of the generated audio.",
    "parameters": {
      "text": {
        "type": "string",
        "description": "The target text to be converted into speech.",
        "required": true,
        "example": "Hello, welcome to use this voice cloning tool."
      },
      "prompt_audio": {
        "type": "string",
        "description": "Path to the reference audio file used for cloning the timbre. Supports common audio formats like WAV, MP3, etc.",
        "required": true,
        "example": "path/to/sample.wav"
      },
      "prompt_text": {
        "type": "string",
        "description": "The text content corresponding to the 'prompt_audio' reference file.",
        "required": true,
        "example": "This is the text of the reference audio."
      },
      "output_path": {
        "type": "string",
        "description": "Path to save the generated audio file.",
        "required": "optional",
        "default": "generated_clone_audio.wav",
        "example": "output/cloned_speech.wav"
      }
    },
    "examples": [
      {
        "task_description": "Clone a voice from a reference audio and text to generate new speech, saving to a specified output path.",
        "Act": {
          "tool": "VoiceCloneTTS",
          "arguments": {
            "text": "Hello, welcome to use this voice cloning tool.",
            "prompt_audio": "path/to/sample.wav",
            "prompt_text": "This is the text of the reference audio.",
            "output_path": "output/cloned_speech.wav"
          }
        }
      }
    ]
  },
  "sparkTTS_tool_api": {
    "description": "A tool for text-to-speech, supporting voice cloning and custom voice creation.",
    "detailed_description": "The SparkTTS tool provides text-to-speech functionality with two main tasks. The 'voice_clone' task allows cloning a voice from an audio sample to read the given text. The 'voice_creation' task generates a custom voice based on specified gender, pitch, and speed parameters to read the text. For the 'voice_clone' task, the 'text', 'prompt_text', and 'prompt_audio_path' are required. For the 'voice_creation' task, 'text' is required, while 'gender', 'pitch', and 'speed' have default values. The function returns the path to the generated audio file upon successful execution.",
    "parameters": {
      "task": {
        "type": "string",
        "description": "The task to be performed, either 'voice_clone' or 'voice_creation'.",
        "required": true,
        "enum": [
          "voice_clone",
          "voice_creation"
        ],
        "example": "voice_clone"
      },
      "text": {
        "type": "string",
        "description": "The input text to be converted to speech.",
        "required": true,
        "example": "This is the cloned voice."
      },
      "output_path": {
        "type": "string",
        "description": "The path to save the generated audio file.",
        "required": true,
        "example": "./clone_output.wav"
      },
      "prompt_text": {
        "type": "string",
        "description": "The text corresponding to the prompt audio for voice cloning. Required when task is 'voice_clone'.",
        "required": "optional",
        "example": "This is the prompt audio's text."
      },
      "prompt_audio_path": {
        "type": "string",
        "description": "The path to the prompt audio file (e.g., .wav) for voice cloning. A sample rate of at least 16kHz is recommended. Required when task is 'voice_clone'.",
        "required": "optional",
        "example": "./sample.wav"
      },
      "gender": {
        "type": "string",
        "description": "The gender of the generated voice, can be 'male' or 'female'. Used only when task is 'voice_creation'.",
        "required": "optional",
        "default": "male",
        "enum": [
          "male",
          "female"
        ],
        "example": "female"
      },
      "pitch": {
        "type": "float",
        "description": "The pitch of the generated voice. Used only when task is 'voice_creation'.",
        "required": "optional",
        "default": 3.0,
        "example": 4.0
      },
      "speed": {
        "type": "float",
        "description": "The speed of the generated voice. Used only when task is 'voice_creation'.",
        "required": "optional",
        "default": 3.0,
        "example": 2.0
      }
    },
    "examples": [
      {
        "task_description": "Clone a voice using a prompt audio and text",
        "Act": {
          "tool": "SparkTTS_tool",
          "arguments": {
            "task": "voice_clone",
            "text": "This is the cloned voice.",
            "prompt_text": "This is the prompt audio's text.",
            "prompt_audio_path": "./sample.wav",
            "output_path": "./clone_output.wav"
          }
        }
      },
      {
        "task_description": "Create a custom female voice with specific pitch and speed",
        "Act": {
          "tool": "SparkTTS_tool",
          "arguments": {
            "task": "voice_creation",
            "text": "This is a custom voice.",
            "gender": "female",
            "pitch": 4.0,
            "speed": 2.0,
            "output_path": "./creation_output.wav"
          }
        }
      }
    ]
  },
  "yue_api": {
    "description": "YuE Music Generation Tool.",
    "detailed_description": "Calls the 'innova-ai/YuE-music-generator-demo' API on Hugging Face Space to generate music. Supports generation by specifying music genre, lyrics, or by providing an audio file as a prompt. The API returns three audio files: a mixed final track, a vocals-only track, and an instrumental-only track.",
    "parameters": {
      "genre_txt": {
        "type": "string",
        "description": "Text description of the music genre, e.g., 'Pop' or 'Lyrical Folk'.",
        "required": "optional",
        "example": "Pop"
      },
      "lyrics_txt": {
        "type": "string",
        "description": "The lyrics text for the music.",
        "required": "optional",
        "example": "Verse 1: The sun shines bright."
      },
      "num_segments": {
        "type": "integer",
        "description": "The number of music segments to generate.",
        "required": "optional",
        "default": 2,
        "example": 4
      },
      "duration": {
        "type": "integer",
        "description": "The duration of the generated song in seconds.",
        "required": "optional",
        "default": 30,
        "example": 60
      },
      "use_audio_prompt": {
        "type": "boolean",
        "description": "Whether to use the provided audio file as a generation prompt.",
        "required": "optional",
        "default": false,
        "example": true
      },
      "audio_prompt_path": {
        "type": "string",
        "description": "Path (local or URL) to the audio file to be used as a prompt. This parameter is required if use_audio_prompt is True.",
        "required": "optional",
        "example": "input/audio/my_prompt.wav"
      },
      "output_dir": {
        "type": "string",
        "description": "Directory path to save the three final generated audio files.",
        "required": "optional",
        "default": "yue_music_output",
        "example": "output/my_generated_music"
      }
    },
    "examples": [
      {
        "task_description": "Generate a 60-second pop song with specified lyrics and save it to a custom directory.",
        "Act": {
          "tool": "yue_api",
          "arguments": {
            "genre_txt": "Pop",
            "lyrics_txt": "Oh, dancing in the moonlight, feeling so free tonight.",
            "duration": 60,
            "output_dir": "output/pop_song_project"
          }
        }
      },
      {
        "task_description": "Generate new music based on an existing audio file as a prompt.",
        "Act": {
          "tool": "yue_api",
          "arguments": {
            "use_audio_prompt": true,
            "audio_prompt_path": "input/prompts/melody_idea.wav",
            "num_segments": 4,
            "duration": 45
          }
        }
      },
      {
        "task_description": "Generate a short instrumental piece by only specifying the genre.",
        "Act": {
          "tool": "yue_api",
          "arguments": {
            "genre_txt": "Lofi hip hop",
            "duration": 30
          }
        }
      }
    ]
  },
  "voicecraft_tts_and_edit_api": {
    "description": "Performs text-to-speech (TTS), audio editing, and long-form text synthesis using the VoiceCraft model.",
    "detailed_description": "Provides three modes of operation: 'TTS' for generating speech from text (with optional voice cloning from a reference audio), 'Edit' for replacing a segment of an audio file with new speech, and 'Long TTS' for synthesizing long-form text by splitting it into sentences and generating them sequentially. The tool requires 'mode' and 'transcript' for all operations. An 'audio_path' is necessary for 'Edit' and 'Long TTS' modes, and also optional for voice cloning in 'TTS' mode. For 'Long TTS', the 'selected_sentence' to be synthesized is also required. It returns a dictionary containing the path to the generated audio file and the inferred transcript.",
    "parameters": {
      "mode": {
        "type": "string",
        "description": "The operating mode. 'TTS' for text-to-speech, 'Edit' for audio editing, 'Long TTS' for long text synthesis.",
        "required": true,
        "enum": [
          "TTS",
          "Edit",
          "Long TTS"
        ],
        "example": "TTS"
      },
      "transcript": {
        "type": "string",
        "description": "The text to be synthesized or used for editing.",
        "required": true,
        "example": "The quick brown fox jumps over the lazy dog."
      },
      "audio_path": {
        "type": "string",
        "description": "Path to the input audio file. Required for 'Edit' and 'Long TTS' modes. Optional for 'TTS' mode to enable voice cloning.",
        "required": false,
        "example": "input/original_audio.wav"
      },
      "output_path": {
        "type": "string",
        "description": "Path to save the generated audio file.",
        "required": false,
        "default": "output.wav",
        "example": "output/generated_speech.wav"
      },
      "seed": {
        "type": "number",
        "description": "Random seed for reproducibility. Use -1.0 for a random seed.",
        "required": false,
        "default": -1.0,
        "example": 42
      },
      "smart_transcript": {
        "type": "boolean",
        "description": "Whether to enable smart transcription.",
        "required": false,
        "default": true,
        "example": true
      },
      "prompt_end_time": {
        "type": "number",
        "description": "In 'Edit' mode, the end time of the audio prompt.",
        "required": false,
        "default": 3.675,
        "example": 3.5
      },
      "edit_start_time": {
        "type": "number",
        "description": "In 'Edit' mode, the start time of the segment to be edited.",
        "required": false,
        "default": 3.83,
        "example": 4.0
      },
      "edit_end_time": {
        "type": "number",
        "description": "In 'Edit' mode, the end time of the segment to be edited.",
        "required": false,
        "default": 5.113,
        "example": 5.0
      },
      "left_margin": {
        "type": "number",
        "description": "Left margin for the audio processing.",
        "required": false,
        "default": 0.08,
        "example": 0.1
      },
      "right_margin": {
        "type": "number",
        "description": "Right margin for the audio processing.",
        "required": false,
        "default": 0.08,
        "example": 0.1
      },
      "temperature": {
        "type": "number",
        "description": "Controls the randomness of the generation. Higher values mean more randomness.",
        "required": false,
        "default": 1.0,
        "example": 0.95
      },
      "top_p": {
        "type": "number",
        "description": "Nucleus sampling threshold.",
        "required": false,
        "default": 0.9,
        "example": 0.9
      },
      "top_k": {
        "type": "number",
        "description": "Top-k sampling. Set to 0 to disable.",
        "required": false,
        "default": 0.0,
        "example": 5
      },
      "sample_batch_size": {
        "type": "number",
        "description": "Sample batch size, which can affect the speech rate.",
        "required": false,
        "default": 2.0,
        "example": 2
      },
      "stop_repetition": {
        "type": "integer",
        "description": "Level for stopping repetition. Must be a number.",
        "required": false,
        "default": 3,
        "enum": [
          -1,
          1,
          2,
          3,
          4
        ],
        "example": 3
      },
      "kvcache": {
        "type": "integer",
        "description": "Whether to use KV caching. 1 for yes, 0 for no. Must be a number.",
        "required": false,
        "default": 1,
        "enum": [
          0,
          1
        ],
        "example": 1
      },
      "split_text": {
        "type": "string",
        "description": "In 'Long TTS' mode, how to split the text.",
        "required": false,
        "default": "Newline",
        "enum": [
          "Newline",
          "Sentence"
        ],
        "example": "Sentence"
      },
      "selected_sentence": {
        "type": [
          "string",
          "null"
        ],
        "description": "In 'Long TTS' mode, the specific sentence to process. Required for this mode. Should be omitted or null for 'TTS' and 'Edit' modes.",
        "required": false,
        "default": null,
        "example": "This is the sentence to be synthesized."
      },
      "codec_audio_sr": {
        "type": "number",
        "description": "Codec audio sample rate.",
        "required": false,
        "default": 16000.0,
        "example": 16000
      },
      "codec_sr": {
        "type": "number",
        "description": "Codec sample rate.",
        "required": false,
        "default": 50.0,
        "example": 50
      },
      "silence_tokens": {
        "type": "string",
        "description": "A string representing the list of silence tokens.",
        "required": false,
        "default": "[1388,1898,131]",
        "example": "[1388,1898,131]"
      }
    },
    "examples": [
      {
        "task_description": "Generate speech from a piece of text using the TTS mode, cloning the voice from a provided audio file.",
        "Act": {
          "tool": "VoiceCraftTool",
          "arguments": {
            "mode": "TTS",
            "transcript": "Hello world, this is a demonstration of text-to-speech with voice cloning.",
            "audio_path": "input/my_voice.wav",
            "output_path": "output/hello_world_cloned.wav"
          }
        }
      },
      {
        "task_description": "Edit a specific portion of an existing audio file.",
        "Act": {
          "tool": "VoiceCraftTool",
          "arguments": {
            "mode": "Edit",
            "transcript": "The quick brown fox jumps over the lazy cat.",
            "audio_path": "input/original_speech.wav",
            "edit_start_time": 3.83,
            "edit_end_time": 5.113,
            "output_path": "output/edited_speech.wav"
          }
        }
      },
      {
        "task_description": "Generate a single sentence as part of a long-form text synthesis.",
        "Act": {
          "tool": "VoiceCraftTool",
          "arguments": {
            "mode": "Long TTS",
            "transcript": "This is the first sentence. This is the second sentence which we will generate now.",
            "audio_path": "input/long_tts_prompt.wav",
            "output_path": "output/long_tts_sentence.wav",
            "split_text": "Sentence",
            "selected_sentence": "This is the second sentence which we will generate now."
          }
        }
      }
    ]
  },
  "image2music_api": {
    "description": "Generates music from a local image file or a web image URL using the image-to-music-v2 model.",
    "detailed_description": "This tool creates a musical piece based on a provided image. The image can be a local file path or a URL. The user can specify the output directory and filename for the generated MP3 file. If not provided, a filename is automatically generated based on the input image name and the current timestamp, and it's saved in the 'outputs/music' directory. The tool supports several underlying models for music generation, with 'Riffusion' being the default, which is generally faster for testing.",
    "parameters": {
      "image_source": {
        "type": "string",
        "description": "The source of your image. This can be a local file path (e.g., \"C:/images/photo.png\") or a URL to an image on the web (e.g., \"https://example.com/image.jpg\"). Required.",
        "required": true,
        "example": "https://example.com/image.jpg"
      },
      "output_dir": {
        "type": "string",
        "description": "The folder where you want to save the generated music file. If not specified, it defaults to the 'outputs/music' folder in the program directory.",
        "required": "optional",
        "default": "outputs/music",
        "example": "my_generated_music"
      },
      "output_filename": {
        "type": "string",
        "description": "The desired name for the output music file (e.g., \"my_song.mp3\"). If omitted, a name will be automatically generated from the image name and a timestamp.",
        "required": "optional",
        "example": "my_composition.mp3"
      },
      "model": {
        "type": "string",
        "description": "The model to use for music generation. 'Riffusion' is generally faster and suitable for quick tests.",
        "required": "optional",
        "default": "Riffusion",
        "enum": [
          "ACE Step",
          "AudioLDM-2",
          "Riffusion",
          "Mustango",
          "Stable Audio Open"
        ],
        "example": "ACE Step"
      },
      "hf_token": {
        "type": "string",
        "description": "Your Hugging Face access token, required if you need to use a private model.",
        "required": "optional",
        "example": "hf_xxxxxxxxxxxxxxxxxxxxxx"
      }
    },
    "examples": [
      {
        "task_description": "Generate music from a local image file and save it to the default directory.",
        "Act": {
          "tool": "image2music_api",
          "arguments": {
            "image_source": "C:/Users/Admin/Pictures/sunset.png"
          }
        }
      },
      {
        "task_description": "Generate music from an image URL, select the 'AudioLDM-2' model, and specify a custom output path and filename.",
        "Act": {
          "tool": "image2music_api",
          "arguments": {
            "image_source": "https://www.example.com/images/mountain_view.jpg",
            "output_dir": "D:/my_music/landscapes",
            "output_filename": "mountain_sonata.mp3",
            "model": "AudioLDM-2"
          }
        }
      },
      {
        "task_description": "Create a piece of music from a web image using the 'Stable Audio Open' model, allowing the system to auto-generate the filename.",
        "Act": {
          "tool": "image2music_api",
          "arguments": {
            "image_source": "https://anothersite.net/art/abstract.gif",
            "model": "Stable Audio Open",
            "output_dir": "outputs/abstract_tunes"
          }
        }
      }
    ]
  }
}




import shutil
from gradio_client import Client, file,handle_file
from mcp.server.fastmcp import FastMCP
from typing import Optional, Dict, Any, List, Union, Literal,Tuple
import os


# åˆå§‹åŒ–MCPæœåŠ¡å™¨
mcp = FastMCP("APITool")
#set yours hf_token=
MD_TOKEN=None
HF_TOKEN = None
'''
This is a wrapper function template for an API tool. If you want to update the API tool yourself, please refer to this example and update your API tool. You can refer to the implementations of the multiple tools below.

@mcp.tool()
def MyAPI_tool_api(
    # 1. Parameter area: corresponds one-to-one with API endpoint parameters, supports default values and type annotations
    param1: str,
    param2: int = 0,
    input_file_path: str = "",
    output_path: str = "output.txt"
):
    """
    Brief description of MyAPI tool functions (such as text-to-speech/audio enhancement/multimodal dialogue, etc.)


    Detailed Description:
    - Supported API endpoints/tasks
    - Supported input types, ranges, and required fields
    - Return value description
    - Example usage

    Args:
	param1 (str): Describe the function of parameter 1, its value range, and whether it is required.
	param2 (int): Describe the function of parameter 2, its default value, and range.
	input_file_path (str): Input file path. Describe the format, sampling rate, and other requirements.
	output_path (str): Path to save the result. Default is "output.txt".

    Returns:
        str: Result file path or text content
    """

    # 2. Parameter Verification (Recommended)

    if not param1:
        raise ValueError("param1 is Required parameter")
    if input_file_path and not os.path.exists(input_file_path):
        raise FileNotFoundError(input_file_path)

    # 3. Instantiate the API client
    client = Client("https://your-api-endpoint-url/", hf_token=MD_TOKEN) #This is the usage path method of the modelscope API.
    # or client("Zeyue7/AudioX", hf_token=HF_TOKEN)  This is the usage path method of the modelscope API.
    # 4. File parameter processing (if any)
    file_input = file(input_file_path) if input_file_path else None

    # 5. CALL API
    result = client.predict(
        param1=param1,
        param2=param2,
        input_file=file_input,
        api_name="/your_api_endpoint"
    )

    # 6. Save the results (if any)
    if output_path:
        os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)
        # Suppose "result" refers to a text.
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(result)
        return output_path
    else:
        return result

'''

@mcp.tool()
def cosyvoice2tool_api(
	tts_text: str = "",
	mode: str = "3sæé€Ÿå¤åˆ»",
	prompt_text: str = "",
	prompt_wav_path: str = "",
	instruct_text: str = "",
	seed: float = 0,
	output_path: str = "output.wav"
):
	"""CosyVoice2è¯­éŸ³åˆæˆå·¥å…·

	ä½¿ç”¨CosyVoice2æ¨¡å‹å°†æ–‡æœ¬è½¬æ¢ä¸ºé€¼çœŸçš„è¯­éŸ³ã€‚æ‚¨å¯ä»¥æä¾›å‚è€ƒéŸ³é¢‘ä½œä¸ºå£°éŸ³é£æ ¼çš„promptï¼Œ
	ä¹Ÿå¯ä»¥ä½¿ç”¨æ–‡æœ¬æŒ‡ä»¤æ§åˆ¶ç”Ÿæˆæ•ˆæœã€‚

	Args:
		tts_text: éœ€è¦åˆæˆä¸ºè¯­éŸ³çš„æ–‡æœ¬å†…å®¹
		mode: æ¨ç†æ¨¡å¼ï¼Œå¯ä»Literal["cosy", "instruct"]ä¸­é€‰æ‹©"
		prompt_text: promptæ–‡æœ¬ï¼Œä»…åœ¨æŸäº›æ¨¡å¼ä¸‹ä½¿ç”¨
		prompt_wav_path: å‚è€ƒéŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼Œæä¾›å£°éŸ³é£æ ¼æ ·æœ¬
		instruct_text: æŒ‡ä»¤æ–‡æœ¬ï¼Œç”¨äºæ§åˆ¶ç”Ÿæˆæ•ˆæœ
		seed: éšæœºæ¨ç†ç§å­ï¼Œæ§åˆ¶ç”Ÿæˆç»“æœçš„éšæœºæ€§
		output_path: ç”ŸæˆéŸ³é¢‘çš„ä¿å­˜è·¯å¾„

	Returns:
		str: ç”ŸæˆéŸ³é¢‘çš„æ–‡ä»¶è·¯å¾„
	"""

	if mode == "cosy":
		mode_checkbox_group = "3sæé€Ÿå¤åˆ»"
	elif mode == "instruct":
		mode_checkbox_group = "è‡ªç„¶è¯­è¨€æ§åˆ¶"
	else:
		return "Unsupported mode !"

	# å‡†å¤‡prompt_wavå‚æ•°
	if prompt_wav_path:
		prompt_wav = file(prompt_wav_path)
	else:
		# ä½¿ç”¨é»˜è®¤æ ·æœ¬
		prompt_wav = file('https://github.com/gradio-app/gradio/raw/main/test/test_files/audio_sample.wav')
	
	# è°ƒç”¨APIç”ŸæˆéŸ³é¢‘
	client = Client("https://iic-cosyvoice2-0-5b.ms.show/",hf_token=MD_TOKEN)
	
	result = client.predict(
		tts_text=tts_text,
		mode_checkbox_group=mode_checkbox_group,
		prompt_text=prompt_text,
		prompt_wav_upload=prompt_wav,
		prompt_wav_record=prompt_wav,  # ä½¿ç”¨ç›¸åŒçš„éŸ³é¢‘æ–‡ä»¶
		instruct_text=instruct_text,
		seed=seed,
		stream="false",
		api_name="/generate_audio"
	)
	
	# å¤„ç†è¿”å›çš„éŸ³é¢‘æ–‡ä»¶
	import shutil
	import os
	
	# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
	os.makedirs(os.path.dirname(os.path.abspath(output_path)), exist_ok=True)
	
	# å¤åˆ¶ç”Ÿæˆçš„éŸ³é¢‘åˆ°æŒ‡å®šè·¯å¾„
	shutil.copy(result, output_path)
	
	return output_path


@mcp.tool()
def AudioX_api(
	prompt: str = "",
	negative_prompt: str = None,
	video_file_path: str = None,
	audio_prompt_file_path: str = None,
	audio_prompt_path: str = None,
	seconds_start: float = 0,
	seconds_total: float = 10,
	cfg_scale: float = 7,
	steps: float = 100,
	preview_every: float = 0,
	seed: str = "-1",
	sampler_type: str = "dpmpp-3m-sde",
	sigma_min: float = 0.03,
	sigma_max: float = 500,
	cfg_rescale: float = 0,
	use_init: bool = False,
	init_audio_path: str = None,
	init_noise_level: float = 0.1,
	output_audio_path: str = "output_audio.wav",
	output_video_path: str = "output_video.mp4"
):
	"""AudioXéŸ³é¢‘ç”Ÿæˆå·¥å…·

	ä½¿ç”¨AudioXæ¨¡å‹æ ¹æ®æ–‡æœ¬æç¤ºã€è§†é¢‘æˆ–éŸ³é¢‘æç¤ºç”Ÿæˆé«˜è´¨é‡çš„éŸ³é¢‘ã€‚

	Args:
		prompt: æ–‡æœ¬æç¤ºï¼Œæè¿°è¦ç”Ÿæˆçš„éŸ³é¢‘å†…å®¹
		negative_prompt: è´Ÿé¢æç¤ºï¼Œæè¿°ä¸å¸Œæœ›åœ¨ç”Ÿæˆç»“æœä¸­å‡ºç°çš„ç‰¹å¾
		video_file_path: è§†é¢‘æ–‡ä»¶è·¯å¾„ï¼Œç”¨ä½œç”Ÿæˆå‚è€ƒ
		audio_prompt_file_path: éŸ³é¢‘æç¤ºæ–‡ä»¶è·¯å¾„ï¼Œç”¨ä½œç”Ÿæˆå‚è€ƒ
		audio_prompt_path: éŸ³é¢‘æç¤ºè·¯å¾„ï¼Œç”¨ä½œç”Ÿæˆå‚è€ƒ
		seconds_start: è§†é¢‘èµ·å§‹ç§’æ•°
		seconds_total: ç”ŸæˆéŸ³é¢‘çš„æ€»æ—¶é•¿(ç§’)
		cfg_scale: CFGç¼©æ”¾å‚æ•°ï¼Œæ§åˆ¶ç”Ÿæˆè¿‡ç¨‹ä¸­å¯¹æç¤ºçš„éµå¾ªç¨‹åº¦
		steps: é‡‡æ ·æ­¥æ•°ï¼Œå½±å“ç”Ÿæˆè´¨é‡å’Œæ—¶é—´
		preview_every: é¢„è§ˆé¢‘ç‡è®¾ç½®
		seed: éšæœºç§å­ï¼Œè®¾ç½®ä¸º-1è¡¨ç¤ºéšæœºç§å­
		sampler_type: é‡‡æ ·å™¨ç±»å‹ï¼Œå¯é€‰å€¼åŒ…æ‹¬'dpmpp-2m-sde', 'dpmpp-3m-sde'ç­‰
		sigma_min: æœ€å°sigmaå€¼
		sigma_max: æœ€å¤§sigmaå€¼
		cfg_rescale: CFGé‡æ–°ç¼©æ”¾é‡
		use_init: æ˜¯å¦ä½¿ç”¨åˆå§‹éŸ³é¢‘
		init_audio_path: åˆå§‹éŸ³é¢‘æ–‡ä»¶è·¯å¾„
		init_noise_level: åˆå§‹å™ªå£°çº§åˆ«
		output_audio_path: è¾“å‡ºéŸ³é¢‘æ–‡ä»¶è·¯å¾„
		output_video_path: è¾“å‡ºè§†é¢‘æ–‡ä»¶è·¯å¾„

	Returns:
		dict: åŒ…å«è¾“å‡ºéŸ³é¢‘è·¯å¾„å’Œè§†é¢‘è·¯å¾„çš„å­—å…¸
	"""
	import os
	import shutil
	from gradio_client import Client, file

	client = Client("Zeyue7/AudioX", hf_token=HF_TOKEN)
	
	# å¤„ç†æ–‡ä»¶è¾“å…¥
	video_file_input = file(video_file_path) if video_file_path else None
	audio_prompt_file_input = file(audio_prompt_file_path) if audio_prompt_file_path else None
	init_audio_input = file(init_audio_path) if init_audio_path and use_init else None
	
	# è°ƒç”¨APIç”ŸæˆéŸ³é¢‘å’Œè§†é¢‘
	result = client.predict(
		prompt=prompt,
		negative_prompt=negative_prompt,
		video_file=video_file_input,
		audio_prompt_file=audio_prompt_file_input,
		audio_prompt_path=audio_prompt_path,
		seconds_start=seconds_start,
		seconds_total=seconds_total,
		cfg_scale=cfg_scale,
		steps=steps,
		preview_every=preview_every,
		seed=seed,
		sampler_type=sampler_type,
		sigma_min=sigma_min,
		sigma_max=sigma_max,
		cfg_rescale=cfg_rescale,
		use_init=use_init,
		init_audio=init_audio_input,
		init_noise_level=init_noise_level,
		api_name="/generate_cond"
	)
	
	# ç»“æœåŒ…å«è§†é¢‘å’ŒéŸ³é¢‘æ–‡ä»¶
	result_video = result[0]['video']
	result_audio = result[1]
	
	# ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
	os.makedirs(os.path.dirname(os.path.abspath(output_audio_path)), exist_ok=True)
	os.makedirs(os.path.dirname(os.path.abspath(output_video_path)), exist_ok=True)
	
	# å¤åˆ¶ç”Ÿæˆçš„æ–‡ä»¶åˆ°æŒ‡å®šè·¯å¾„
	shutil.copy(result_audio, output_audio_path)
	shutil.copy(result_video, output_video_path)
	
	return {
		"audio_path": output_audio_path,
		"video_path": output_video_path
	}


@mcp.tool()
def Qwen2audio_api(
	prompt: str = "",
	audio_file_path: str = None,
	chatbot_history: list = None,
	action: str = "chat",
	save_history: bool = True,
	output_file: str = "conversation_history.json"
):
	"""Qwen2-Audio-7B-Instructå¤šæ¨¡æ€å¯¹è¯å·¥å…·

	ä½¿ç”¨Qwen2-Audio-7B-Instructæ¨¡å‹è¿›è¡Œæ–‡æœ¬å’ŒéŸ³é¢‘çš„å¤šæ¨¡æ€å¯¹è¯ã€‚
	å¯ä»¥å‘é€æ–‡æœ¬æˆ–éŸ³é¢‘è¿›è¡Œå¯¹è¯ï¼Œä¹Ÿå¯ä»¥ç®¡ç†å¯¹è¯å†å²ã€‚

	Args:
		prompt: æ–‡æœ¬æç¤ºï¼Œç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬å†…å®¹
		audio_file_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼Œç”¨æˆ·è¾“å…¥çš„éŸ³é¢‘å†…å®¹
		chatbot_history: èŠå¤©å†å²è®°å½•ï¼Œæ ¼å¼ä¸ºåˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneåˆ™åˆ›å»ºæ–°å¯¹è¯
		action: æ“ä½œç±»å‹ï¼Œå¯é€‰å€¼ä¸º"chat"(å¯¹è¯)ã€"regenerate"(é‡æ–°ç”Ÿæˆ)ã€"reset"(é‡ç½®å¯¹è¯)
		save_history: æ˜¯å¦ä¿å­˜å¯¹è¯å†å²åˆ°æ–‡ä»¶
		output_file: å¯¹è¯å†å²ä¿å­˜çš„æ–‡ä»¶è·¯å¾„

	Returns:
		dict: åŒ…å«å¯¹è¯ç»“æœå’Œæ›´æ–°åçš„å†å²è®°å½•
	"""
	import json
	import os
	from gradio_client import Client, file

	client = Client("https://qwen-qwen2-audio-instruct-demo.ms.show/",hf_token=MD_TOKEN)
	
	# åˆå§‹åŒ–èŠå¤©å†å²
	if chatbot_history is None:
		chatbot_history = []
	
	result = None
	
	# æ ¹æ®ä¸åŒæ“ä½œç±»å‹å¤„ç†
	if action == "reset":
		# é‡ç½®å¯¹è¯
		result = client.predict(api_name="/reset_state")
		chatbot_history = []
	
	elif action == "regenerate" and chatbot_history:
		# é‡æ–°ç”Ÿæˆæœ€åä¸€æ¬¡å›å¤
		result = client.predict(
			chatbot=chatbot_history,
			api_name="/regenerate"
		)
		# æ›´æ–°å†å²è®°å½•
		if result:
			chatbot_history = result
	
	elif action == "chat":
		# å‡†å¤‡è¾“å…¥æ•°æ®
		input_data = {"files": [], "text": prompt}
		
		# å¦‚æœæä¾›äº†éŸ³é¢‘æ–‡ä»¶ï¼Œæ·»åŠ åˆ°è¾“å…¥ä¸­
		if audio_file_path:
			input_data["files"] = [file(audio_file_path)]
		
		# å‘é€æ¶ˆæ¯
		result = client.predict(
			chatbot=chatbot_history,
			input=input_data,
			api_name="/add_text"
		)
		
		# è·å–æ¨¡å‹å›å¤
		if result:
			# æ›´æ–°å¯¹è¯å†å²
			chatbot_history = result[0]
	
	# ä¿å­˜å¯¹è¯å†å²åˆ°æ–‡ä»¶
	if save_history and chatbot_history:
		os.makedirs(os.path.dirname(os.path.abspath(output_file)), exist_ok=True)
		with open(output_file, 'w', encoding='utf-8') as f:
			json.dump(chatbot_history, f, ensure_ascii=False, indent=2)
	
	return {
		"response": result,
		"history": chatbot_history
	}


@mcp.tool()
def clearervoice_api(
	# é€šç”¨å‚æ•°
	task: str = "enhancement",  # å¯é€‰: enhancement, separation, super_resolution, av_extraction
	input_path: str = "",
	# ä»…åœ¨enhancementä»»åŠ¡æ—¶ä½¿ç”¨
	model: str = "MossFormer2_48000Hz",
	# ä»…åœ¨super_resolutionä»»åŠ¡æ—¶ä½¿ç”¨
	apply_se: bool = True,
	# è‡ªå®šä¹‰è¾“å‡º
	output_audio_path: str = "output.wav",
	output_audio_path2: str = "output_2.wav",
	output_dir: str = "av_outputs"
):
	"""ClearerVoice å¤šä»»åŠ¡éŸ³é¢‘å¤„ç†å·¥å…·

	è¯¥å·¥å…·å°è£…äº† ClearerVoice Studio å¹³å°çš„å››é¡¹æ ¸å¿ƒèƒ½åŠ›ï¼Œå¯é€šè¿‡ `task` å‚æ•°é€‰æ‹©ä¸åŒä»»åŠ¡å¹¶è‡ªåŠ¨
	è°ƒç”¨å¯¹åº”çš„ API Endpointã€‚

	æ”¯æŒçš„ä»»åŠ¡åŠå…¶è¯¦ç»†è¯´æ˜:
	1. speech enhancement  (task="enhancement")
	   - ç«¯ç‚¹: /predict
	   - ä½œç”¨: é™å™ªä¸è¯­éŸ³å¢å¼º
	   - å¯é€‰æ¨¡å‹(model):
	       â€¢ FRCRN_16000Hz        (ä½ç®—åŠ›å®æ—¶å¢å¼º, 16kHz)
	       â€¢ MossFormerGAN_16000Hz (é«˜çº§ GAN å¢å¼º, 16kHz)
	       â€¢ MossFormer2_48000Hz   (æ——èˆ°å¢å¼º, 48kHz, é»˜è®¤)

	2. speech separation   (task="separation")
	   - ç«¯ç‚¹: /predict_1
	   - ä½œç”¨: å£°é“/è¯´è¯äººåˆ†ç¦»ï¼Œè¿”å›ä¸¤è·¯éŸ³é¢‘ (å¦‚äººå£°/èƒŒæ™¯)

	3. speech super resolution (task="super_resolution")
	   - ç«¯ç‚¹: /predict_2
	   - ä½œç”¨: å°†ä½é‡‡æ ·ç‡è¯­éŸ³æå‡è‡³é«˜é‡‡æ ·ç‡ï¼Œå¯é€‰æ‹©æ˜¯å¦å åŠ  Speech Enhancement
	   - å‚æ•°: apply_se (bool) â€” æ˜¯å¦å åŠ å¢å¼º (é»˜è®¤ True)

	4. audio-visual speaker extraction (task="av_extraction")
	   - ç«¯ç‚¹: /predict_3
	   - ä½œç”¨: åŸºäºè¾“å…¥è§†é¢‘æå–è¯´è¯äººè¯­éŸ³/è§†é¢‘ç‰‡æ®µï¼Œè¿”å›åŒ…å«å›¾ç‰‡æˆ–è§†é¢‘çš„åˆ—è¡¨

	é€šç”¨å‚æ•°:
	    input_path: è¾“å…¥éŸ³/è§†é¢‘è·¯å¾„ (wav / mp4 ç­‰)
	    output_audio_path: è¾“å‡ºéŸ³é¢‘è·¯å¾„ (å•è¾“å‡ºä»»åŠ¡)
	    output_audio_path2: è¾“å‡ºéŸ³é¢‘è·¯å¾„ 2 (åˆ†ç¦»ä»»åŠ¡ç¬¬äºŒè·¯)
	    output_dir: A/V ä»»åŠ¡çš„è¾“å‡ºç›®å½•

	è¿”å›:
	    å¯¹åº”ä»»åŠ¡çš„ç»“æœæ–‡ä»¶è·¯å¾„æˆ–è·¯å¾„åˆ—è¡¨
	"""
	import os
	import shutil
	from gradio_client import Client, handle_file

	client = Client("https://iic-clearervoice-studio.ms.show/",hf_token=MD_TOKEN)

	if task == "enhancement":
		if not input_path:
			raise ValueError("enhancement ä»»åŠ¡éœ€è¦æä¾› input_path (wav)")
		result = client.predict(
			input_wav=handle_file(input_path),
			model=model,
			api_name="/predict"
		)
		os.makedirs(os.path.dirname(os.path.abspath(output_audio_path)), exist_ok=True)
		shutil.copy(result, output_audio_path)
		return output_audio_path

	elif task == "separation":
		if not input_path:
			raise ValueError("separation ä»»åŠ¡éœ€è¦æä¾› input_path (wav)")
		result = client.predict(
			input_wav=handle_file(input_path),
			api_name="/predict_1"
		)
		track1, track2 = result
		os.makedirs(os.path.dirname(os.path.abspath(output_audio_path)), exist_ok=True)
		os.makedirs(os.path.dirname(os.path.abspath(output_audio_path2)), exist_ok=True)
		shutil.copy(track1, output_audio_path)
		shutil.copy(track2, output_audio_path2)
		return {"track1": output_audio_path, "track2": output_audio_path2}

	elif task == "super_resolution":
		if not input_path:
			raise ValueError("super_resolution ä»»åŠ¡éœ€è¦æä¾› input_path (wav)")
		result = client.predict(
			input_wav=handle_file(input_path),
			apply_se=apply_se,
			api_name="/predict_2"
		)
		os.makedirs(os.path.dirname(os.path.abspath(output_audio_path)), exist_ok=True)
		shutil.copy(result, output_audio_path)
		return output_audio_path

	elif task == "av_extraction":
		if not input_path:
			raise ValueError("av_extraction ä»»åŠ¡éœ€è¦æä¾› input_path (video)")
		result = client.predict(
			input_video={"video": handle_file(input_path)},
			api_name="/predict_3"
		)
		# result ä¸ºåŒ…å«å›¾åƒ/è§†é¢‘å­—å…¸çš„åˆ—è¡¨
		os.makedirs(output_dir, exist_ok=True)
		saved_files = []
		for idx, item in enumerate(result):
			# item å¯èƒ½åŒ…å« image æˆ– video é”®
			for key in ("image", "video"):
				if key in item and item[key]:
					src = item[key]
					filename = f"{key}_{idx}{os.path.splitext(src)[-1]}"
					dst = os.path.join(output_dir, filename)
					shutil.copy(src, dst)
					saved_files.append(dst)
		return saved_files

	else:
		raise ValueError("æœªçŸ¥ä»»åŠ¡ç±»å‹: " + task)


@mcp.tool()
def diffrhythm_api(
	task: str = "infer_music",  # å¯é€‰: prompt_type, theme_tags, lyrics_lrc, lambda_val, infer_music
	# theme/tags ç”Ÿæˆå‚æ•°
	theme: str = "",
	tags_gen: str = "",
	language: str = "en",
	# æ­Œè¯è½¬LRCå‚æ•°
	tags_lyrics: str = "",
	lyrics_input: str = "",
	# éŸ³ä¹ç”Ÿæˆå‚æ•°
	lrc: str = "",
	ref_audio_path: str = "",
	text_prompt: str = "",
	seed: float = 0,
	randomize_seed: bool = True,
	steps: float = 32,
	cfg_strength: float = 4,
	file_type: str = "mp3",
	odeint_method: str = "euler",
	preference_infer: str = "quality first",
	edit: bool = False,
	edit_segments: str = None,
	# è¾“å‡ºè·¯å¾„
	output_music_path: str = "diff_music_output.mp3"
):
	"""DiffRhythm éŸ³ä¹ç”Ÿæˆä¸è¾…åŠ©å·¥å…·

	è¯¥å·¥å…·å°è£…äº† DiffRhythm (ASLP-lab/DiffRhythm) Web å¹³å°çš„å¤šä¸ªç«¯ç‚¹ï¼Œæä¾›ä»ä¸»é¢˜/æ ‡ç­¾ç”Ÿæˆã€
	æ­Œè¯å¯¹é½åˆ°æœ€ç»ˆä¼´å¥éŸ³ä¹ç”Ÿæˆçš„ä¸€ç«™å¼èƒ½åŠ›ã€‚

	ä»»åŠ¡(task)åˆ—è¡¨:
	1. prompt_type (task="prompt_type")
	   - ç«¯ç‚¹: /update_prompt_type
	   - åŠŸèƒ½: æ›´æ–°æˆ–æŸ¥è¯¢å½“å‰å¯ç”¨çš„ Prompt ç±»å‹æšä¸¾ï¼Œé›¶å‚æ•°ã€‚

	2. theme_tags  (task="theme_tags")
	   - ç«¯ç‚¹: /R1_infer1
	   - åŠŸèƒ½: æ ¹æ®ä¸»é¢˜(theme)ä¸æ ‡ç­¾(tags_gen)ç”Ÿæˆå¸¦æ—¶é—´è½´çš„ LRC ç‰‡æ®µã€‚
	   - é‡è¦å‚æ•°:
	       â€¢ theme (str): ä¸»é¢˜æè¿°
	       â€¢ tags_gen (str): ç”Ÿæˆæ ‡ç­¾ (é€—å·åˆ†éš”)
	       â€¢ language (str): é€‰æ‹© 'cn' æˆ– 'en'

	3. lyrics_lrc (task="lyrics_lrc")
	   - ç«¯ç‚¹: /R1_infer2
	   - åŠŸèƒ½: å°†åŸå§‹æ­Œè¯(lyrics_input)ä¸æ ‡ç­¾(tags_lyrics)è½¬æ¢ä¸ºå¯¹é½åçš„ LRC æ ¼å¼ã€‚

	4. lambda_val (task="lambda_val")
	   - ç«¯ç‚¹: /lambda
	   - åŠŸèƒ½: æŸ¥è¯¢/æ›´æ–°å†…éƒ¨ Î» å‚æ•° (å¹³å°ä¿ç•™åŠŸèƒ½)ã€‚

	5. infer_music (task="infer_music", é»˜è®¤)
	   - ç«¯ç‚¹: /infer_music
	   - åŠŸèƒ½: æ ¹æ® LRCã€æ–‡æœ¬/éŸ³é¢‘ Prompt åŠæ‰©æ•£å‚æ•°ç”ŸæˆéŸ³ä¹
	   - å¯è°ƒå‚æ•°:
	       â€¢ lrc: å¯¹é½æ­Œè¯ (LRC)
	       â€¢ ref_audio_path: å‚è€ƒéŸ³é¢‘è·¯å¾„ (å¯ç©º)
	       â€¢ text_prompt: æ–‡æœ¬æç¤º
	       â€¢ seed/randomize_seed/steps/cfg_strength ç­‰ä¸æ‰©æ•£è¿‡ç¨‹ç›¸å…³å‚æ•°
	       â€¢ file_type: è¾“å‡ºæ ¼å¼ 'wav'/'mp3'/'ogg'
	       â€¢ odeint_method: æ¨ç† ODE æ–¹æ³• 'euler'/'midpoint'/'rk4'/'implicit_adams'
	       â€¢ preference_infer: 'quality first' æˆ– 'speed first'
	       â€¢ edit & edit_segments: æ”¯æŒå¯¹å·²ç”ŸæˆéŸ³é¢‘è¿›è¡Œå±€éƒ¨ç¼–è¾‘
	   - è¾“å‡º: ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶å°†ä¿å­˜åˆ° output_music_path

	è¿”å›å€¼:
	    â€¢ prompt_type/lambda_val ä»»åŠ¡: åŸå§‹è¿”å›å†…å®¹
	    â€¢ theme_tags/lyrics_lrc ä»»åŠ¡: ç”Ÿæˆçš„ LRC å­—ç¬¦ä¸²
	    â€¢ infer_music ä»»åŠ¡: è¾“å‡ºéŸ³é¢‘æ–‡ä»¶è·¯å¾„
	"""
	import os
	import shutil
	from gradio_client import Client, handle_file

	client = Client("ASLP-lab/DiffRhythm", hf_token=HF_TOKEN)

	# --- prompt_type ---
	if task == "prompt_type":
		return client.predict(api_name="/update_prompt_type")

	# --- theme_tags generation ---
	elif task == "theme_tags":
		if not theme or not tags_gen:
			raise ValueError("theme_tags ä»»åŠ¡éœ€è¦ theme ä¸ tags_gen")
		result = client.predict(
			theme=theme,
			tags_gen=tags_gen,
			language=language,
			api_name="/R1_infer1"
		)
		return result

	# --- lyrics to LRC ---
	elif task == "lyrics_lrc":
		if not tags_lyrics or not lyrics_input:
			raise ValueError("lyrics_lrc ä»»åŠ¡éœ€è¦ tags_lyrics ä¸ lyrics_input")
		result = client.predict(
			tags_lyrics=tags_lyrics,
			lyrics_input=lyrics_input,
			api_name="/R1_infer2"
		)
		return result

	# --- lambda value ---
	elif task == "lambda_val":
		return client.predict(api_name="/lambda")

	# --- infer_music ---
	elif task == "infer_music":
		if not lrc or not text_prompt:
			raise ValueError("infer_music ä»»åŠ¡éœ€è¦ lrc ä¸ text_prompt")
		
		ref_audio = handle_file(ref_audio_path) if ref_audio_path else None
		result = client.predict(
			lrc=lrc,
			ref_audio_path=ref_audio,
			text_prompt=text_prompt,
			seed=seed,
			randomize_seed=randomize_seed,
			steps=steps,
			cfg_strength=cfg_strength,
			file_type=file_type,
			odeint_method=odeint_method,
			preference_infer=preference_infer,
			edit=edit,
			edit_segments=edit_segments,
			api_name="/infer_music"
		)
		# result ä¸ºç”Ÿæˆçš„éŸ³é¢‘è·¯å¾„
		os.makedirs(os.path.dirname(os.path.abspath(output_music_path)), exist_ok=True)
		shutil.copy(result, output_music_path)
		return output_music_path

	else:
		raise ValueError("æœªçŸ¥ä»»åŠ¡ç±»å‹: " + task)


@mcp.tool()
def ACE_Step_api(
	task: str = "text2music",  # text2music, retake, repaint, edit, extend, sample_data, get_audio, edit_type
	# é€šç”¨å‚æ•°
	input_json_1: dict | list | str | float | bool = None,
	input_json_2: dict | list | str | float | bool = None,
	input_audio_path: str = None,
	# ç”Ÿæˆ/ç¼–è¾‘å…¬å…±æ§åˆ¶å‚æ•°
	prompt: str = "",
	lyrics: str = "",
	infer_step: float = 27,
	guidance_scale: float = 15,
	scheduler_type: str = "euler",
	cfg_type: str = "apg",
	omega_scale: float = 10,
	manual_seeds: str | None = None,
	guidance_interval: float = 0.5,
	guidance_interval_decay: float = 0.0,
	min_guidance_scale: float = 3,
	use_erg_tag: bool = True,
	use_erg_lyric: bool = True,
	use_erg_diffusion: bool = True,
	oss_steps: str | None = None,
	guidance_scale_text: float = 0.0,
	guidance_scale_lyric: float = 0.0,
	# retake / repaint ç‰¹æœ‰
	retake_variance: float = 0.2,
	retake_seeds: str = "",
	# repaint ç‰¹æœ‰
	repaint_start: float = 0.0,
	repaint_end: float = 30.0,
	repaint_source: str = "text2music",  # text2music / last_repaint / upload
	# edit ç±»å‹
	edit_type: str = "only_lyrics",  # or remix
	edit_prompt: str = "",
	edit_lyrics: str = "",
	edit_n_min: float = 0.6,
	edit_n_max: float = 1.0,
	# extend å‚æ•°
	left_extend_length: float = 0.0,
	right_extend_length: float = 30.0,
	extend_source: str = "text2music",  # text2music / last_extend / upload
	extend_seeds: str = "",
	# get_audio lambda é˜¶æ®µ
	lambda_stage: str = "text2music",  # text2music / last_repaint / upload / last_edit / last_extend
	# è¾“å‡º
	output_audio_path: str = "ace_step_output.wav",
	output_json_path: str = "ace_step_params.json"
):
	"""ACE-Step å…¨æµç¨‹éŸ³ä¹ç”Ÿæˆ Â· ç¼–è¾‘ Â· æ‰©å±•ä¸€ä½“åŒ–å·¥å…·

	æ¦‚è¿°
	------
	ACE-Step æ˜¯ä¸€ä¸ªé’ˆå¯¹ *æ–‡æœ¬â†’éŸ³ä¹*ã€ä»¥åŠåç»­ *éŸ³é¢‘å¾®è°ƒ / é‡ç»˜ / æ‰©å±•* çš„ç«¯åˆ°ç«¯ Web å¹³å°ã€‚æœ¬
	å·¥å…·å¯¹å…¶ 9 ä¸ªå…³é”®ç«¯ç‚¹åšäº†äºŒæ¬¡å°è£…ï¼Œç”¨æˆ·åªéœ€ `task`+å‚æ•°å³å¯å®Œæˆä»é›¶åˆ°å¤šè½®ç¼–è¾‘çš„å®Œæ•´
	 pipelineã€‚

	æ”¯æŒçš„ task ä¸ç«¯ç‚¹
	-------------------
	1. **text2music**  â†’ /__call__
	   ç›´æ¥æŒ‰ *prompt+lyrics* ç”ŸæˆéŸ³ä¹ã€‚
	2. **retake**      â†’ /retake_process_func
	   å¯¹ *text2music* ç»“æœé‡æ–°æŠ½æ · (variance/seed è°ƒæ•´)ã€‚
	3. **repaint**     â†’ /repaint_process_func
	   å¯¹æŒ‡å®šæ—¶é—´æ®µåš"é‡ç»˜"ï¼Œå¯é€‰å‚è€ƒéŸ³é¢‘ã€‚
	4. **edit**        â†’ /edit_process_func
	   åŸºäºå·²æœ‰éŸ³é¢‘æ‰§è¡Œ"å±€éƒ¨ç¼–è¾‘"ï¼š
	   â€‘ only_lyricsï¼šåªæ›¿æ¢æ­Œè¯ï¼›
	   â€‘ remixï¼šåŒæ—¶ä¿®æ”¹æ­Œè¯å’Œé£æ ¼ Tagã€‚
	5. **extend**      â†’ /extend_process_func
	   åœ¨å·¦å³æ–¹å‘æ‰©å±•éŸ³é¢‘æ—¶é•¿ï¼Œå¯ç»§ç»­ç”¨äºä¸‹ä¸€è½®ç¼–è¾‘ã€‚
	6. **sample_data** â†’ /sample_data
	   è¿”å›ä¸€ç»„å®˜æ–¹çš„**è¶…å‚æ•°æ¨¡æ¿**ï¼ˆ18 ä¸ªå€¼ï¼‰ç”¨äºå¿«é€Ÿè°ƒå‚ã€‚
	7. **get_audio**   â†’ /lambda, /lambda_1, /lambda_2
	   æ ¹æ® `lambda_stage` è·å–å„é˜¶æ®µæºéŸ³é¢‘ï¼ˆç”Ÿæˆã€é‡ç»˜ã€ç¼–è¾‘ã€æ‰©å±•ï¼‰ã€‚
	8. **edit_type**   â†’ /edit_type_change_func
	   è¯»å– / è®¾ç½®å½“å‰ç¼–è¾‘ç±»å‹ï¼Œè·å¾—å¯è°ƒåŒºé—´ *(edit_n_min, edit_n_max)*ã€‚

	ä»»åŠ¡ä¹‹é—´çš„å…³è”
	----------------
	â€¢ text2music äº§ç”Ÿ *A.wav* ä¸ *A.json* â†’ å¯å–‚ç»™ retake / repaint / edit / extendã€‚
	â€¢ repaint/edit/extend æ‰§è¡Œåä¼šè¿”å›æ–°çš„ *(éŸ³é¢‘, json)*ï¼Œå¯çº§è”ä½œä¸ºä¸‹ä¸€è½®è¾“å…¥ã€‚
	â€¢ get_audio èƒ½éšæ—¶æ‹‰å–å½“å‰é˜¶æ®µçš„"å·¥ä½œéŸ³é¢‘"ä¾›å¤–éƒ¨è¯•å¬æˆ–æ··éŸ³ã€‚
	å› æ­¤æ‚¨å¯ä»¥è‡ªç”±ç»„åˆï¼š
	```
	# å…ˆç”Ÿæˆ â†’ å±€éƒ¨é‡ç»˜ â†’ å‘å³æ‰©å±• 15s â†’ æœ€åå†å±€éƒ¨ remix
	res1 = ACE_Step_api(task="text2music", prompt=..., lyrics=...)
	res2 = ACE_Step_api(task="repaint", input_json_1=res1["params"], repaint_json_data={...})
	res3 = ACE_Step_api(task="extend",  input_json_1=res2["params"], extend_input_params_json={...}, right_extend_length=15)
	res4 = ACE_Step_api(task="edit",    input_json_1=res3["params"], edit_input_params_json={...}, edit_type="remix")
	```

	å‚æ•°è¯´æ˜ä¸å–å€¼èŒƒå›´
	------------------
	â€¢ **prompt** *(str, éç©º)*â€ƒâ€ƒ   éŸ³ä¹é£æ ¼ Tagï¼Œç”¨é€—å·åˆ†éš”ï¼›ç¤ºä¾‹ï¼š"pop, 120BPM, energetic"ã€‚
	â€¢ **lyrics** *(str, éç©º)*â€ƒâ€ƒ   åŸå§‹æ­Œè¯ï¼Œå¤šæ®µæ–‡æœ¬ã€‚
	â€¢ **infer_step** *(float, 1-100)*â€ƒ æ‹“æ‰‘æ­¥æ•°ï¼Œè¶Šå¤§è´¨é‡è¶Šé«˜ä½†è¶Šæ…¢ï¼›é»˜è®¤ 27ã€‚
	â€¢ **guidance_scale** *(float, 1-30)* æ¨ç† CFG Scaleï¼›é»˜è®¤ 15ã€‚
	â€¢ **scheduler_type** *(str)*â€ƒ  {"euler", "heun"} é‡‡æ ·å™¨ç±»å‹ã€‚
	â€¢ **cfg_type** *(str)*â€ƒâ€ƒ      {"cfg", "apg", "cfg_star"} CFG æ–¹æ¡ˆï¼Œé»˜è®¤ apgã€‚
	â€¢ **omega_scale** *(float, 0-20)*  é¢—ç²’åº¦æ§åˆ¶ï¼›é»˜è®¤ 10ã€‚
	â€¢ **manual_seeds** *(str|None)* æ‰‹åŠ¨ç§å­ï¼Œå½¢å¦‚ "42,17"ï¼›ä¸ºç©ºåˆ™éšæœºã€‚
	â€¢ **guidance_interval** *(float 0-1)*â€ƒCFG è§¦å‘é—´éš”ï¼Œé»˜è®¤ 0.5ã€‚
	â€¢ **use_erg_* ç³»åˆ—** *(bool)*â€ƒ æ˜¯å¦å¯ç”¨ ERG å¼ºåŒ–æ¨¡å—ï¼ˆtag/lyric/diffusionï¼‰ã€‚
	â€¢ **oss_steps** *(str|None)*â€ƒ   é€—å·åˆ†éš”çš„ OSS é˜¶æ®µæ­¥æ•°ï¼Œå¦‚ "0.2,0.6"ï¼›ç•™ç©ºä½¿ç”¨é»˜è®¤ã€‚
	â€¢ **output_audio_path / output_json_path** *(str)*â€ƒä¿å­˜ç»“æœçš„æœ¬åœ°è·¯å¾„ã€‚

	ç‰¹å®šä»»åŠ¡é¢å¤–å‚æ•°
	~~~~~~~~~~~~~~~~~
	- **retake_variance** *(0-1)*â€ƒ   éšæœºæ‰°åŠ¨å¹…åº¦ (retake / repaint)ã€‚
	- **retake_seeds** *(str)*â€ƒ     é€—å·åˆ†éš”çš„ç§å­é›†åˆï¼Œæ§åˆ¶é‡é‡‡æ ·ã€‚
	- **repaint_start / repaint_end** *(ç§’)*  é‡ç»˜åŒºé—´ã€‚
	- **left_extend_length / right_extend_length** *(ç§’)* æ‰©å±•æ—¶é•¿ã€‚
	- **edit_prompt / edit_lyrics**â€ƒç¼–è¾‘æ¨¡å¼ä¸‹çš„æ–° Tag / æ–°æ­Œè¯ã€‚
	- **edit_n_min / edit_n_max**â€ƒ  remix ç¼–è¾‘çš„éšæœºåŒºé—´ã€‚

	å¿…å¡«ä¸é€‰å¡«
	~~~~~~~~~~
	| task | å¿…å¡«é”® | è¯´æ˜ |
	|------|--------|------|
	| text2music | prompt, lyrics | å¦‚ç•™ç©ºä¼šè§¦å‘ ValueError |
	| retake | input_json_1, retake_seeds | input_json_1 ä¸ºä¸Šä¸€é˜¶æ®µ JSON |
	| repaint | input_json_1, input_json_2 | éœ€æä¾› text2music_json ä¸ repaint_json |
	| edit | input_json_1, input_json_2, edit_prompt/lyrics | |
	| extend | input_json_1, input_json_2, extend_seeds | |
	| sample_data / get_audio / edit_type | æ— å¼ºåˆ¶å‚æ•° | |

	ç¤ºä¾‹ï¼šæœ€å°åŒ–è°ƒç”¨
	----------------
	```python
	# ä¸€å¥ä»£ç ç”Ÿæˆ 30s demo
	ACE_Step_api(task="text2music", prompt="lofi, chill", lyrics="We are coding in the night")
	```
	"""
	import os
	import shutil
	import json
	from gradio_client import Client, handle_file

	client = Client("https://ace-step-ace-step.ms.show/",hf_token=MD_TOKEN)

	# è¾…åŠ©å‡½æ•°: ä¿å­˜æ–‡ä»¶/JSON
	def _save_audio(src_path: str, dst_path: str):
		os.makedirs(os.path.dirname(os.path.abspath(dst_path)), exist_ok=True)
		shutil.copy(src_path, dst_path)
		return dst_path

	def _save_json(data, dst_path: str):
		os.makedirs(os.path.dirname(os.path.abspath(dst_path)), exist_ok=True)
		with open(dst_path, 'w', encoding='utf-8') as f:
			json.dump(data, f, ensure_ascii=False, indent=2)
		return dst_path

	# ---------- ä»»åŠ¡åˆ†å‘ ----------
	if task == "sample_data":
		return client.predict(api_name="/sample_data")

	elif task == "get_audio":
		lambda_map = {
			"text2music": "/lambda",
			"last_repaint": "/lambda",
			"upload": "/lambda",
			"last_edit": "/lambda_1",
			"last_extend": "/lambda_2"
		}
		api_endpoint = lambda_map.get(lambda_stage)
		if not api_endpoint:
			raise ValueError("æ— æ•ˆ lambda_stage")
		result = client.predict(x=lambda_stage, api_name=api_endpoint)
		# è¿”å›éŸ³é¢‘è·¯å¾„
		return result

	elif task == "edit_type":
		result = client.predict(edit_type=edit_type, api_name="/edit_type_change_func")
		return {"edit_n_min": result[0], "edit_n_max": result[1]}

	elif task == "text2music":
		result = client.predict(
			audio_duration=-1,
			prompt=prompt,
			lyrics=lyrics,
			infer_step=infer_step,
			guidance_scale=guidance_scale,
			scheduler_type=scheduler_type,
			cfg_type=cfg_type,
			omega_scale=omega_scale,
			manual_seeds=manual_seeds,
			guidance_interval=guidance_interval,
			guidance_interval_decay=guidance_interval_decay,
			min_guidance_scale=min_guidance_scale,
			use_erg_tag=use_erg_tag,
			use_erg_lyric=use_erg_lyric,
			use_erg_diffusion=use_erg_diffusion,
			oss_steps=oss_steps,
			guidance_scale_text=guidance_scale_text,
			guidance_scale_lyric=guidance_scale_lyric,
			api_name="/__call__"
		)
		audio_path, params_json = result
		_audio_dst = _save_audio(audio_path, output_audio_path)
		_json_dst = _save_json(params_json, output_json_path)
		return {"audio": _audio_dst, "params": _json_dst}

	elif task == "retake":
		if input_json_1 is None:
			raise ValueError("retake ä»»åŠ¡éœ€è¦ input_json_1 ä½œä¸º text2music_json_data")
		result = client.predict(
			json_data=input_json_1,
			retake_variance=retake_variance,
			retake_seeds=retake_seeds,
			api_name="/retake_process_func"
		)
		audio_path, params_json = result
		_audio_dst = _save_audio(audio_path, output_audio_path)
		_json_dst = _save_json(params_json, output_json_path)
		return {"audio": _audio_dst, "params": _json_dst}

	elif task == "repaint":
		if input_json_1 is None or input_json_2 is None:
			raise ValueError("repaint ä»»åŠ¡éœ€è¦ text2music_json_data ä¸ repaint_json_data")
		file_upload = handle_file(input_audio_path) if input_audio_path else None
		result = client.predict(
			text2music_json_data=input_json_1,
			repaint_json_data=input_json_2,
			retake_variance=retake_variance,
			retake_seeds=retake_seeds,
			repaint_start=repaint_start,
			repaint_end=repaint_end,
			repaint_source=repaint_source,
			repaint_source_audio_upload=file_upload,
			prompt=prompt,
			lyrics=lyrics,
			infer_step=infer_step,
			guidance_scale=guidance_scale,
			scheduler_type=scheduler_type,
			cfg_type=cfg_type,
			omega_scale=omega_scale,
			manual_seeds=manual_seeds,
			guidance_interval=guidance_interval,
			guidance_interval_decay=guidance_interval_decay,
			min_guidance_scale=min_guidance_scale,
			use_erg_tag=use_erg_tag,
			use_erg_lyric=use_erg_lyric,
			use_erg_diffusion=use_erg_diffusion,
			oss_steps=oss_steps,
			guidance_scale_text=guidance_scale_text,
			guidance_scale_lyric=guidance_scale_lyric,
			api_name="/repaint_process_func"
		)
		audio_path, params_json = result
		_audio_dst = _save_audio(audio_path, output_audio_path)
		_json_dst = _save_json(params_json, output_json_path)
		return {"audio": _audio_dst, "params": _json_dst}

	elif task == "edit":
		if input_json_1 is None or input_json_2 is None:
			raise ValueError("edit ä»»åŠ¡éœ€è¦ text2music_json_data ä¸ edit_input_params_json")
		file_upload = handle_file(input_audio_path) if input_audio_path else None
		result = client.predict(
			text2music_json_data=input_json_1,
			edit_input_params_json=input_json_2,
			edit_source=extend_source,  # å¤ç”¨ extend_source ä½œä¸º edit_source é€‰æ‹©
			edit_source_audio_upload=file_upload,
			prompt=prompt,
			lyrics=lyrics,
			edit_prompt=edit_prompt,
			edit_lyrics=edit_lyrics,
			edit_n_min=edit_n_min,
			edit_n_max=edit_n_max,
			infer_step=infer_step,
			guidance_scale=guidance_scale,
			scheduler_type=scheduler_type,
			cfg_type=cfg_type,
			omega_scale=omega_scale,
			manual_seeds=manual_seeds,
			guidance_interval=guidance_interval,
			guidance_interval_decay=guidance_interval_decay,
			min_guidance_scale=min_guidance_scale,
			use_erg_tag=use_erg_tag,
			use_erg_lyric=use_erg_lyric,
			use_erg_diffusion=use_erg_diffusion,
			oss_steps=oss_steps,
			guidance_scale_text=guidance_scale_text,
			guidance_scale_lyric=guidance_scale_lyric,
			retake_seeds=retake_seeds,
			api_name="/edit_process_func"
		)
		audio_path, params_json = result
		_audio_dst = _save_audio(audio_path, output_audio_path)
		_json_dst = _save_json(params_json, output_json_path)
		return {"audio": _audio_dst, "params": _json_dst}

	elif task == "extend":
		if input_json_1 is None or input_json_2 is None:
			raise ValueError("extend ä»»åŠ¡éœ€è¦ text2music_json_data ä¸ extend_input_params_json")
		file_upload = handle_file(input_audio_path) if input_audio_path else None
		result = client.predict(
			text2music_json_data=input_json_1,
			extend_input_params_json=input_json_2,
			extend_seeds=extend_seeds,
			left_extend_length=left_extend_length,
			right_extend_length=right_extend_length,
			extend_source=extend_source,
			extend_source_audio_upload=file_upload,
			prompt=prompt,
			lyrics=lyrics,
			infer_step=infer_step,
			guidance_scale=guidance_scale,
			scheduler_type=scheduler_type,
			cfg_type=cfg_type,
			omega_scale=omega_scale,
			manual_seeds=manual_seeds,
			guidance_interval=guidance_interval,
			guidance_interval_decay=guidance_interval_decay,
			min_guidance_scale=min_guidance_scale,
			use_erg_tag=use_erg_tag,
			use_erg_lyric=use_erg_lyric,
			use_erg_diffusion=use_erg_diffusion,
			oss_steps=oss_steps,
			guidance_scale_text=guidance_scale_text,
			guidance_scale_lyric=guidance_scale_lyric,
			api_name="/extend_process_func"
		)
		audio_path, params_json = result
		_audio_dst = _save_audio(audio_path, output_audio_path)
		_json_dst = _save_json(params_json, output_json_path)
		return {"audio": _audio_dst, "params": _json_dst}

	else:
		raise ValueError("æœªçŸ¥ä»»åŠ¡ç±»å‹: " + task)


@mcp.tool()
def SenseVoice_api(
	input_wav_path: str,
	language: str = "auto",
	output_txt_path: str = "sensevoice_result.txt"
):
	"""SenseVoice-Small è¯­éŸ³ç†è§£å·¥å…·

	åŸºäº iic-sensevoice Spaceï¼ˆç«¯ç‚¹ /model_inferenceï¼‰ã€‚
	ä¸€æ¬¡è°ƒç”¨å³å¯å®Œæˆï¼š
	1. è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰
	2. è¯­è¨€è¯†åˆ«ï¼ˆLIDï¼‰
	3. è¯­éŸ³æƒ…ç»ªè¯†åˆ«ï¼ˆSERï¼‰
	4. å£°å­¦äº‹ä»¶æ£€æµ‹ï¼ˆAEDï¼‰

	æ¨¡å‹å…·å¤‡è¶…ä½å»¶è¿Ÿï¼Œæ”¯æŒä»¥ä¸‹è¯­è¨€ï¼š
		â€¢ zhï¼ˆä¸­æ–‡ï¼‰
		â€¢ enï¼ˆè‹±è¯­ï¼‰
		â€¢ yueï¼ˆç²¤è¯­ï¼‰
		â€¢ jaï¼ˆæ—¥è¯­ï¼‰
		â€¢ koï¼ˆéŸ©è¯­ï¼‰
	é€‰æ‹© "auto" å¯è‡ªåŠ¨æ£€æµ‹ã€‚æ¨èè¾“å…¥æ—¶é•¿ â‰¤30 ç§’ã€‚

	Args:
		input_wav_path: æœ¬åœ°éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆ16k/44k å‡å¯ï¼‰ã€‚
		language: è¯­è¨€ä»£ç ï¼Œå–å€¼ {"auto","zh","en","yue","ja","ko","nospeech"}ã€‚
		output_txt_path: æ¨ç†ç»“æœä¿å­˜è·¯å¾„ï¼›æ–‡ä»¶å†…å®¹ç¤ºä¾‹ï¼š
			"ğŸ¼[music] ä½ å¥½ï¼Œä»Šå¤©å¿ƒæƒ…ä¸é”™ã€‚ ğŸ˜Š"

	Returns:
		str: ç»“æœæ–‡æœ¬æˆ–ä¿å­˜æ–‡ä»¶è·¯å¾„ï¼ˆå½“æŒ‡å®š output_txt_path æ—¶ï¼‰ã€‚
	"""
	import os
	from gradio_client import Client, file

	# å‚æ•°æ ¡éªŒ
	valid_langs = {"auto","zh","en","yue","ja","ko","nospeech"}
	if language not in valid_langs:
		raise ValueError(f"language å¿…é¡»æ˜¯ {valid_langs}")
	if not os.path.exists(input_wav_path):
		raise FileNotFoundError(input_wav_path)

	client = Client("https://iic-sensevoice.ms.show/",hf_token=MD_TOKEN)
	result = client.predict(
		input_wav=file(input_wav_path),
		language=language,
		api_name="/model_inference"
	)

	# ä¿å­˜ç»“æœ
	os.makedirs(os.path.dirname(os.path.abspath(output_txt_path)), exist_ok=True)
	with open(output_txt_path, 'w', encoding='utf-8') as f:
		f.write(result)

	return output_txt_path

@mcp.tool()
def whisper_large_v3_turbo_api(
    audio_path: str = "",
    yt_url: str = "",
    task: str = "transcribe",
    output_path: str = ""
):
    """
    ä½¿ç”¨ hf-audio/whisper-large-v3-turbo æ¨¡å‹è¿›è¡ŒéŸ³é¢‘è½¬å½•æˆ–ç¿»è¯‘ã€‚

    - æœ¬å·¥å…·æ”¯æŒä¸‰ç§è¾“å…¥æ–¹å¼ï¼šæœ¬åœ°éŸ³é¢‘æ–‡ä»¶ã€åœ¨çº¿éŸ³é¢‘æ–‡ä»¶URLã€YouTubeè§†é¢‘URLã€‚
    - å½“æä¾› `audio_path` æ—¶ï¼Œå°†è°ƒç”¨ /predict ç«¯ç‚¹è¿›è¡Œå¤„ç†ã€‚
    - å½“æä¾› `yt_url` æ—¶ï¼Œå°†è°ƒç”¨ /predict_2 ç«¯ç‚¹è¿›è¡Œå¤„ç†ã€‚
    - `audio_path` å’Œ `yt_url` å‚æ•°æ˜¯äº’æ–¥çš„ï¼Œè¯·åªæä¾›å…¶ä¸­ä¸€ä¸ªã€‚
    - æ”¯æŒçš„ä»»åŠ¡ç±»å‹ (task) åŒ…æ‹¬ 'transcribe' (è¯­éŸ³è½¬æ–‡æœ¬) å’Œ 'translate' (å°†éŸ³é¢‘ç¿»è¯‘æˆè‹±æ–‡)ã€‚
    - è¿”å›ç»“æœä¸ºè½¬å½•æˆ–ç¿»è¯‘åçš„æ–‡æœ¬å†…å®¹ã€‚å¦‚æœæŒ‡å®šäº† `output_path`ï¼Œç»“æœå°†å­˜å…¥æ–‡ä»¶å¹¶è¿”å›æ–‡ä»¶è·¯å¾„ã€‚

    Args:
        audio_path (str): æœ¬åœ°éŸ³é¢‘æ–‡ä»¶è·¯å¾„æˆ–å¯ç›´æ¥è®¿é—®çš„éŸ³é¢‘æ–‡ä»¶URLã€‚ä¾‹å¦‚ 'path/to/audio.wav' æˆ– 'https://example.com/audio.mp3'ã€‚
        yt_url (str): YouTube è§†é¢‘çš„ URLã€‚ä¾‹å¦‚ 'https://www.youtube.com/watch?v=xxxx'ã€‚
        task (str): è¦æ‰§è¡Œçš„ä»»åŠ¡ï¼Œå¯é€‰å€¼ä¸º 'transcribe' (é»˜è®¤) æˆ– 'translate'ã€‚
        output_path (str): (å¯é€‰) ä¿å­˜ç»“æœçš„æ–‡ä»¶è·¯å¾„ã€‚å¦‚æœæä¾›ï¼Œå‡½æ•°å°†ç»“æœå†™å…¥è¯¥æ–‡ä»¶å¹¶è¿”å›è·¯å¾„ï¼›å¦åˆ™ï¼Œç›´æ¥è¿”å›æ–‡æœ¬ç»“æœã€‚

    Returns:
        str: è¯†åˆ«/ç¿»è¯‘çš„æ–‡æœ¬ç»“æœï¼Œæˆ–è€…ç»“æœæ–‡ä»¶çš„ä¿å­˜è·¯å¾„ã€‚
    """

    # 1. å‚æ•°æ ¡éªŒ
    if not audio_path and not yt_url:
        raise ValueError("å¿…é¡»æä¾› 'audio_path' æˆ– 'yt_url' å‚æ•°ä¹‹ä¸€ã€‚")
    if audio_path and yt_url:
        raise ValueError("'audio_path' å’Œ 'yt_url' æ˜¯äº’æ–¥å‚æ•°ï¼Œè¯·åªæä¾›ä¸€ä¸ªã€‚")
    if task not in ["transcribe", "translate"]:
        raise ValueError("å‚æ•° 'task' çš„å€¼å¿…é¡»æ˜¯ 'transcribe' æˆ– 'translate'ã€‚")
    
    # æ ¡éªŒæœ¬åœ°æ–‡ä»¶è·¯å¾„æ˜¯å¦å­˜åœ¨
    if audio_path and not (audio_path.startswith("http://") or audio_path.startswith("https://")):
        if not os.path.exists(audio_path):
            raise FileNotFoundError(f"æŒ‡å®šçš„æœ¬åœ°éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")

    # 2. å®ä¾‹åŒ–API client
    try:
        client = Client("hf-audio/whisper-large-v3-turbo", hf_token=HF_TOKEN)
    except Exception as e:
        raise ConnectionError(f"æ— æ³•è¿æ¥åˆ° Hugging Face Space 'hf-audio/whisper-large-v3-turbo'ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æœåŠ¡çŠ¶æ€ã€‚é”™è¯¯: {e}")

    result_text = None

    # 3. æ ¹æ®å‚æ•°è°ƒç”¨ä¸åŒçš„ API ç«¯ç‚¹
    if audio_path:
        # è°ƒç”¨ /predict ç«¯ç‚¹å¤„ç†éŸ³é¢‘æ–‡ä»¶æˆ–URL
        # handle_file å¯ä»¥æ™ºèƒ½å¤„ç†æœ¬åœ°è·¯å¾„å’ŒURL
        print(f"è°ƒç”¨ /predict APIï¼Œè¾“å…¥: {audio_path}, ä»»åŠ¡: {task}")
        result = client.predict(
            inputs=handle_file(audio_path),
            task=task,
            api_name="/predict"
        )
        result_text = result

    elif yt_url:
        # è°ƒç”¨ /predict_2 ç«¯ç‚¹å¤„ç† YouTube URL
        print(f"è°ƒç”¨ /predict_2 APIï¼Œè¾“å…¥: {yt_url}, ä»»åŠ¡: {task}")
        result = client.predict(
            yt_url=yt_url,
            task=task,
            api_name="/predict_2"
        )
        # /predict_2 è¿”å›ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªå…ƒç´ çš„å…ƒç»„ï¼Œæ ¹æ®æ–‡æ¡£ï¼Œç¬¬äºŒä¸ªå…ƒç´ æ˜¯æ‰€éœ€çš„æ–‡æœ¬è¾“å‡º
        if isinstance(result, (list, tuple)) and len(result) > 1:
            result_text = result[1]
        else:
            raise TypeError(f"è°ƒç”¨ YouTube API çš„è¿”å›æ ¼å¼ä¸ç¬¦åˆé¢„æœŸã€‚æ”¶åˆ°: {result}")

    # 4. ç»“æœå¤„ç†å’Œä¿å­˜
    if not result_text:
        return "æœªèƒ½è·å–åˆ°æœ‰æ•ˆçš„è½¬å½•/ç¿»è¯‘ç»“æœã€‚"
        
    if output_path:
        output_dir = os.path.dirname(os.path.abspath(output_path))
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
        
        try:
            with open(output_path, "w", encoding="utf-8") as f:
                f.write(result_text)
            print(f"ç»“æœå·²æˆåŠŸä¿å­˜åˆ°: {output_path}")
            return output_path
        except IOError as e:
            raise IOError(f"æ— æ³•å°†ç»“æœå†™å…¥æ–‡ä»¶ {output_path}ã€‚é”™è¯¯: {e}")
    else:
        return result_text

@mcp.tool()
def tiger_api(
    input_file_path: str,
    task: str,
    output_dir: str = "output"
):
    """
    TIGER éŸ³é¢‘æå–å·¥å…·ï¼Œå¯ä»éŸ³é¢‘æˆ–è§†é¢‘ä¸­åˆ†ç¦»éŸ³è½¨ã€‚

    è¯¦ç»†è¯´æ˜ï¼š
    - æ”¯æŒçš„APIç«¯ç‚¹/ä»»åŠ¡:
        - '/separate_dnr': ä»éŸ³é¢‘æ–‡ä»¶ä¸­åˆ†ç¦»å¯¹è¯ã€éŸ³æ•ˆå’ŒéŸ³ä¹ã€‚
        - '/separate_speakers': ä»éŸ³é¢‘æ–‡ä»¶ä¸­åˆ†ç¦»æœ€å¤š4ä¸ªè¯´è¯äººçš„å£°éŸ³ã€‚
        - '/separate_dnr_video': ä»è§†é¢‘æ–‡ä»¶ä¸­åˆ†ç¦»å¯¹è¯ã€éŸ³æ•ˆå’ŒéŸ³ä¹ï¼Œå¹¶è¿”å›åˆ†ç¦»åçš„è§†é¢‘ã€‚
        - '/separate_speakers_video': ä»è§†é¢‘æ–‡ä»¶ä¸­åˆ†ç¦»æœ€å¤š4ä¸ªè¯´è¯äººçš„å£°éŸ³ï¼Œå¹¶è¿”å›åˆ†ç¦»åçš„è§†é¢‘ã€‚
    - æ”¯æŒçš„è¾“å…¥: å•ä¸ªéŸ³é¢‘æˆ–è§†é¢‘æ–‡ä»¶çš„è·¯å¾„ã€‚
    - è¿”å›å€¼: ä¸€ä¸ªåŒ…å«æ‰€æœ‰è¾“å‡ºæ–‡ä»¶è·¯å¾„çš„åˆ—è¡¨ã€‚
    - ç¤ºä¾‹ç”¨æ³•:
        audio_result = tiger_audio_extraction("path/to/audio.wav", "/separate_speakers", "results/speakers")
        video_result = tiger_audio_extraction("path/to/video.mp4", "/separate_dnr_video", "results/dnr")

    Args:
        input_file_path (str): è¾“å…¥çš„éŸ³é¢‘æˆ–è§†é¢‘æ–‡ä»¶è·¯å¾„ (å¿…éœ€)ã€‚
        task (str): éœ€è¦æ‰§è¡Œçš„ä»»åŠ¡ï¼Œå¿…é¡»æ˜¯å››ä¸ªæœ‰æ•ˆAPIç«¯ç‚¹ä¹‹ä¸€ (å¿…éœ€)ã€‚
        output_dir (str): ä¿å­˜è¾“å‡ºæ–‡ä»¶çš„ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸º "output"ã€‚

    Returns:
        list[str]: åŒ…å«æ‰€æœ‰å·²ä¿å­˜ç»“æœæ–‡ä»¶è·¯å¾„çš„åˆ—è¡¨ã€‚
    """

    # 1. å‚æ•°æ ¡éªŒ
    VALID_TASKS = [
        "/separate_dnr",
        "/separate_speakers",
        "/separate_dnr_video",
        "/separate_speakers_video"
    ]
    if task not in VALID_TASKS:
        raise ValueError(f"ä»»åŠ¡å‚æ•° '{task}' æ— æ•ˆ. "
                         f"æœ‰æ•ˆé€‰é¡¹ä¸º: {', '.join(VALID_TASKS)}")
    if not os.path.exists(input_file_path):
        raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶æœªæ‰¾åˆ°: {input_file_path}")

    # 2. å®ä¾‹åŒ–API client
    # è¿™æ˜¯ä¸€ä¸ª Hugging Face Space API
    client = Client("fffiloni/TIGER-audio-extraction", hf_token=HF_TOKEN)

    # 3. æ ¹æ®ä»»åŠ¡å‡†å¤‡å‚æ•°å’Œè°ƒç”¨API
    is_video_task = "video" in task
    
    if is_video_task:
        # è§†é¢‘ä»»åŠ¡çš„å‚æ•°å¤„ç†
        api_input = {
            "video_path": {"video": handle_file(input_file_path)}
        }
    else:
        # éŸ³é¢‘ä»»åŠ¡çš„å‚æ•°å¤„ç†
        # APIå¯¹éŸ³é¢‘å‚æ•°åä¸ä¸€è‡´ï¼Œ/separate_dnrç”¨'audio_file', /separate_speakersç”¨'audio_path'
        param_name = "audio_file" if task == "/separate_dnr" else "audio_path"
        api_input = {
            param_name: handle_file(input_file_path)
        }

    # è°ƒç”¨API
    print(f"æ­£åœ¨è°ƒç”¨API '{task}'ï¼Œå¤„ç†æ–‡ä»¶: {input_file_path}...")
    result_tuple = client.predict(
        **api_input,
        api_name=task
    )
    print("APIè°ƒç”¨å®Œæˆï¼Œæ­£åœ¨å¤„ç†ç»“æœ...")

    # 4. ç»“æœä¿å­˜
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    saved_files = []
    # APIè¿”å›çš„æ˜¯ä¸€ä¸ªåŒ…å«ä¸´æ—¶æ–‡ä»¶è·¯å¾„çš„å…ƒç»„
    for item in result_tuple:
        # å¯¹äºè§†é¢‘ä»»åŠ¡ï¼Œitemæ˜¯å­—å…¸ï¼›å¯¹äºéŸ³é¢‘ä»»åŠ¡ï¼Œitemæ˜¯å­—ç¬¦ä¸²è·¯å¾„
        if is_video_task and isinstance(item, dict):
            temp_path = item.get("video")
        elif isinstance(item, str):
            temp_path = item
        else:
            print(f"è­¦å‘Š: è·³è¿‡æ— æ³•è¯†åˆ«çš„ç»“æœé¡¹: {item}")
            continue

        if temp_path and os.path.exists(temp_path):
            # æ„å»ºç›®æ ‡è·¯å¾„å¹¶å¤åˆ¶æ–‡ä»¶
            dest_path = os.path.join(output_dir, os.path.basename(temp_path))
            shutil.copy(temp_path, dest_path)
            saved_files.append(dest_path)
            print(f"ç»“æœå·²ä¿å­˜è‡³: {dest_path}")
        else:
            print(f"è­¦å‘Š: æœªæ‰¾åˆ°æœ‰æ•ˆçš„ä¸´æ—¶æ–‡ä»¶è·¯å¾„: {temp_path}")

    return saved_files

@mcp.tool()
def audio_super_resolution_api(
    audio_file_path: str,
    output_path: str,
    model_name: str = "basic",
    guidance_scale: float = 3.5,
    ddim_steps: int = 50,
    seed: int = 42
):
    """
    ä½¿ç”¨ Nick088/Audio-SR æ¨¡å‹è¿›è¡ŒéŸ³é¢‘è¶…åˆ†è¾¨ç‡ã€‚

    è¯¦ç»†è¯´æ˜ï¼š
    - æ­¤å·¥å…·é€šè¿‡æé«˜éŸ³é¢‘æ–‡ä»¶çš„åˆ†è¾¨ç‡æ¥å¢å¼ºå…¶è´¨é‡ã€‚
    - å®ƒä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ç”Ÿæˆè¾“å…¥éŸ³é¢‘çš„æ›´é«˜è´¨é‡ç‰ˆæœ¬ã€‚
    - æ”¯æŒçš„æ¨¡å‹ï¼š'basic' (åŸºç¡€), 'speech' (è¯­éŸ³)ã€‚
    - è¾“å…¥ï¼šæœ¬åœ°éŸ³é¢‘æ–‡ä»¶çš„è·¯å¾„ï¼ˆä¾‹å¦‚ .wav, .mp3ï¼‰ã€‚
    - è¿”å›å€¼ï¼šå¢å¼ºåè¾“å‡ºéŸ³é¢‘çš„æ–‡ä»¶è·¯å¾„ã€‚

    å‚æ•°:
        audio_file_path (str): å¿…å¡«ã€‚éœ€è¦å¢å¼ºçš„è¾“å…¥éŸ³é¢‘æ–‡ä»¶çš„è·¯å¾„ã€‚
        output_path (str): å¿…å¡«ã€‚ç”¨äºä¿å­˜ç»“æœå¢å¼ºéŸ³é¢‘æ–‡ä»¶çš„è·¯å¾„ã€‚
        model_name (str): è¦ä½¿ç”¨çš„æ¨¡å‹ã€‚å¯é€‰å€¼ä¸º 'basic' æˆ– 'speech'ã€‚é»˜è®¤ä¸º "basic"ã€‚
        guidance_scale (float): ç”¨äºæŒ‡å¯¼ç”Ÿæˆè¿‡ç¨‹çš„å°ºåº¦ã€‚é»˜è®¤ä¸º 3.5ã€‚
        ddim_steps (int): DDIM æ‰©æ•£æ¨¡å‹æ­¥æ•°ã€‚é»˜è®¤ä¸º 50ã€‚
        seed (int): ç”¨äºå¤ç°ç»“æœçš„éšæœºç§å­ã€‚é»˜è®¤ä¸º 42ã€‚

    è¿”å›:
        str: ä¿å­˜å¢å¼ºåéŸ³é¢‘çš„æ–‡ä»¶è·¯å¾„ã€‚
    """

    # å‚æ•°æ ¡éªŒ
    if not audio_file_path:
        raise ValueError("audio_file_path æ˜¯ä¸€ä¸ªå¿…å¡«å‚æ•°ã€‚")
    if not os.path.exists(audio_file_path):
        raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶æœªæ‰¾åˆ°: {audio_file_path}")
    if model_name not in ["basic", "speech"]:
        raise ValueError("model_name å¿…é¡»æ˜¯ 'basic' æˆ– 'speech'ã€‚")

    # å®ä¾‹åŒ–APIå®¢æˆ·ç«¯
    # æ­¤å¤„ä½¿ç”¨ Hugging Face Space API çš„è·¯å¾„çº¦å®šã€‚
    client = Client("Nick088/Audio-SR", hf_token=HF_TOKEN)

    # è°ƒç”¨API
    # handle_file å‡½æ•°ä¼šå¤„ç†æœ¬åœ°æ–‡ä»¶ï¼Œä¸ºä¸Šä¼ åšå‡†å¤‡ã€‚
    result_temp_path = client.predict(
        audio_file=handle_file(audio_file_path),
        model_name=model_name,
        guidance_scale=guidance_scale,
        ddim_steps=ddim_steps,
        seed=seed,
        api_name="/predict"
    )

    # ä¿å­˜ç»“æœ
    # å¦‚æœç›®æ ‡ç›®å½•ä¸å­˜åœ¨ï¼Œåˆ™åˆ›å»ºå®ƒã€‚
    output_dir = os.path.dirname(os.path.abspath(output_path))
    os.makedirs(output_dir, exist_ok=True)

    # å°† API è¿”å›çš„ä¸´æ—¶ç»“æœæ–‡ä»¶ç§»åŠ¨åˆ°ç”¨æˆ·æŒ‡å®šçš„è¾“å‡ºè·¯å¾„ã€‚
    shutil.move(result_temp_path, output_path)
    
    print(f"å¢å¼ºåçš„éŸ³é¢‘å·²ä¿å­˜è‡³: {output_path}")
    return output_path

@mcp.tool()
def index_tts_1_5_api(
    prompt_audio_path: str,
    target_text: str,
    output_path: str = "generated_audio.wav"
):
    """
    æ–‡æœ¬è½¬è¯­éŸ³å·¥å…· (TTS)ï¼Œå¯æ ¹æ®å‚è€ƒéŸ³é¢‘å…‹éš†éŸ³è‰²æ¥ç”Ÿæˆç›®æ ‡æ–‡æœ¬çš„è¯­éŸ³ã€‚

    è¯¦ç»†è¯´æ˜ï¼š
    - API ç«¯ç‚¹: /gen_single
    - åŠŸèƒ½: æ¥æ”¶ä¸€æ®µå‚è€ƒéŸ³é¢‘å’Œä¸€ä¸ªæ–‡æœ¬å­—ç¬¦ä¸²ï¼Œç„¶åç”Ÿæˆä¸€ä¸ªä½¿ç”¨å‚è€ƒéŸ³é¢‘éŸ³è‰²çš„æ–°éŸ³é¢‘æ–‡ä»¶ã€‚
    - è¾“å…¥:
        - prompt_audio_path (str): å¿…éœ€ã€‚ä½œä¸ºå£°éŸ³å‚è€ƒçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆä¾‹å¦‚ .wav, .mp3ï¼‰ã€‚
        - target_text (str): å¿…éœ€ã€‚éœ€è¦è½¬æ¢ä¸ºè¯­éŸ³çš„æ–‡æœ¬ã€‚
    - è¿”å›å€¼: ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶çš„ä¿å­˜è·¯å¾„ã€‚
    - ç¤ºä¾‹ç”¨æ³•:
      generate_audio_from_prompt(
          prompt_audio_path="my_reference_voice.wav",
          target_text="ä½ å¥½ï¼Œæ¬¢è¿ä½¿ç”¨è¿™ä¸ªè¯­éŸ³åˆæˆå·¥å…·ï¼",
          output_path="result.wav"
      )

    Args:
        prompt_audio_path (str): å‚è€ƒéŸ³é¢‘çš„æ–‡ä»¶è·¯å¾„ã€‚æ­¤ä¸ºå¿…éœ€å‚æ•°ã€‚
        target_text (str): éœ€è¦è½¬æ¢ä¸ºè¯­éŸ³çš„ç›®æ ‡æ–‡æœ¬ã€‚æ­¤ä¸ºå¿…éœ€å‚æ•°ã€‚
        output_path (str): ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶çš„ä¿å­˜è·¯å¾„ã€‚é»˜è®¤ä¸º "generated_audio.wav"ã€‚

    Returns:
        str: æˆåŠŸç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶çš„æœ€ç»ˆè·¯å¾„ã€‚
    """

    # 1. å‚æ•°æ ¡éªŒ
    if not prompt_audio_path:
        raise ValueError("å‚æ•° 'prompt_audio_path' æ˜¯å¿…å¡«é¡¹ã€‚")
    if not os.path.exists(prompt_audio_path):
        raise FileNotFoundError(f"æŒ‡å®šçš„å‚è€ƒéŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {prompt_audio_path}")
    if not target_text:
        raise ValueError("å‚æ•° 'target_text' æ˜¯å¿…å¡«é¡¹ã€‚")

    # 2. å®ä¾‹åŒ–API client
    # æ ¹æ®APIæ–‡æ¡£ï¼Œè¿™æ˜¯ä¸€ä¸ªGradioåº”ç”¨ï¼Œä½¿ç”¨å…¶URLè¿›è¡Œå®ä¾‹åŒ–
    client = Client("https://indexteam-indextts-demo.ms.show/",hf_token=MD_TOKEN)

    # 3. è°ƒç”¨API
    # ä½¿ç”¨ handle_file å¤„ç†æ–‡ä»¶å‚æ•°ï¼ŒAPIä¼šè¿”å›ä¸€ä¸ªä¸´æ—¶çš„æ–‡ä»¶è·¯å¾„
    print("æ­£åœ¨è°ƒç”¨APIç”ŸæˆéŸ³é¢‘...")
    temp_output_path = client.predict(
        prompt=handle_file(prompt_audio_path),
        text=target_text,
        api_name="/gen_single"
    )
    print(f"APIè¿”å›ä¸´æ—¶æ–‡ä»¶è·¯å¾„: {temp_output_path}")

    # 4. ç»“æœä¿å­˜
    # åˆ›å»ºè¾“å‡ºè·¯å¾„æ‰€åœ¨çš„ç›®å½•ï¼ˆå¦‚æœç›®å½•ä¸å­˜åœ¨çš„è¯ï¼‰
    output_dir = os.path.dirname(os.path.abspath(output_path))
    os.makedirs(output_dir, exist_ok=True)

    # å°†APIè¿”å›çš„ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶å¤åˆ¶åˆ°ç”¨æˆ·æŒ‡å®šçš„æœ€ç»ˆè·¯å¾„
    shutil.copy(temp_output_path, output_path)
    print(f"éŸ³é¢‘æ–‡ä»¶å·²æˆåŠŸä¿å­˜åˆ°: {output_path}")

    # 5. è¿”å›æœ€ç»ˆæ–‡ä»¶è·¯å¾„
    return output_path

@mcp.tool()
def audiocraft_jasco_api(
    # 1. å‚æ•°åŒºï¼šä¸APIç«¯ç‚¹å‚æ•°ä¸€ä¸€å¯¹åº”
    model: str = "facebook/jasco-chords-drums-melody-400M",
    text: str = "Strings, woodwind, orchestral, symphony.",
    chords_sym: str = "(C, 0.0), (D, 2.0), (F, 4.0), (Ab, 6.0), (Bb, 7.0), (C, 8.0)",
    melody_file_path: str = "",
    drums_file_path: str = "",
    drums_mic_path: str = "",
    drum_input_src: str = "file",
    cfg_coef_all: float = 1.25,
    cfg_coef_txt: float = 2.5,
    ode_rtol: float = 0.0001,
    ode_atol: float = 0.0001,
    ode_solver: str = "euler",
    ode_steps: float = 10,
    output_dir: str = "output_audio"
):
    """
    AudiocraftéŸ³ä¹ç”Ÿæˆå·¥å…·

    è¯¦ç»†è¯´æ˜ï¼š
    - è°ƒç”¨ Tonic/audiocraft çš„ /predict_full API ç«¯ç‚¹ï¼Œæ ¹æ®æ–‡æœ¬ã€å’Œå¼¦ã€æ—‹å¾‹å’Œé¼“ç‚¹ç”ŸæˆéŸ³ä¹ã€‚
    - æ”¯æŒçš„è¾“å…¥ç±»å‹ï¼šæ–‡æœ¬æè¿°ã€å’Œå¼¦è¿›è¡Œå­—ç¬¦ä¸²ã€æ—‹å¾‹éŸ³é¢‘æ–‡ä»¶ã€é¼“ç‚¹éŸ³é¢‘æ–‡ä»¶ã€‚
    - è¿”å›å€¼è¯´æ˜ï¼šè¿”å›ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«ä¸¤ä¸ªç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶ï¼ˆJasco Stem 1, Jasco Stem 2ï¼‰çš„æœ¬åœ°ä¿å­˜è·¯å¾„ã€‚
    - æ³¨æ„ï¼šæ—‹å¾‹å’Œé¼“ç‚¹æ–‡ä»¶æ˜¯å¯é€‰çš„ï¼Œä½†å…·ä½“ä½¿ç”¨å–å†³äºæ‰€é€‰æ¨¡å‹ã€‚ä¾‹å¦‚ï¼ŒåŒ…å« "melody" çš„æ¨¡å‹éœ€è¦ `melody_file_path`ã€‚

    ç¤ºä¾‹ç”¨æ³•ï¼š
    ```python
    # ç¤ºä¾‹1ï¼šä½¿ç”¨æ–‡æœ¬å’Œå’Œå¼¦ç”ŸæˆéŸ³ä¹
    stem1, stem2 = audiocraft_music_generation(
        model="facebook/jasco-chords-drums-400M",
        text="Acoustic folk song with a gentle guitar and a simple beat.",
        chords_sym="(G, 0.0), (C, 4.0), (G, 8.0), (D, 12.0)",
        output_dir="generated_music"
    )
    print(f"éŸ³ä¹å·²ç”Ÿæˆè‡³: {stem1}, {stem2}")

    # ç¤ºä¾‹2ï¼šåŠ å…¥æ—‹å¾‹å’Œé¼“ç‚¹æ–‡ä»¶
    # å‡è®¾ 'melody.wav' å’Œ 'drums.wav' å·²å­˜åœ¨
    stem1, stem2 = audiocraft_music_generation(
        model="facebook/jasco-chords-drums-melody-400M",
        text="Upbeat pop track with a catchy synth melody.",
        chords_sym="(Am, 0.0), (F, 2.0), (C, 4.0), (G, 6.0)",
        melody_file_path="path/to/your/melody.wav",
        drums_file_path="path/to/your/drums.wav",
        drum_input_src="file",
        output_dir="pop_track"
    )
    print(f"éŸ³ä¹å·²ç”Ÿæˆè‡³: {stem1}, {stem2}")
    ```

    Args:
        model (str): è¦ä½¿ç”¨çš„æ¨¡å‹åç§°ã€‚å¯é€‰å€¼: 'facebook/jasco-chords-drums-400M', 'facebook/jasco-chords-drums-1B', 'facebook/jasco-chords-drums-melody-400M', 'facebook/jasco-chords-drums-melody-1B'ã€‚
        text (str): æè¿°éŸ³ä¹é£æ ¼ã€ä¹å™¨ç­‰çš„æ–‡æœ¬æç¤ºã€‚
        chords_sym (str): å’Œå¼¦è¿›è¡Œå­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º `(CHORD, START_TIME_IN_SECONDS)`ã€‚
        melody_file_path (str): æ—‹å¾‹å‚è€ƒéŸ³é¢‘æ–‡ä»¶çš„æœ¬åœ°è·¯å¾„ã€‚å¯¹äºä½¿ç”¨æ—‹å¾‹çš„æ¨¡å‹æ˜¯å¿…éœ€çš„ã€‚
        drums_file_path (str): é¼“ç‚¹å‚è€ƒéŸ³é¢‘æ–‡ä»¶çš„æœ¬åœ°è·¯å¾„ã€‚å½“ `drum_input_src` ä¸º 'file' æ—¶ä½¿ç”¨ã€‚
        drums_mic_path (str): é€šè¿‡éº¦å…‹é£å½•åˆ¶çš„é¼“ç‚¹éŸ³é¢‘æ–‡ä»¶çš„æœ¬åœ°è·¯å¾„ã€‚å½“ `drum_input_src` ä¸º 'mic' æ—¶ä½¿ç”¨ã€‚
        drum_input_src (str): é¼“ç‚¹è¾“å…¥æºã€‚å¯é€‰å€¼: 'file', 'mic'ã€‚
        cfg_coef_all (float): Classifier-Free Guidance (CFG) çš„å…¨å±€ç³»æ•°ã€‚
        cfg_coef_txt (float): æ–‡æœ¬æ¡ä»¶çš„ CFG ç³»æ•°ã€‚
        ode_rtol (float): ODE æ±‚è§£å™¨çš„ç›¸å¯¹å®¹å·®ã€‚
        ode_atol (float): ODE æ±‚è§£å™¨çš„ç»å¯¹å®¹å·®ã€‚
        ode_solver (str): ODE æ±‚è§£å™¨ç±»å‹ã€‚å¯é€‰å€¼: 'euler', 'dopri5'ã€‚
        ode_steps (float): 'euler' æ±‚è§£å™¨çš„æ­¥æ•°ã€‚
        output_dir (str): ä¿å­˜ç”ŸæˆéŸ³é¢‘æ–‡ä»¶çš„ç›®å½•è·¯å¾„ï¼Œé»˜è®¤ä¸º "output_audio"ã€‚

    Returns:
        tuple[str, str]: åŒ…å«ä¸¤ä¸ªç”ŸæˆéŸ³é¢‘æ–‡ä»¶ï¼ˆstem1, stem2ï¼‰çš„å®Œæ•´è·¯å¾„çš„å…ƒç»„ã€‚
    """

    # 2. å‚æ•°æ ¡éªŒ
    if "melody" in model and not melody_file_path:
        raise ValueError(f"æ¨¡å‹ '{model}' éœ€è¦ä¸€ä¸ªæ—‹å¾‹æ–‡ä»¶ï¼Œè¯·æä¾› 'melody_file_path'ã€‚")
    if melody_file_path and not os.path.exists(melody_file_path):
        raise FileNotFoundError(f"æ—‹å¾‹æ–‡ä»¶æœªæ‰¾åˆ°: {melody_file_path}")
    if drum_input_src == "file" and drums_file_path and not os.path.exists(drums_file_path):
        raise FileNotFoundError(f"é¼“ç‚¹æ–‡ä»¶æœªæ‰¾åˆ°: {drums_file_path}")
    if drum_input_src == "mic" and drums_mic_path and not os.path.exists(drums_mic_path):
        raise FileNotFoundError(f"éº¦å…‹é£é¼“ç‚¹æ–‡ä»¶æœªæ‰¾åˆ°: {drums_mic_path}")

    # 3. å®ä¾‹åŒ–API client
    try:
        client = Client("Tonic/audiocraft", hf_token=HF_TOKEN)
    except Exception as e:
        raise ConnectionError(f"æ— æ³•è¿æ¥åˆ° Hugging Face Space 'Tonic/audiocraft': {e}")

    # 4. æ–‡ä»¶å‚æ•°å¤„ç†
    # ä½¿ç”¨ handle_file å¤„ç†è·¯å¾„ï¼Œå¦‚æœè·¯å¾„ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œåˆ™ä¼ é€’ None
    melody_input = handle_file(melody_file_path) if melody_file_path else None
    drums_input = handle_file(drums_file_path) if drums_file_path else None
    mic_input = handle_file(drums_mic_path) if drums_mic_path else None

    # 5. è°ƒç”¨API
    print("æ­£åœ¨è¿æ¥ API å¹¶ç”ŸæˆéŸ³é¢‘ï¼Œè¯·ç¨å€™...")
    result = client.predict(
        model=model,
        text=text,
        chords_sym=chords_sym,
        melody_file=melody_input,
        drums_file=drums_input,
        drums_mic=mic_input,
        drum_input_src=drum_input_src,
        cfg_coef_all=cfg_coef_all,
        cfg_coef_txt=cfg_coef_txt,
        ode_rtol=ode_rtol,
        ode_atol=ode_atol,
        ode_solver=ode_solver,
        ode_steps=ode_steps,
        api_name="/predict_full"
    )
    print("éŸ³é¢‘ç”ŸæˆæˆåŠŸï¼Œæ­£åœ¨å¤„ç†æ–‡ä»¶...")
    
    # 6. ç»“æœä¿å­˜
    # result æ˜¯ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªä¸´æ—¶æ–‡ä»¶è·¯å¾„çš„å…ƒç»„
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        
        # è·å–è¿”å›çš„ä¸´æ—¶æ–‡ä»¶è·¯å¾„
        temp_stem1_path, temp_stem2_path = result
        
        # æ„å»ºç›®æ ‡æ–‡ä»¶è·¯å¾„
        # ä»ä¸´æ—¶è·¯å¾„ä¸­æå–æ–‡ä»¶å
        stem1_filename = os.path.basename(temp_stem1_path)
        stem2_filename = os.path.basename(temp_stem2_path)
        
        output_stem1_path = os.path.join(output_dir, f"jasco_stem_1_{stem1_filename}")
        output_stem2_path = os.path.join(output_dir, f"jasco_stem_2_{stem2_filename}")
        
        # å°†ä¸´æ—¶æ–‡ä»¶ç§»åŠ¨æˆ–å¤åˆ¶åˆ°æŒ‡å®šç›®å½•
        shutil.move(temp_stem1_path, output_stem1_path)
        shutil.move(temp_stem2_path, output_stem2_path)
        
        print(f"æ–‡ä»¶å·²ä¿å­˜åˆ°: {output_stem1_path} å’Œ {output_stem2_path}")
        return output_stem1_path, output_stem2_path
    else:
        # å¦‚æœä¸æŒ‡å®šè¾“å‡ºç›®å½•ï¼Œåˆ™è¿”å›Gradioå®¢æˆ·ç«¯ä¸‹è½½çš„ä¸´æ—¶æ–‡ä»¶è·¯å¾„
        print(f"æ–‡ä»¶å·²ä¸‹è½½åˆ°ä¸´æ—¶ç›®å½•: {result[0]} å’Œ {result[1]}")
        return result

@mcp.tool()
def step_audio_tts_3b_api(
    text: str,
    prompt_audio: str,
    prompt_text: str,
    output_path: str = "generated_clone_audio.wav"
):
    """
    è¯­éŸ³å…‹éš†å·¥å…·ï¼šä½¿ç”¨ä¸€æ®µå‚è€ƒéŸ³é¢‘æ¥å…‹éš†å…¶éŸ³è‰²ï¼Œå¹¶ç”Ÿæˆæ–°çš„è¯­éŸ³ã€‚

    è¯¦ç»†è¯´æ˜ï¼š
    - APIç«¯ç‚¹: /generate_clone
    - åŠŸèƒ½: å£°éŸ³å…‹éš†æ–‡æœ¬è½¬è¯­éŸ³ (Voice Clone TTS)
    - è¾“å…¥:
        - text (str): éœ€è¦è½¬æ¢æˆè¯­éŸ³çš„ç›®æ ‡æ–‡æœ¬ (å¿…å¡«)ã€‚
        - prompt_audio (str): ç”¨äºå…‹éš†éŸ³è‰²çš„å‚è€ƒéŸ³é¢‘æ–‡ä»¶è·¯å¾„ (å¿…å¡«)ã€‚
        - prompt_text (str): å‚è€ƒéŸ³é¢‘æ–‡ä»¶å¯¹åº”çš„æ–‡æœ¬å†…å®¹ (å¿…å¡«)ã€‚
    - è¿”å›å€¼:
        - str: ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶çš„ä¿å­˜è·¯å¾„ã€‚

    ç¤ºä¾‹ç”¨æ³•:
    >>> voice_clone_tts(
    ...     text="ä½ å¥½ï¼Œæ¬¢è¿ä½¿ç”¨è¿™ä¸ªå£°éŸ³å…‹éš†å·¥å…·ã€‚",
    ...     prompt_audio="path/to/sample.wav",
    ...     prompt_text="è¿™æ˜¯å‚è€ƒéŸ³é¢‘çš„æ–‡æœ¬ã€‚",
    ...     output_path="output/cloned_speech.wav"
    ... )
    'output/cloned_speech.wav'

    Args:
        text (str): å¸Œæœ›åˆæˆè¯­éŸ³çš„æ–‡æœ¬å†…å®¹ï¼Œå¿…å¡«é¡¹ã€‚
        prompt_audio (str): ä½œä¸ºå£°éŸ³æ ·æœ¬çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼Œç”¨äºæå–éŸ³è‰²ã€‚æ”¯æŒå¸¸è§çš„éŸ³é¢‘æ ¼å¼å¦‚ WAV, MP3 ç­‰ï¼Œå¿…å¡«é¡¹ã€‚
        prompt_text (str): æç¤ºéŸ³é¢‘ `prompt_audio` ä¸­å¯¹åº”çš„æ–‡æœ¬å†…å®¹ï¼Œå¿…å¡«é¡¹ã€‚
        output_path (str): ç”ŸæˆéŸ³é¢‘çš„ä¿å­˜è·¯å¾„ã€‚é»˜è®¤ä¸ºå½“å‰ç›®å½•ä¸‹çš„ "generated_clone_audio.wav"ã€‚

    Returns:
        str: æœ€ç»ˆä¿å­˜çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„ã€‚
    """
    # 1. å‚æ•°æ ¡éªŒ
    if not all([text, prompt_audio, prompt_text]):
        raise ValueError("å‚æ•° 'text', 'prompt_audio', å’Œ 'prompt_text' éƒ½æ˜¯å¿…å¡«é¡¹ã€‚")
    
    if not os.path.exists(prompt_audio):
        raise FileNotFoundError(f"æŒ‡å®šçš„æç¤ºéŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {prompt_audio}")

    # 2. å®ä¾‹åŒ–API client
    client = Client(src="https://swarmeta-ai-step-audio-tts-3b.ms.show/",hf_token=MD_TOKEN)

    # 3. è°ƒç”¨API
    # client.predict ä¼šè¿”å›ä¸€ä¸ªä¿å­˜åœ¨æœ¬åœ°ä¸´æ—¶ç›®å½•çš„æ–‡ä»¶è·¯å¾„
    temp_output_path = client.predict(
        text=text,
        prompt_audio=handle_file(prompt_audio),  # ä½¿ç”¨ handle_file å¤„ç†æ–‡ä»¶å‚æ•°
        prompt_text=prompt_text,
        api_name="/generate_clone"
    )

    # 4. ç»“æœä¿å­˜
    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶æ‰€åœ¨çš„ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    output_dir = os.path.dirname(output_path)
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
    
    # å°†ç»“æœä»ä¸´æ—¶è·¯å¾„ç§»åŠ¨åˆ°ç”¨æˆ·æŒ‡å®šçš„è·¯å¾„
    shutil.move(temp_output_path, output_path)
    
    print(f"éŸ³é¢‘å·²æˆåŠŸç”Ÿæˆå¹¶ä¿å­˜è‡³: {output_path}")
    return output_path


@mcp.tool()
def sparkTTS_tool_api(
    task: str,
    text: str,
    output_path: str,
    prompt_text: str = "",
    prompt_audio_path: str = "",
    gender: str = "male",
    pitch: float = 3.0,
    speed: float = 3.0
):
    """
    SparkTTSå·¥å…·ï¼Œç”¨äºæ–‡æœ¬è½¬è¯­éŸ³ï¼Œæ”¯æŒå£°éŸ³å…‹éš†å’Œè‡ªå®šä¹‰å£°éŸ³åˆ›å»ºã€‚

    è¯¦ç»†è¯´æ˜ï¼š
    - æ”¯æŒçš„APIç«¯ç‚¹/ä»»åŠ¡:
        - 'voice_clone': é€šè¿‡ä¸Šä¼ ä¸€ä¸ªéŸ³é¢‘æ ·æœ¬æ¥å…‹éš†å£°éŸ³ï¼Œå¹¶ç”¨è¯¥å£°éŸ³æœ—è¯»æŒ‡å®šçš„æ–‡æœ¬ã€‚
        - 'voice_creation': æ ¹æ®æŒ‡å®šçš„æ€§åˆ«ã€éŸ³é«˜å’Œè¯­é€Ÿå‚æ•°ï¼Œç”Ÿæˆè‡ªå®šä¹‰å£°éŸ³æ¥æœ—è¯»æ–‡æœ¬ã€‚
    - è¾“å…¥è¯´æ˜:
        - å¯¹äº 'voice_clone' ä»»åŠ¡, `text`, `prompt_text`, å’Œ `prompt_audio_path` æ˜¯å¿…å¡«é¡¹ã€‚
        - å¯¹äº 'voice_creation' ä»»åŠ¡, `text` æ˜¯å¿…å¡«é¡¹, è€Œ `gender`, `pitch`, `speed` æ‹¥æœ‰é»˜è®¤å€¼ã€‚
    - è¿”å›å€¼è¯´æ˜:
        - å‡½æ•°æ‰§è¡ŒæˆåŠŸåè¿”å›ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„ã€‚
    - ç¤ºä¾‹ç”¨æ³•:
        - å£°éŸ³å…‹éš†: SparkTTS_tool(task='voice_clone', text='è¿™æ˜¯å…‹éš†çš„å£°éŸ³ã€‚', prompt_text='è¿™æ˜¯æç¤ºéŸ³é¢‘çš„æ–‡æœ¬ã€‚', prompt_audio_path='./sample.wav', output_path='./clone_output.wav')
        - è‡ªå®šä¹‰å£°éŸ³åˆ›å»º: SparkTTS_tool(task='voice_creation', text='è¿™æ˜¯è‡ªå®šä¹‰çš„å£°éŸ³ã€‚', gender='female', pitch=4, speed=2, output_path='./creation_output.wav')

    Args:
        task (str): è¦æ‰§è¡Œçš„ä»»åŠ¡ï¼Œå¿…å¡«é¡¹ï¼Œå¯é€‰å€¼ä¸º 'voice_clone' æˆ– 'voice_creation'ã€‚
        text (str): è¦è½¬æ¢ä¸ºè¯­éŸ³çš„è¾“å…¥æ–‡æœ¬ï¼Œå¿…å¡«é¡¹ã€‚
        output_path (str): ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶çš„ä¿å­˜è·¯å¾„ï¼Œå¿…å¡«é¡¹ã€‚
        prompt_text (str): ç”¨äºå£°éŸ³å…‹éš†çš„æç¤ºéŸ³é¢‘å¯¹åº”çš„æ–‡æœ¬ã€‚å½“ task='voice_clone' æ—¶ä¸ºå¿…å¡«é¡¹ã€‚
        prompt_audio_path (str): ç”¨äºå£°éŸ³å…‹éš†çš„æç¤ºéŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚ .wavï¼‰ï¼Œé‡‡æ ·ç‡å»ºè®®ä¸ä½äº16kHzã€‚å½“ task='voice_clone' æ—¶ä¸ºå¿…å¡«é¡¹ã€‚
        gender (str): ç”Ÿæˆå£°éŸ³çš„æ€§åˆ«ï¼Œå¯é€‰å€¼ä¸º 'male' æˆ– 'female'ã€‚é»˜è®¤ä¸º 'male'ã€‚ä»…åœ¨ task='voice_creation' æ—¶ä½¿ç”¨ã€‚
        pitch (float): ç”Ÿæˆå£°éŸ³çš„éŸ³é«˜ã€‚é»˜è®¤ä¸º 3.0ã€‚ä»…åœ¨ task='voice_creation' æ—¶ä½¿ç”¨ã€‚
        speed (float): ç”Ÿæˆå£°éŸ³çš„è¯­é€Ÿã€‚é»˜è®¤ä¸º 3.0ã€‚ä»…åœ¨ task='voice_creation' æ—¶ä½¿ç”¨ã€‚

    Returns:
        str: æˆåŠŸæ—¶è¿”å›åŒ…å«ç”ŸæˆéŸ³é¢‘æ–‡ä»¶æœ€ç»ˆè·¯å¾„çš„å­—ç¬¦ä¸²ã€‚
    """

    # 2. å‚æ•°æ ¡éªŒ
    if task not in ['voice_clone', 'voice_creation']:
        raise ValueError("å‚æ•° 'task' å¿…é¡»æ˜¯ 'voice_clone' æˆ– 'voice_creation'")
    if not text:
        raise ValueError("å‚æ•° 'text' ä¸ºå¿…å¡«é¡¹")
    if not output_path:
        raise ValueError("å‚æ•° 'output_path' ä¸ºå¿…å¡«é¡¹")

    # 3. å®ä¾‹åŒ–API client
    client = Client("thunnai/SparkTTS", hf_token=HF_TOKEN)
    
    result_temp_path = None

    if task == 'voice_clone':
        # 'voice_clone' ä»»åŠ¡çš„ç‰¹å®šå‚æ•°æ ¡éªŒ
        if not prompt_text:
            raise ValueError("å½“ task='voice_clone' æ—¶, 'prompt_text' ä¸ºå¿…å¡«å‚æ•°")
        if not prompt_audio_path:
            raise ValueError("å½“ task='voice_clone' æ—¶, 'prompt_audio_path' ä¸ºå¿…å¡«å‚æ•°")
        if not os.path.exists(prompt_audio_path):
            raise FileNotFoundError(f"æä¾›çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„ä¸å­˜åœ¨: {prompt_audio_path}")

        # 4. æ–‡ä»¶å‚æ•°å¤„ç† & 5. è°ƒç”¨API
        result_temp_path = client.predict(
            text=text,
            prompt_text=prompt_text,
            prompt_wav_upload=handle_file(prompt_audio_path),
            prompt_wav_record=handle_file(prompt_audio_path), # APIè¦æ±‚ä¸¤ä¸ªéŸ³é¢‘è¾“å…¥ï¼Œä¼ å…¥åŒä¸€ä¸ªå³å¯
            api_name="/voice_clone"
        )
    
    elif task == 'voice_creation':
        # 5. è°ƒç”¨API
        result_temp_path = client.predict(
            text=text,
            gender=gender,
            pitch=pitch,
            speed=speed,
            api_name="/voice_creation"
        )

    # 6. ç»“æœä¿å­˜
    if result_temp_path and os.path.exists(result_temp_path):
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(os.path.abspath(output_path))
        os.makedirs(output_dir, exist_ok=True)
        
        # å°†Gradioå®¢æˆ·ç«¯ä¸‹è½½çš„ä¸´æ—¶æ–‡ä»¶ç§»åŠ¨åˆ°ç”¨æˆ·æŒ‡å®šçš„è·¯å¾„
        shutil.move(result_temp_path, output_path)
        return f"éŸ³é¢‘æ–‡ä»¶å·²æˆåŠŸä¿å­˜è‡³: {output_path}"
    else:
        raise ConnectionError("APIè°ƒç”¨å¤±è´¥æˆ–æœªè¿”å›æœ‰æ•ˆçš„æ–‡ä»¶è·¯å¾„")

@mcp.tool()
def yue_api(
    genre_txt: Optional[str] = None,
    lyrics_txt: Optional[str] = None,
    num_segments: int = 2,
    duration: int = 30,
    use_audio_prompt: bool = False,
    audio_prompt_path: str = "",
    output_dir: str = "yue_music_output"
) -> Dict[str, str]:
    """
    YuEéŸ³ä¹ç”Ÿæˆå·¥å…·

    è¯¦ç»†è¯´æ˜ï¼š
    - è°ƒç”¨ Hugging Face Space ä¸Šçš„ "innova-ai/YuE-music-generator-demo" API æ¥ç”ŸæˆéŸ³ä¹ã€‚
    - æ”¯æŒé€šè¿‡æŒ‡å®šéŸ³ä¹æµæ´¾ã€æ­Œè¯æ¥ç”Ÿæˆï¼Œä¹Ÿå¯ä»¥æä¾›ä¸€ä¸ªéŸ³é¢‘æ–‡ä»¶ä½œä¸ºç”Ÿæˆçš„æç¤ºï¼ˆpromptï¼‰ã€‚
    - APIä¼šè¿”å›ä¸‰ä¸ªéŸ³é¢‘æ–‡ä»¶ï¼šä¸€ä¸ªæ··åˆæˆå“ï¼Œä¸€ä¸ªçº¯äººå£°ï¼Œä¸€ä¸ªçº¯ä¼´å¥ã€‚

    Args:
        genre_txt (Optional[str]): éŸ³ä¹æµæ´¾çš„æ–‡æœ¬æè¿°ï¼Œä¾‹å¦‚ "Pop" æˆ– "æŠ’æƒ…æ°‘è°£"ã€‚é»˜è®¤ä¸º Noneã€‚
        lyrics_txt (Optional[str]): éŸ³ä¹çš„æ­Œè¯æ–‡æœ¬ã€‚é»˜è®¤ä¸º Noneã€‚
        num_segments (int): è¦ç”Ÿæˆçš„éŸ³ä¹ç‰‡æ®µæ•°é‡ã€‚é»˜è®¤ä¸º 2ã€‚
        duration (int): ç”Ÿæˆæ­Œæ›²çš„æ—¶é•¿ï¼ˆç§’ï¼‰ã€‚é»˜è®¤ä¸º 30ã€‚
        use_audio_prompt (bool): æ˜¯å¦ä½¿ç”¨æä¾›çš„éŸ³é¢‘æ–‡ä»¶ä½œä¸ºç”Ÿæˆæç¤ºã€‚é»˜è®¤ä¸º Falseã€‚
        audio_prompt_path (str): ä½œä¸ºæç¤ºçš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆæœ¬åœ°è·¯å¾„æˆ–URLï¼‰ã€‚å¦‚æœ use_audio_prompt ä¸º Trueï¼Œåˆ™æ­¤å‚æ•°ä¸ºå¿…å¡«é¡¹ã€‚
        output_dir (str): ç”¨äºä¿å­˜æœ€ç»ˆç”Ÿæˆçš„ä¸‰ä¸ªéŸ³é¢‘æ–‡ä»¶çš„ç›®å½•è·¯å¾„ã€‚é»˜è®¤ä¸º "yue_music_output"ã€‚

    Returns:
        Dict[str, str]: ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«ä¸‰ä¸ªå·²ä¿å­˜éŸ³é¢‘æ–‡ä»¶çš„è·¯å¾„ï¼Œé”®åˆ†åˆ«ä¸º 'mixed_audio', 'vocal_audio', 'instrumental_audio'ã€‚
    """

    # 1. å‚æ•°æ ¡éªŒ
    if use_audio_prompt and not audio_prompt_path:
        raise ValueError("å½“ use_audio_prompt è®¾ç½®ä¸º True æ—¶, å¿…é¡»æä¾› audio_prompt_pathã€‚")
    
    if audio_prompt_path and not audio_prompt_path.startswith(('http://', 'https://')) and not os.path.exists(audio_prompt_path):
        raise FileNotFoundError(f"è¾“å…¥éŸ³é¢‘æ–‡ä»¶æœªæ‰¾åˆ°: {audio_prompt_path}")

    # 2. å®ä¾‹åŒ–API client
    # è¿™æ˜¯huggingface space api çš„ä½¿ç”¨è·¯å¾„æ–¹å¼
    try:
        client = Client("innova-ai/YuE-music-generator-demo", hf_token=HF_TOKEN)
    except Exception as e:
        raise ConnectionError(f"æ— æ³•è¿æ¥åˆ°Hugging Face Space 'innova-ai/YuE-music-generator-demo'ã€‚è¯·æ£€æŸ¥ç½‘ç»œæˆ–APIçŠ¶æ€ã€‚é”™è¯¯: {e}")

    # 3. æ–‡ä»¶å‚æ•°å¤„ç†
    # æ ¹æ®APIæ–‡æ¡£ï¼Œaudio_prompt_path æ˜¯ä¸€ä¸ªå¿…éœ€å‚æ•°ï¼Œå³ä½¿ä¸ä½¿ç”¨ä¹Ÿéœ€è¦ä¼ é€’ä¸€ä¸ªå ä½ç¬¦ã€‚
    # å¦‚æœç”¨æˆ·æœªæä¾›ï¼Œåˆ™ä½¿ç”¨æ–‡æ¡£ä¸­çš„é»˜è®¤URLã€‚
    prompt_file_source = audio_prompt_path if audio_prompt_path else 'https://github.com/gradio-app/gradio/raw/main/test/test_files/audio_sample.wav'
    
    # 4. è°ƒç”¨API
    # æ³¨æ„ï¼šAPIæ–‡æ¡£ä¸­çš„ run_n_segments å’Œ max_new_tokens æœŸæœ›çš„æ˜¯ float ç±»å‹ã€‚
    result: Tuple[str, str, str] = client.predict(
        genre_txt=genre_txt,
        lyrics_txt=lyrics_txt,
        run_n_segments=float(num_segments),
        max_new_tokens=float(duration),
        use_audio_prompt=use_audio_prompt,
        audio_prompt_path=handle_file(prompt_file_source),
        api_name="/generate_music"
    )

    # 5. ç»“æœä¿å­˜
    # result æ˜¯ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªä¸´æ—¶æ–‡ä»¶è·¯å¾„çš„å…ƒç»„
    mixed_temp_path, vocal_temp_path, instrumental_temp_path = result
    
    os.makedirs(output_dir, exist_ok=True)
    
    # å°†ä¸´æ—¶æ–‡ä»¶å¤åˆ¶åˆ°æŒ‡å®šçš„è¾“å‡ºç›®å½•ï¼Œå¹¶ä½¿ç”¨æ›´æ˜ç¡®çš„æ–‡ä»¶å
    final_paths = {
        "mixed_audio": os.path.join(output_dir, "mixed_audio_result.wav"),
        "vocal_audio": os.path.join(output_dir, "vocal_audio_result.wav"),
        "instrumental_audio": os.path.join(output_dir, "instrumental_audio_result.wav")
    }
    
    shutil.copy(mixed_temp_path, final_paths["mixed_audio"])
    shutil.copy(vocal_temp_path, final_paths["vocal_audio"])
    shutil.copy(instrumental_temp_path, final_paths["instrumental_audio"])

    print(f"éŸ³ä¹ç”ŸæˆæˆåŠŸï¼æ–‡ä»¶å·²ä¿å­˜è‡³ç›®å½•: {os.path.abspath(output_dir)}")
    return final_paths

@mcp.tool()
def voicecraft_tts_and_edit_api(
    # 1. å‚æ•°åŒº
    mode: Literal['TTS', 'Edit', 'Long TTS'],
    transcript: str,
    audio_path: Optional[str] = None,
    output_path: str = "output.wav",
    seed: int = -1,
    smart_transcript: bool = True,
    prompt_end_time: float = 3.675,
    edit_start_time: float = 3.83,
    edit_end_time: float = 5.113,
    left_margin: float = 0.08,
    right_margin: float = 0.08,
    temperature: float = 1.0,
    top_p: float = 0.9,
    top_k: int = 0,
    sample_batch_size: int = 2,
    stop_repetition: Literal['-1', '1', '2', '3', '4'] = "3",
    kvcache: Literal['0', '1'] = "1",
    split_text: Literal['Newline', 'Sentence'] = "Newline",
    selected_sentence: Optional[str] = None,
    codec_audio_sr: int = 16000,
    codec_sr: int = 50,
    silence_tokens: str = "[1388,1898,131]"
):
    """
    ä½¿ç”¨ VoiceCraft æ¨¡å‹è¿›è¡Œæ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰ã€éŸ³é¢‘ç¼–è¾‘å’Œé•¿æ–‡æœ¬åˆæˆã€‚

    è¯¦ç»†è¯´æ˜ï¼š
    - æ”¯æŒçš„APIç«¯ç‚¹/ä»»åŠ¡: /run
    - æ”¯æŒä¸‰ç§æ¨¡å¼:
        1. 'TTS': æ ¹æ®è¾“å…¥æ–‡æœ¬å’Œå‚è€ƒéŸ³é¢‘ï¼ˆå¯é€‰ï¼Œç”¨äºéŸ³è‰²å…‹éš†ï¼‰ç”Ÿæˆè¯­éŸ³ã€‚
        2. 'Edit': ç¼–è¾‘è¾“å…¥éŸ³é¢‘çš„æŒ‡å®šéƒ¨åˆ†ï¼Œæ ¹æ®æ–°çš„æ–‡æœ¬è¿›è¡Œæ›¿æ¢ã€‚
        3. 'Long TTS': å°†é•¿æ–‡æœ¬åˆ†å‰²åï¼Œé€å¥ç”Ÿæˆå¹¶æ‹¼æ¥æˆå®Œæ•´çš„é•¿éŸ³é¢‘ã€‚
    - è¾“å…¥ç±»å‹ã€èŒƒå›´ã€å¿…å¡«é¡¹:
        - 'mode' å’Œ 'transcript' æ˜¯å¿…å¡«é¡¹ã€‚
        - åœ¨ 'Edit' æˆ– 'Long TTS' æ¨¡å¼ä¸‹ï¼Œ'audio_path' æ˜¯å¿…å¡«çš„ã€‚
        - 'selected_sentence' åœ¨ 'Long TTS' æ¨¡å¼ä¸‹æ˜¯å¿…å¡«çš„ã€‚
    - è¿”å›å€¼è¯´æ˜:
        - è¿”å›ä¸€ä¸ªåŒ…å«ç”Ÿæˆç»“æœçš„å­—å…¸ï¼ŒåŒ…æ‹¬è¾“å‡ºéŸ³é¢‘çš„è·¯å¾„å’Œæ¨æ–­å‡ºçš„æ–‡æœ¬ã€‚
    - ç¤ºä¾‹ç”¨æ³•:
        # çº¯æ–‡æœ¬è½¬è¯­éŸ³ (TTS)
        voicecraft_tts_and_edit(mode='TTS', transcript='Hello world, this is a test.', output_path='hello.wav')
        
        # éŸ³é¢‘ç¼–è¾‘ (Edit)
        voicecraft_tts_and_edit(mode='Edit', transcript='The quick brown fox jumps over the lazy dog.', audio_path='original.wav', edit_start_time=1.2, edit_end_time=2.5, output_path='edited.wav')

    Args:
        mode (Literal['TTS', 'Edit', 'Long TTS']): æ“ä½œæ¨¡å¼ï¼Œå¿…å¡«ã€‚
        transcript (str): ç”¨äºç”Ÿæˆæˆ–ç¼–è¾‘çš„æ–‡æœ¬ï¼Œå¿…å¡«ã€‚
        audio_path (Optional[str]): è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„ã€‚åœ¨ 'Edit' å’Œ 'Long TTS' æ¨¡å¼ä¸‹æ˜¯å¿…éœ€çš„ã€‚
        output_path (str): ç”Ÿæˆçš„éŸ³é¢‘æ–‡ä»¶çš„ä¿å­˜è·¯å¾„ã€‚é»˜è®¤ "output.wav"ã€‚
        seed (int): éšæœºç§å­ï¼Œç”¨äºå¯å¤ç°çš„ç»“æœã€‚-1è¡¨ç¤ºéšæœºã€‚é»˜è®¤ -1ã€‚
        smart_transcript (bool): æ˜¯å¦å¯ç”¨æ™ºèƒ½è½¬å½•ã€‚é»˜è®¤ Trueã€‚
        prompt_end_time (float): åœ¨ 'Edit' æ¨¡å¼ä¸‹ï¼Œä½œä¸ºæç¤ºçš„éŸ³é¢‘çš„ç»“æŸæ—¶é—´ç‚¹ã€‚é»˜è®¤ 3.675ã€‚
        edit_start_time (float): åœ¨ 'Edit' æ¨¡å¼ä¸‹ï¼Œéœ€è¦ç¼–è¾‘çš„èµ·å§‹æ—¶é—´ç‚¹ã€‚é»˜è®¤ 3.83ã€‚
        edit_end_time (float): åœ¨ 'Edit' æ¨¡å¼ä¸‹ï¼Œéœ€è¦ç¼–è¾‘çš„ç»“æŸæ—¶é—´ç‚¹ã€‚é»˜è®¤ 5.113ã€‚
        left_margin (float): éŸ³é¢‘å·¦è¾¹ç•Œè£•é‡ã€‚é»˜è®¤ 0.08ã€‚
        right_margin (float): éŸ³é¢‘å³è¾¹ç•Œè£•é‡ã€‚é»˜è®¤ 0.08ã€‚
        temperature (float): ç”Ÿæˆçš„å¤šæ ·æ€§ï¼Œè¶Šé«˜è¶Šéšæœºã€‚é»˜è®¤ 1.0ã€‚
        top_p (float): nucleusé‡‡æ ·é˜ˆå€¼ã€‚é»˜è®¤ 0.9ã€‚
        top_k (int): top-ké‡‡æ ·ã€‚0è¡¨ç¤ºç¦ç”¨ã€‚é»˜è®¤ 0ã€‚
        sample_batch_size (int): é‡‡æ ·æ‰¹æ¬¡å¤§å°ï¼Œå¯è§†ä¸ºå½±å“è¯­é€Ÿã€‚é»˜è®¤ 2ã€‚
        stop_repetition (Literal['-1', '1', '2', '3', '4']): åœæ­¢é‡å¤çš„ç­‰çº§ã€‚é»˜è®¤ "3"ã€‚
        kvcache (Literal['0', '1']): æ˜¯å¦ä½¿ç”¨KVç¼“å­˜ã€‚'1'ä¸ºæ˜¯ï¼Œ'0'ä¸ºå¦ã€‚é»˜è®¤ "1"ã€‚
        split_text (Literal['Newline', 'Sentence']): åœ¨ 'Long TTS' æ¨¡å¼ä¸‹ï¼Œæ–‡æœ¬åˆ†å‰²çš„æ–¹å¼ã€‚é»˜è®¤ "Newline"ã€‚
        selected_sentence (Optional[str]): åœ¨ 'Long TTS' æ¨¡å¼ä¸‹ï¼Œå½“å‰è¦å¤„ç†çš„å¥å­ã€‚æ­¤æ¨¡å¼ä¸‹å¿…å¡«ã€‚é»˜è®¤ Noneã€‚
        codec_audio_sr (int): ç¼–è§£ç å™¨éŸ³é¢‘é‡‡æ ·ç‡ã€‚é»˜è®¤ 16000ã€‚
        codec_sr (int): ç¼–è§£ç å™¨é‡‡æ ·ç‡ã€‚é»˜è®¤ 50ã€‚
        silence_tokens (str): ä»£è¡¨é™éŸ³çš„tokenåˆ—è¡¨ã€‚é»˜è®¤ "[1388,1898,131]"ã€‚

    Returns:
        dict: ä¸€ä¸ªåŒ…å«ç»“æœçš„å­—å…¸ï¼Œæ ¼å¼ä¸º {'output_audio_path': str, 'inference_transcript': str}ã€‚
    """

    # 2. å‚æ•°æ ¡éªŒ
    if not transcript:
        raise ValueError("å‚æ•° 'transcript' æ˜¯å¿…å¡«é¡¹ã€‚")

    if mode in ['Edit', 'Long TTS']:
        if not audio_path:
            raise ValueError(f"åœ¨ '{mode}' æ¨¡å¼ä¸‹, 'audio_path' æ˜¯å¿…å¡«å‚æ•°ã€‚")
        if not os.path.exists(audio_path):
            raise FileNotFoundError(f"è¾“å…¥éŸ³é¢‘æ–‡ä»¶æœªæ‰¾åˆ°: {audio_path}")
    
    if mode == 'Long TTS' and not selected_sentence:
        raise ValueError("åœ¨ 'Long TTS' æ¨¡å¼ä¸‹, 'selected_sentence' æ˜¯å¿…å¡«å‚æ•°ã€‚")

    # 3. å®ä¾‹åŒ–API client
    try:
        client = Client("Approximetal/VoiceCraft_gradio", hf_token=HF_TOKEN)
    except Exception as e:
        raise ConnectionError(f"æ— æ³•è¿æ¥åˆ°Gradio Space 'Approximetal/VoiceCraft_gradio': {e}")

    # 4. æ–‡ä»¶å‚æ•°å¤„ç†
    input_audio_file = file(audio_path) if audio_path else None
    
    # 5. è°ƒç”¨API
    # APIçš„ 'selected_sentence' å‚æ•°æ˜¯å¿…éœ€çš„ï¼Œå³ä½¿åœ¨é'Long TTS'æ¨¡å¼ä¸‹ï¼Œå› æ­¤æˆ‘ä»¬ä¼ é€’Noneã€‚
    result = client.predict(
        seed=seed,
        left_margin=left_margin,
        right_margin=right_margin,
        codec_audio_sr=float(codec_audio_sr),
        codec_sr=float(codec_sr),
        top_k=top_k,
        top_p=top_p,
        temperature=temperature,
        stop_repetition=stop_repetition,
        sample_batch_size=float(sample_batch_size),
        kvcache=kvcache,
        silence_tokens=silence_tokens,
        audio_path=input_audio_file,
        transcript=transcript,
        smart_transcript=smart_transcript,
        mode=mode,
        prompt_end_time=prompt_end_time,
        edit_start_time=edit_start_time,
        edit_end_time=edit_end_time,
        split_text=split_text,
        selected_sentence=selected_sentence,
        api_name="/run"
    )
    
    # 6. ç»“æœä¿å­˜
    # resultæ˜¯ä¸€ä¸ªå…ƒç»„ï¼Œç¬¬0ä¸ªå…ƒç´ æ˜¯è¾“å‡ºéŸ³é¢‘çš„ä¸´æ—¶è·¯å¾„ï¼Œç¬¬1ä¸ªæ˜¯æ¨æ–­æ–‡æœ¬
    temp_audio_path, inference_transcript = result[0], result[1]
    
    if temp_audio_path and os.path.exists(temp_audio_path):
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(os.path.abspath(output_path))
        os.makedirs(output_dir, exist_ok=True)
        
        # å°†ç»“æœä»ä¸´æ—¶è·¯å¾„å¤åˆ¶åˆ°ç”¨æˆ·æŒ‡å®šçš„è·¯å¾„
        shutil.copy(temp_audio_path, output_path)
        
        return {
            "output_audio_path": os.path.abspath(output_path),
            "inference_transcript": inference_transcript
        }
    elif not temp_audio_path:
        raise RuntimeError("APIè°ƒç”¨æˆåŠŸï¼Œä½†æœªè¿”å›ä»»ä½•éŸ³é¢‘æ–‡ä»¶ã€‚è¯·æ£€æŸ¥è¾“å…¥å‚æ•°ã€‚")
    else:
        raise FileNotFoundError(f"APIè¿”å›äº†ä¸€ä¸ªä¸´æ—¶çš„éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼Œä½†è¯¥æ–‡ä»¶ä¸å­˜åœ¨: {temp_audio_path}")



@mcp.tool()
def image2music_api(
    image_path: str,
    output_dir: str = "outputs/music",
    model: Literal['ACE Step', 'AudioLDM-2', 'Riffusion', 'Mustango', 'Stable Audio Open'] = 'ACE Step'

) -> str:
    """Generate music from an image using the image-to-music-v2 model.
    
    Args:
        image_path (str): Path to the local image file.
        output_dir (str, optional): Path to the output directory. Defaults to "outputs/music".
        model (Literal, optional): Model to use for music generation. 
            Options: 'ACE Step', 'AudioLDM-2', 'Riffusion', 'Mustango', 'Stable Audio Open'. 
            Defaults to 'ACE Step'.
    
    Returns:
        str: Path to the generated audio file.
    """
    # Create a client connection to the Hugging Face space
    client = Client("fffiloni/image-to-music-v2")
    
    # Ensure the image path exists
    if not os.path.exists(image_path):
        raise FileNotFoundError(f"Image file not found: {image_path}")
    

    
    # Generate output filename
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = output_dir / f"music_{timestamp}.wav"
    
    try:
        # Call the API with the image and parameters
        result = client.predict(
            image_in=handle_file(image_path),  # æ­£ç¡®å¤„ç†å›¾ç‰‡æ–‡ä»¶
            chosen_model=model,                # é€‰æ‹©æ¨¡å‹
            api_name="/infer"                  # æ­£ç¡®çš„APIåç§°
        )
        
        # å¤„ç†æç¤ºè¯
        prompt_data = result[0]
        if isinstance(prompt_data, dict) and 'value' in prompt_data:
            prompt = prompt_data['value']
            print(f"Generated inspirational prompt: {prompt}")
        else:
            print(f"Generated data: {prompt_data}")
        
        # å¤„ç†éŸ³é¢‘æ–‡ä»¶
        audio_url = result[1]
        
        # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶ä¸‹è½½éŸ³é¢‘
        temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".mp3")
        temp_file.close()
        
        # å¦‚æœæ˜¯URLï¼Œåˆ™ä¸‹è½½
        if isinstance(audio_url, str) and (audio_url.startswith('http://') or audio_url.startswith('https://')):
            response = requests.get(audio_url)
            if response.status_code == 200:
                with open(temp_file.name, 'wb') as f:
                    f.write(response.content)
            else:
                raise Exception(f"Failed to download audio file: {response.status_code}")
        # å¦‚æœå·²ç»æ˜¯æœ¬åœ°æ–‡ä»¶è·¯å¾„
        elif isinstance(audio_url, str) and os.path.exists(audio_url):
            with open(audio_url, 'rb') as f_in:
                with open(temp_file.name, 'wb') as f_out:
                    f_out.write(f_in.read())
        else:
            raise Exception(f"Unexpected audio result format: {type(audio_url)}")
        
        # å¤åˆ¶åˆ°æœ€ç»ˆè¾“å‡ºä½ç½®
        with open(temp_file.name, "rb") as f_in:
            with open(output_file, "wb") as f_out:
                f_out.write(f_in.read())
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            os.unlink(temp_file.name)
        except:
            pass
            
        return str(output_file)
    except Exception as e:
        import traceback
        return f"Error generating music: {str(e)}\n{traceback.format_exc()}"



if __name__ == "__main__":
	# å¯åŠ¨MCPæœåŠ¡å™¨
	mcp.run()
